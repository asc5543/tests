<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Incoming Test Cases on Harvester manual test cases</title>
    <link>https://harvester.github.io/tests/manual/_incoming/</link>
    <description>Recent content in Incoming Test Cases on Harvester manual test cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://harvester.github.io/tests/manual/_incoming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Add extra disks by using raw disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</guid>
      <description>Prepare a disk (with WWN) and attach it to the node. Navigate to &amp;ldquo;Host&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo; and open the dropdown menu &amp;ldquo;Add disks&amp;rdquo;. Choose a disk to add, e.g. /dev/sda but not /dev/sda1. Expected Results The raw disk shall be schedulable as a longhorn disk as a whole (without any partition). Ths raw disk shall be in provisioned phase. Reboot the host and the disk shall be reattached and added back as a longhorn disk.</description>
    </item>
    
    <item>
      <title>All Namespace filtering in VM list</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</guid>
      <description> Related issues: #2578 [BUG] When first entering the harvester cluster from Virtualization Managements, some vm&amp;rsquo;s in namespace are not shown in the list Category: UI Verification Steps Create a harvester cluster Create a VM in the default namespace Creating a Namespace (eg: test-vm) Import the Harvester cluster in Rancher access to the harvester cluster from Virtualization Management click Virtual Machines tab Expected Results test-vm-1 should also be shown in the list </description>
    </item>
    
    <item>
      <title>Auto provision lots of extra disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</guid>
      <description>:warning: This is a heuristic test plan since real world race condition is hard to reproduce. If you find any better alternative, feel free to update.
This test is better to perform under QEMU/libvirt environment.
Related issues: #1718 [BUG] Automatic disk provisioning result in unusable ghost disks on NVMe drives Category: Storage Verification Steps Create a harvester cluster and attach 10 or more extra disks (needs WWN so that they can be identified uniquely).</description>
    </item>
    
    <item>
      <title>Boot installer under Legacy BIOS and UEFI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</guid>
      <description>Related issues #2023 Legacy Iso for older servers Verification Steps BIOS Test Build harvester-installer Boot build artifact using BIOS Legacy mode: qemu-system-x86_64 -m 2048 -cdrom ../dist/artifacts/harvester-master-amd64 Verify that the installer boot process reaches the screen that says &amp;ldquo;Create New Cluster&amp;rdquo; or &amp;ldquo;Join existing cluster&amp;rdquo; UEFI Test Build harvester-installer (or use the same one from the BIOS Test, it&amp;rsquo;s a hybrid ISO) Boot build artifact using UEFI mode: qemu-system-x86_64 -m 2048 -cdrom .</description>
    </item>
    
    <item>
      <title>Check conditions when stop/pause VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</guid>
      <description>Related issues: #1987 Verification Steps Stop Request should not have failure message
Create a VM with runStrategy: RunStrategyAlways. Stop the VM. Check there is no Failure attempting to delete VMI: &amp;lt;nil&amp;gt; in VM status. UI should not show pause message
Create a VM. Pause the VM. Although the message The status of pod readliness gate &amp;quot;kubevirt.io/virtual-machine-unpaused&amp;quot; is not &amp;quot;True&amp;quot;, but False is in the VM condition, UI should not show it.</description>
    </item>
    
    <item>
      <title>Check DNS on install with Github SSH keys</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</guid>
      <description>Related issues: #1903 DNS server not available during install Verification Steps Without PXE Start a new install Set DNS as 8.8.8.8 Add in github SSH keys Finish install SSH into node with SSH keys from github (rancher@hostname) Verify login was successful With PXE Got vagrant setup from https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Changed settings.yml DHCP config and added dns: 8.8.8.8 dhcp_server: ip: 192.168.0.254 subnet: 192.168.0.0 netmask: 255.255.255.0 range: 192.168.0.50 192.168.0.130 dns: 8.8.8.8 https: false Also changed ssh_authorized_keys and commented out default SSH key and added username for github</description>
    </item>
    
    <item>
      <title>Check IPAM configuration with IPAM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</guid>
      <description> Related issues: #1697 Optimization for the Harvester load balancer Verification Steps Install the latest rancher and import a Harvester cluster Create a cluster by Harvester node driver Navigate to the workload Page, create a workload Click &amp;ldquo;Add ports&amp;rdquo;, select type as LB, protocol as TCP Check IPAM selector Navigate to the service page, create a LB Click &amp;ldquo;Add-on config&amp;rdquo; tab and check IPAM and port </description>
    </item>
    
    <item>
      <title>Check logs on Harvester</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</guid>
      <description> Related issues: #2528 [BUG] Tons of AppArmor denied messages Category: Logging Environment Setup This should be run on a Harvester node that has been up for a while and has been in use Verification Steps SSH to harvester node Execute journalctl -b -f Look through logs and verify that there isn&amp;rsquo;t anything generating lots of erroneous messages Expected Results There shouldn&amp;rsquo;t be large volumes of erroneous messages </description>
    </item>
    
    <item>
      <title>Clone image</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</guid>
      <description> Related issues: #2562 [[BUG] Image&amp;rsquo;s labels will not be copied when execute Clone Category: Images Verification Steps Install Harvester with any nodes Create a Image via URL Clone the Image and named image-b Check image-b labels in Labels tab Expected Results All labels should be cloned and shown in labels tab </description>
    </item>
    
    <item>
      <title>Generate Install Support Config Bundle For Single Node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</guid>
      <description>Related issue: #1864 Support bundle for a single node (Live/Installed)
Related issue: #272 Generate supportconfig for failed installations
Category: Support Environment setup Setup a single node harvester from ISO install but don&amp;rsquo;t complete the installation
Gain SSH Access to the Single Harvester Node Once Shelled into the Single Harvester Node edit the /usr/sbin/harv-install Using: harvester-installer&amp;rsquo;s harv-install as a reference edit around line #362 adding exit 1: exit 1 trap cleanup exit check_iso save the file.</description>
    </item>
    
    <item>
      <title>Image naming with inline CSS</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</guid>
      <description> Related issues: #2563 [[BUG] harvesterhci.io.virtualmachineimage spec.displayName displays differently in single view of image Category: Images Verification Steps Go to images Click &amp;ldquo;Create&amp;rdquo; Upload an image or leverage an url - but name the image something like: &amp;lt;strong&amp;gt;&amp;lt;em&amp;gt;something_interesting&amp;lt;/em&amp;gt;&amp;lt;/strong&amp;gt; Wait for upload to complete. Observe the display name within the list of images Compare that to clicking into the single image and viewing it Expected Results The list view naming would be the same as the single view of the image </description>
    </item>
    
    <item>
      <title>Install Harvester over previous GNU/Linux install</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</guid>
      <description> Related issues: #2230 [BUG] harvester installer - always first attempt failed if before was linux installed Related issues: #2450 [backport v1.0][BUG] harvester installer - always first attempt failed if before was linux installed #2450 Category: Installtion Verification Steps Install GNU/LInux LVM configuration reboot Install Harvester via ISO over previous linux install Verifiy Harvester install by changing password and logging in. Expected Results Install should complete </description>
    </item>
    
    <item>
      <title>Multiple Disks Swapping Paths</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</guid>
      <description>Related issues: #1874 Multiple Disks Swapping Paths Verification Steps Prepare a harvester cluster (single node is sufficient) Prepare two additional disks and format both of them. Hotplug both disks and add them to the host via Harvester Dashboard (&amp;ldquo;Hosts&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo;) Shutdown the host. Swap the address and slot of the two disks in order to make their dev paths swapped For libvirt environment, you can swap &amp;lt;address&amp;gt; and &amp;lt;target&amp;gt; in the XML of the disk.</description>
    </item>
    
    <item>
      <title>Negative change backup target while restoring backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</guid>
      <description>Related issues: #2560 [BUG] VM hanging on restoring state when backup-target disconnected suddenly Category: Category Verification Steps Install Harvester with any nodes Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3 Create Image for VM creation Create VM vm1 Take Backup vm1b from vm1 Restore the backup vm1b to New/Existing VM When the VM still in restoring state, update backup-target settings to Use the default value then setup it back.</description>
    </item>
    
    <item>
      <title>Negative Harvester installer input same NIC IP and VIP</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</guid>
      <description> Related issues: #2229 [BUG] input nic ip and vip with same ip address in Harvester-Installer Related issues: #2377 [Backport v1.0.3] input nic ip and vip with same ip address in Harvester-Installer Category: Installation Verification Steps Boot into ISO installer Specify same IP for NIC and VIP Expected Results Error message is displayed </description>
    </item>
    
    <item>
      <title>Negative Restore a backup while VM is restoring</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</guid>
      <description>Related issues: #2559 [BUG] Backup unable to be restored and the VM can&amp;rsquo;t be deleted Category: Backup/Restore Verification Steps Install Harvester with any nodes Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3 Create Image for VM creation Create VM vm1 Take backup from vm1 as vm1b Take backup from vm1 as vm1b2 Click Edit YAML of vm1b, update field status.source.spec.spec.domain.cpu.cores, increase 1 Stop VM vm1 Restore backup vm1b2 with Replace Existing Restore backup vm1b with Replace Existing when the VM vm1 still in state restoring Expected Results You should get an error when trying to restore.</description>
    </item>
    
    <item>
      <title>RBAC Cluster Owner</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</guid>
      <description> Related issues: #2626 [BUG] Access Harvester project/namespace page hangs with no response timeout with local owner role from Rancher Category: Authentication Verification Steps Import Harvester from Rancher Create a standard user local in Rancher User &amp;amp; Authentication Open Cluster Management page Edit cluster config Expand Member Roles Add local user with Cluster Owner role Logout Admin Login with local user Access Harvester from virtualization management Click the Project/Namespace page Expected Results Local owner role user can access and display Harvester project/namespace place correctly without hanging to timeout </description>
    </item>
    
    <item>
      <title>RBAC Create VM with restricted admin user</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</guid>
      <description>Related issues: #2587 [BUG] namespace on create VM is wrong when going through Rancher #2116 [BUG] You can see cattle-monitoring-system volumes as restricted admin in Harvester Category: Authentication Verification Steps Verification Steps Import Harvester into Rancher Create a restricted admin Navigate to Volumes page Verify you only see associated Volumes Log out of admin and log in to restricted admin Navigate to Harvester UI via virtualization management Open virtual machines tab Click create Verified that namespace was default.</description>
    </item>
    
    <item>
      <title>Remove Pod Scheduling from harvester rke2 and rke1</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</guid>
      <description>Related issues: #2642 [BUG] Remove Pod Scheduling from harvester rke2 and rke1 Category: Rancher Test Information Test Environment: 1 node harvester on local kvm machine Harvester version: v1.0-44fb5f1a-head (08/10) Rancher version: v2.6.7-rc7
Environment Setup Prepare Harvester master node Prepare Rancher v2.6.7-rc7 Import Harvester to Rancher Set ui-offline-preferred: Remote Go to Harvester Support page Download Kubeconfig Copy the content of Kubeconfig Verification Steps RKE2 Verification Steps Open Harvester Host page then edit host config Add the following key value in the labels page: topology.</description>
    </item>
    
    <item>
      <title>Restart/Stop VM with in progress Backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</guid>
      <description> Related issues: #1702 Don&amp;rsquo;t allow restart/stop vm when backup is in progress Verification Steps Create a VM. Create a VMBackup for it. Before VMBackup is done, stop/restart the VM. Verify VM can&amp;rsquo;t be stopped/restarted. </description>
    </item>
    
    <item>
      <title>template with EFI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</guid>
      <description>Related issues: #2577 [BUG] Boot in EFI mode not selected when creating multiple VM instances using VM template with EFI mode selected. Category: Template Verification Steps Go to Template, create a VM template with Boot in EFI mode selected. Go to Virtual Machines, click Create, select Multiple instance, type in a random name prefix, and select the VM template we just created. Go to Advanced Options, for now this EFI checkbox should be checked without any issue.</description>
    </item>
    
    <item>
      <title>Terraform import VLAN</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</guid>
      <description>Related issues: #2261 [FEATURE] enhance terraform network to not pruge route_cidr and route_gateway Category: Terraform Verification Steps Install Harvester with any nodes Install terraform-harvester-provider (using master-head for testing) Execute terraform init Create the file network.tf as following snippets, then execute terraform import harvester_clusternetwork.vlan vlan to import default vlan settings resource &amp;#34;harvester_clusternetwork&amp;#34; &amp;#34;vlan&amp;#34; { name = &amp;#34;vlan&amp;#34; enable = true default_physical_nic = &amp;#34;harvester-mgmt&amp;#34; } resource &amp;#34;harvester_network&amp;#34; &amp;#34;vlan1&amp;#34; { name = &amp;#34;vlan1&amp;#34; namespace = &amp;#34;harvester-public&amp;#34; vlan_id = 1 route_mode = &amp;#34;auto&amp;#34; } execute terraform apply Login to dashboard then navigate to Advanced/Networks, make sure the Route Connectivity becomes Active Execute terraform apply again and many more times Expected Results Resources should not be changed or added or destroyed.</description>
    </item>
    
    <item>
      <title>Terraformer import KUBECONFIG</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</guid>
      <description>Related issues: #2604 [BUG] Terraformer imported VLAN always be 0 Category: Terraformer Verification Steps Install Harvester with any nodes Login to dashboard, navigate to: Advanced/Settings -&amp;gt; then enabledvlan` Navigate to Advanced/Networks and Create a Network which Vlan ID is not 0 Navigate to Support Page and Download KubeConfig file Initialize a terraform environment, download Harvester Terraformer Execute command terraformer import harvester -r network to generate terraform configuration from the cluster Generated file generated/harvester/network/network.</description>
    </item>
    
    <item>
      <title>Zero downtime upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</guid>
      <description> Related issues: #1707 [BUG] Zero downtime upgrade stuck in &amp;ldquo;Waiting for VM live-migration or shutdown&amp;hellip;&amp;rdquo; Category: Upgrade Verification Steps Create a ubuntu image from URL Enable Network with management-mgmt Create a virtual network vlan1 with id 1 Setup backup target Create a VM backup Follow the guide to do upgrade test Expected Results Can upgrade correctly with all VMs remain in running </description>
    </item>
    
  </channel>
</rss>
