<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Incoming Test Cases on Harvester manual test cases</title>
    <link>https://harvester.github.io/tests/manual/_incoming/</link>
    <description>Recent content in Incoming Test Cases on Harvester manual test cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://harvester.github.io/tests/manual/_incoming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Add extra disks by using raw disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</guid>
      <description>Prepare a disk (with WWN) and attach it to the node. Navigate to &amp;ldquo;Host&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo; and open the dropdown menu &amp;ldquo;Add disks&amp;rdquo;. Choose a disk to add, e.g. /dev/sda but not /dev/sda1. Expected Results The raw disk shall be schedulable as a longhorn disk as a whole (without any partition). Ths raw disk shall be in provisioned phase. Reboot the host and the disk shall be reattached and added back as a longhorn disk.</description>
    </item>
    
    <item>
      <title>All Namespace filtering in VM list</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</guid>
      <description> Related issues: #2578 [BUG] When first entering the harvester cluster from Virtualization Managements, some vm&amp;rsquo;s in namespace are not shown in the list Category: UI Verification Steps Create a harvester cluster Create a VM in the default namespace Creating a Namespace (eg: test-vm) Import the Harvester cluster in Rancher access to the harvester cluster from Virtualization Management click Virtual Machines tab Expected Results test-vm-1 should also be shown in the list </description>
    </item>
    
    <item>
      <title>Auto provision lots of extra disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</guid>
      <description>:warning: This is a heuristic test plan since real world race condition is hard to reproduce. If you find any better alternative, feel free to update.
This test is better to perform under QEMU/libvirt environment.
Related issues: #1718 [BUG] Automatic disk provisioning result in unusable ghost disks on NVMe drives Category: Storage Verification Steps Create a harvester cluster and attach 10 or more extra disks (needs WWN so that they can be identified uniquely).</description>
    </item>
    
    <item>
      <title>Boot installer under Legacy BIOS and UEFI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</guid>
      <description>Related issues #2023 Legacy Iso for older servers Verification Steps BIOS Test Build harvester-installer Boot build artifact using BIOS Legacy mode: qemu-system-x86_64 -m 2048 -cdrom ../dist/artifacts/harvester-master-amd64 Verify that the installer boot process reaches the screen that says &amp;ldquo;Create New Cluster&amp;rdquo; or &amp;ldquo;Join existing cluster&amp;rdquo; UEFI Test Build harvester-installer (or use the same one from the BIOS Test, it&amp;rsquo;s a hybrid ISO) Boot build artifact using UEFI mode: qemu-system-x86_64 -m 2048 -cdrom .</description>
    </item>
    
    <item>
      <title>Check can start VM after Harvester upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/start-vm-after-harvester-upgrade-complete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/start-vm-after-harvester-upgrade-complete/</guid>
      <description> Related issues: #2270 [BUG] Unable start VM after upgraded v1.0.1 to v1.0.2-rc2 Category: Harvester Upgrade Verification Steps Prepare the previous stable Harvester release cluster Create image Enable Network and create VM Create several virtual machine Follow the official document steps to prepare the online or offline upgrade Shutdown all virtual machines Start the upgrade Confirm all the upgrade process complete Start all the virtual machines Expected Results All virtual machine could be correctly started and work as expected </description>
    </item>
    
    <item>
      <title>Check conditions when stop/pause VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</guid>
      <description>Related issues: #1987 Verification Steps Stop Request should not have failure message
Create a VM with runStrategy: RunStrategyAlways. Stop the VM. Check there is no Failure attempting to delete VMI: &amp;lt;nil&amp;gt; in VM status. UI should not show pause message
Create a VM. Pause the VM. Although the message The status of pod readliness gate &amp;quot;kubevirt.io/virtual-machine-unpaused&amp;quot; is not &amp;quot;True&amp;quot;, but False is in the VM condition, UI should not show it.</description>
    </item>
    
    <item>
      <title>Check DNS on install with Github SSH keys</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</guid>
      <description>Related issues: #1903 DNS server not available during install Verification Steps Without PXE Start a new install Set DNS as 8.8.8.8 Add in github SSH keys Finish install SSH into node with SSH keys from github (rancher@hostname) Verify login was successful With PXE Got vagrant setup from https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Changed settings.yml DHCP config and added dns: 8.8.8.8 dhcp_server: ip: 192.168.0.254 subnet: 192.168.0.0 netmask: 255.255.255.0 range: 192.168.0.50 192.168.0.130 dns: 8.8.8.8 https: false Also changed ssh_authorized_keys and commented out default SSH key and added username for github</description>
    </item>
    
    <item>
      <title>Check IPAM configuration with IPAM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</guid>
      <description> Related issues: #1697 Optimization for the Harvester load balancer Verification Steps Install the latest rancher and import a Harvester cluster Create a cluster by Harvester node driver Navigate to the workload Page, create a workload Click &amp;ldquo;Add ports&amp;rdquo;, select type as LB, protocol as TCP Check IPAM selector Navigate to the service page, create a LB Click &amp;ldquo;Add-on config&amp;rdquo; tab and check IPAM and port </description>
    </item>
    
    <item>
      <title>Check logs on Harvester</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</guid>
      <description> Related issues: #2528 [BUG] Tons of AppArmor denied messages Category: Logging Environment Setup This should be run on a Harvester node that has been up for a while and has been in use Verification Steps SSH to harvester node Execute journalctl -b -f Look through logs and verify that there isn&amp;rsquo;t anything generating lots of erroneous messages Expected Results There shouldn&amp;rsquo;t be large volumes of erroneous messages </description>
    </item>
    
    <item>
      <title>Check support bundle for SLE Micro OS</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2420-2464-check-support-bundle-sle-micro-os/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2420-2464-check-support-bundle-sle-micro-os/</guid>
      <description>Related issues: #2420 [FEATURE] support bundle: support SLE Micro OS Related issues: #2464 [backport v1.0] [FEATURE] support bundle: support SLE Micro OS Category: Support Bundle Verification Steps Download support bundle in support page Extract the support bundle, check every file have content ssh to harvester node Check the /etc/os-release file content Expected Results Check can download support bundle correctly, check can access every file without empty
Checked every harvester nodes, the ID have changed to sle-micro-rancher</description>
    </item>
    
    <item>
      <title>Check the VM is available when Harvester upgrade failed</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vm-availability-when-harvester-upgrade-failed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/vm-availability-when-harvester-upgrade-failed/</guid>
      <description>Category: Harvester Upgrade Verification Steps Prepare the previous stable Harvester release cluster Create image Enable Network and create VM Create several virtual machine Follow the official document steps to prepare the online or offline upgrade Do not shutdown virtual machine Start the upgrade Check the VM status if the upgrade failed at Preload images, Upgrade Rancher and Upgrade Harvester phase Check the VM status if the upgrade failed at the Pre-drain, Post-drain and RKE2 &amp;amp; OS upgrade phase Expected Results The VM should be work when upgrade failed at Preload images, Upgrade Rancher and Upgrade Harvester phase The VM could not able to function well when upgrade failed at the Pre-drain, Post-drain and RKE2 &amp;amp; OS upgrade phase </description>
    </item>
    
    <item>
      <title>Clone image</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</guid>
      <description> Related issues: #2562 [[BUG] Image&amp;rsquo;s labels will not be copied when execute Clone Category: Images Verification Steps Install Harvester with any nodes Create a Image via URL Clone the Image and named image-b Check image-b labels in Labels tab Expected Results All labels should be cloned and shown in labels tab </description>
    </item>
    
    <item>
      <title>Create multiple VM instances using VM template with EFI mode selected</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2577-create-multiple-vm-using-template-efi-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2577-create-multiple-vm-using-template-efi-mode/</guid>
      <description>Related issues: #2577 [BUG] Boot in EFI mode not selected when creating multiple VM instances using VM template with EFI mode selected. Category: Virtual Machine Verification Steps Create a VM template, check the Booting in EFI mode Create multiple VM instance and use the VM template have Booting in EFI mode checked Wait for all VM running Check the EFI mode is enabled in VM config ssh to each VM Check the /etc/firmware/efi file Expected Results Can create multiple VM instance using VM template with EFI mode selected</description>
    </item>
    
    <item>
      <title>Delete VM template default version</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2376-2379-delete-vm-template-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2376-2379-delete-vm-template-default-version/</guid>
      <description> Related issues: #2376 [BUG] Cannot delete Template Related issues: #2379 [backport v1.0.3] Cannot delete Template Category: VM Template Verification Steps Go to Advanced -&amp;gt; Templates Create a new template Modify the template to create a new version Click the config button of the default version template Click the config button of the non default version template Expected Results If the template is the default version, it will not display the delete button If the template is not the default version, it will display the delete button We can also delete the entire template from the config button </description>
    </item>
    
    <item>
      <title>Deploy guest cluster to specific node with Node selector label</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2316-2384-deploy-guest-cluster-node-selector-label-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2316-2384-deploy-guest-cluster-node-selector-label-copy/</guid>
      <description>Related issues: #2316 [BUG] Guest cluster nodes distributed across failure domain Related issues: #2384 [backport v1.0.3] Guest cluster nodes distributed across failure domains Category: Rancher integration Verification Steps RKE2 Verification Steps Open Harvester Host page then edit host config Add the following key value in the labels page: topology.kubernetes.io/zone: zone_bp topology.kubernetes.io/region: region_bp Open the RKE2 provisioning page Expand the show advanced Click add Node selector in Node scheduling Use default Required priority Click Add Rule Provide the following key/value pairs topology.</description>
    </item>
    
    <item>
      <title>Generate Install Support Config Bundle For Single Node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</guid>
      <description>Related issue: #1864 Support bundle for a single node (Live/Installed)
Related issue: #272 Generate supportconfig for failed installations
Category: Support Environment setup Setup a single node harvester from ISO install but don&amp;rsquo;t complete the installation
Gain SSH Access to the Single Harvester Node Once Shelled into the Single Harvester Node edit the /usr/sbin/harv-install Using: harvester-installer&amp;rsquo;s harv-install as a reference edit around line #362 adding exit 1: exit 1 trap cleanup exit check_iso save the file.</description>
    </item>
    
    <item>
      <title>Harvester pull Rancher agent image from private registry</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2175-2332-harvester-pull-rancher-image-private-registry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2175-2332-harvester-pull-rancher-image-private-registry/</guid>
      <description>Related issues: #2175 [BUG] Harvester fails to pull Rancher agent image from private registry Related issues: #2332 [Backport v1.0] Harvester fails to pull Rancher agent image from private registry Category: Virtual Machine Verification Steps Create a harvester cluster and a ubuntu server. Make sure they can reach each other. On each harvester node, add ubuntu IP to /etc/hosts. # vim /etc/hosts &amp;lt;host ip&amp;gt; myregistry.local On the ubuntu server, install docker and run the following commands.</description>
    </item>
    
    <item>
      <title>Image filtering by labels</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2474-image-filtering-by-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2474-image-filtering-by-labels/</guid>
      <description>Related issues: #2474 [backport v1.0] [FEATURE] Image filtering by labels Category: Image Verification Steps Upload several images and add related label Go to the image list page Add filter according to test plan 1 Go to VM creation page Check the image list and search by name Import Harvester in Rancher Go to cluster management page Create a RKE2 cluster Check the image list and search by name Expected Results The image list page can be filtered by label in the following cases</description>
    </item>
    
    <item>
      <title>Image naming with inline CSS</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</guid>
      <description> Related issues: #2563 [[BUG] harvesterhci.io.virtualmachineimage spec.displayName displays differently in single view of image Category: Images Verification Steps Go to images Click &amp;ldquo;Create&amp;rdquo; Upload an image or leverage an url - but name the image something like: &amp;lt;strong&amp;gt;&amp;lt;em&amp;gt;something_interesting&amp;lt;/em&amp;gt;&amp;lt;/strong&amp;gt; Wait for upload to complete. Observe the display name within the list of images Compare that to clicking into the single image and viewing it Expected Results The list view naming would be the same as the single view of the image </description>
    </item>
    
    <item>
      <title>Image upload does not start when HTTP Proxy is configured</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2436-2524-image-upload-failed-when-http-proxy-configured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2436-2524-image-upload-failed-when-http-proxy-configured/</guid>
      <description>Related issues: #2436 [BUG] Image upload does not start when HTTP Proxy is configured Related issues: #2524 [backport v1.0] [BUG] Image upload does not start when HTTP Proxy is configured Category: Image Verification Steps Clone ipxe-example vagrant project https://github.com/harvester/ipxe-examples Edit settings.yml Set harvester_network_config.offline=true Create a one node air gapped Harvester with a HTTP proxy server Access Harvester settings page Add the following http proxy configuration { &amp;#34;httpProxy&amp;#34;: &amp;#34;http://192.168.0.254:3128&amp;#34;, &amp;#34;httpsProxy&amp;#34;: &amp;#34;http://192.</description>
    </item>
    
    <item>
      <title>Install Harvester over previous GNU/Linux install</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</guid>
      <description> Related issues: #2230 [BUG] harvester installer - always first attempt failed if before was linux installed Related issues: #2450 [backport v1.0][BUG] harvester installer - always first attempt failed if before was linux installed #2450 Category: Installtion Verification Steps Install GNU/LInux LVM configuration reboot Install Harvester via ISO over previous linux install Verifiy Harvester install by changing password and logging in. Expected Results Install should complete </description>
    </item>
    
    <item>
      <title>Limit VM of guest cluster in the same namespace</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2354-limit-vm-of-guest-cluster-same-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2354-limit-vm-of-guest-cluster-same-namespace/</guid>
      <description>Related issues: #2354 [FEATURE] Limit all VMs of the Harvester guest cluster in the same namespace Category: Rancher integration Verification Steps Import Harvester from Rancher Access Harvester via virtualization management Create a test project and ns1 namespace Create two RKE1 node template, one set to default namespace and another set to ns1 namespace Create a RKE1 cluster, select the first pool using the first node template Create another pool, check can&amp;rsquo;t select the second node template Create a RKE2 cluster, set the first pool using specific namespace Add another machine pool, check it will automatically assigned the same namespace as the first pool Expected Results On RKE2 cluster page, when we select the first machine pool to specific namespace, then the second pool will automatically and can only use the same namespace as the first pool</description>
    </item>
    
    <item>
      <title>Local cluster user input topology key</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2567-local-cluster-user-input-topology-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2567-local-cluster-user-input-topology-key/</guid>
      <description> Related issues: #2567 [BUG] Local cluster owner create Harvester cluster failed(RKE2) Category: Rancher integration Verification Steps Import Harvester from Rancher Create a standard user local in Rancher User &amp;amp; Authentication Open Cluster Management page Edit cluster config Expand Member Roles Add local user with Cluster Owner role Create cloud credential of Harvester Login with local user Open the provisioning RKE2 cluster page Select Advanced settings Add Pod Scheduling Select Pods in these namespaces Check can input Topology key value Expected Results Login with cluster owner role and provision a RKE2 cluster we can input the topology key in the Topology key field of the pod selector </description>
    </item>
    
    <item>
      <title>Multiple Disks Swapping Paths</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</guid>
      <description>Related issues: #1874 Multiple Disks Swapping Paths Verification Steps Prepare a harvester cluster (single node is sufficient) Prepare two additional disks and format both of them. Hotplug both disks and add them to the host via Harvester Dashboard (&amp;ldquo;Hosts&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo;) Shutdown the host. Swap the address and slot of the two disks in order to make their dev paths swapped For libvirt environment, you can swap &amp;lt;address&amp;gt; and &amp;lt;target&amp;gt; in the XML of the disk.</description>
    </item>
    
    <item>
      <title>Negative change backup target while restoring backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</guid>
      <description>Related issues: #2560 [BUG] VM hanging on restoring state when backup-target disconnected suddenly Category: Category Verification Steps Install Harvester with any nodes Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3 Create Image for VM creation Create VM vm1 Take Backup vm1b from vm1 Restore the backup vm1b to New/Existing VM When the VM still in restoring state, update backup-target settings to Use the default value then setup it back.</description>
    </item>
    
    <item>
      <title>Negative Harvester installer input same NIC IP and VIP</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</guid>
      <description> Related issues: #2229 [BUG] input nic ip and vip with same ip address in Harvester-Installer Related issues: #2377 [Backport v1.0.3] input nic ip and vip with same ip address in Harvester-Installer Category: Installation Verification Steps Boot into ISO installer Specify same IP for NIC and VIP Expected Results Error message is displayed </description>
    </item>
    
    <item>
      <title>Negative Restore a backup while VM is restoring</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</guid>
      <description>Related issues: #2559 [BUG] Backup unable to be restored and the VM can&amp;rsquo;t be deleted Category: Backup/Restore Verification Steps Install Harvester with any nodes Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3 Create Image for VM creation Create VM vm1 Take backup from vm1 as vm1b Take backup from vm1 as vm1b2 Click Edit YAML of vm1b, update field status.source.spec.spec.domain.cpu.cores, increase 1 Stop VM vm1 Restore backup vm1b2 with Replace Existing Restore backup vm1b with Replace Existing when the VM vm1 still in state restoring Expected Results You should get an error when trying to restore.</description>
    </item>
    
    <item>
      <title>NIC ip and vip can&#39;t be the same in Harvester installer</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2229-2449-nic-ip-vip-different-harvester-installer-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2229-2449-nic-ip-vip-different-harvester-installer-copy/</guid>
      <description> Related issues: #2229 [BUG] input nic ip and vip with same ip address in Harvester-Installer Related issues: #2449 [backport v1.0] [BUG] input nic ip and vip with same ip address in Harvester-Installer Category: Harvester Installer Verification Steps Launch ISO install process Set static node IP and gateway Set the same node IP to the VIP field and press enter
Expected Results During Harvester ISO installer process, when we set static node IP address with the same one as the VIP IP address There will be an error message to prevent the installation process VIP must not be the same as Management NIC IP </description>
    </item>
    
    <item>
      <title>Press the Enter key in setting field shouldn&#39;t refresh page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2569-press-enter-settings-should-not-refresh-page-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2569-press-enter-settings-should-not-refresh-page-copy/</guid>
      <description>Related issues: #2569 [BUG] Press the Enter key, the page will be refreshed automatically Category: Settings Verification Steps Check every page have input filed in the Settings page Move cursor to any input field Click the Enter button Check the page will not be automatically loaded Expected Results On v1.0.3 backport, when we press the Enter key in the following page fields, it will not being refreshed automatically.
Also checked the following pages</description>
    </item>
    
    <item>
      <title>Project owner role on customized project open Harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2394-2395-project-owner-customized-project-open-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2394-2395-project-owner-customized-project-open-harvester/</guid>
      <description> Related issues: #2394 [BUG] Standard rancher user with project owner role of customized project to access Harvester get &amp;ldquo;404 Not Found&amp;rdquo; error Related issues: #2395 [backport v1.0] [BUG] Standard rancher user with project owner role of customized project to access Harvester get &amp;ldquo;404 Not Found&amp;rdquo; error Category: Rancher integration Verification Steps Import Harvester from Rancher Access Harvester on virtualization management page Create a project test and namespace test under it Go to user authentication page Create a stand rancher user test Access Harvester in Rancher Set project owner role of test project to test user Login Rancher with test user Access the virtualization management page Expected Results Now the standard user with project owner role can access harvester in virtualization management page correctly </description>
    </item>
    
    <item>
      <title>Project owner should not see additional alert</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2288-2350-project-owner-should-not-see-alert-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2288-2350-project-owner-should-not-see-alert-copy/</guid>
      <description>Related issues: #2288 [BUG] The project-owner user will see an additional alert Related issues: #2350 [Backport v1.0] The project-owner user will see an additional alert Category: Rancher integration Verification Steps Importing a harvester cluster in a rancher cluster enter the imported harvester cluster from the Virtualization Management page create a new Project (test), Create a test namespace in the test project. go to Network page, add vlan 1 create a vm， choose test namespace, choose vlan network, click save create a new user (test), choose Standard User go to the project page, edit test Project, set test user to Project Owner。 login again with test user go to the vm page Expected Results Use rancher standard user test with project owner permission to access Harvester.</description>
    </item>
    
    <item>
      <title>RBAC Cluster Owner</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</guid>
      <description> Related issues: #2626 [BUG] Access Harvester project/namespace page hangs with no response timeout with local owner role from Rancher Category: Authentication Verification Steps Import Harvester from Rancher Create a standard user local in Rancher User &amp;amp; Authentication Open Cluster Management page Edit cluster config Expand Member Roles Add local user with Cluster Owner role Logout Admin Login with local user Access Harvester from virtualization management Click the Project/Namespace page Expected Results Local owner role user can access and display Harvester project/namespace place correctly without hanging to timeout </description>
    </item>
    
    <item>
      <title>RBAC Create VM with restricted admin user</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</guid>
      <description>Related issues: #2587 [BUG] namespace on create VM is wrong when going through Rancher #2116 [BUG] You can see cattle-monitoring-system volumes as restricted admin in Harvester Category: Authentication Verification Steps Verification Steps Import Harvester into Rancher Create a restricted admin Navigate to Volumes page Verify you only see associated Volumes Log out of admin and log in to restricted admin Navigate to Harvester UI via virtualization management Open virtual machines tab Click create Verified that namespace was default.</description>
    </item>
    
    <item>
      <title>Remove Pod Scheduling from harvester rke2 and rke1</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</guid>
      <description>Related issues: #2642 [BUG] Remove Pod Scheduling from harvester rke2 and rke1 Category: Rancher Test Information Test Environment: 1 node harvester on local kvm machine Harvester version: v1.0-44fb5f1a-head (08/10) Rancher version: v2.6.7-rc7
Environment Setup Prepare Harvester master node Prepare Rancher v2.6.7-rc7 Import Harvester to Rancher Set ui-offline-preferred: Remote Go to Harvester Support page Download Kubeconfig Copy the content of Kubeconfig Verification Steps RKE2 Verification Steps Open Harvester Host page then edit host config Add the following key value in the labels page: topology.</description>
    </item>
    
    <item>
      <title>Restart/Stop VM with in progress Backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</guid>
      <description> Related issues: #1702 Don&amp;rsquo;t allow restart/stop vm when backup is in progress Verification Steps Create a VM. Create a VMBackup for it. Before VMBackup is done, stop/restart the VM. Verify VM can&amp;rsquo;t be stopped/restarted. </description>
    </item>
    
    <item>
      <title>Restricted admin should not see cattle-monitoring-system volumes</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2116-2351-restricted-admin-no-cattle-monitoring-system-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2116-2351-restricted-admin-no-cattle-monitoring-system-volumes/</guid>
      <description>Related issues: #2116 [BUG] You can see cattle-monitoring-system volumes as restricted admin in Harvester Related issues: #2351 [Backport v1.0] You can see cattle-monitoring-system volumes as restricted admin in Harvester Category: Rancher integration Verification Steps Import Harvester to Rancher Create restricted admin in Rancher Log out of rancher Log in as restricted admin Navigate to Harvester ui in virtualization management Navigate to volumes page Expected Results Login Rancher with restricted admin and access Harvester volume page.</description>
    </item>
    
    <item>
      <title>Setup and test local Harvester upgrade responder</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1849-setup-test-local-upgrade-responder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1849-setup-test-local-upgrade-responder/</guid>
      <description>Related issues: #1849 [Task] Improve Harvester upgrade responder Category: Upgrade Verification Steps Follow the steps in https://github.com/harvester/harvester/issues/1849#issuecomment-1180346017
Clone longhorn/upgrade-responder and checkout to v0.1.4. Edit response.json content in config folder { &amp;#34;Versions&amp;#34;: [ { &amp;#34;Name&amp;#34;: &amp;#34;v1.0.2-master-head&amp;#34;, &amp;#34;ReleaseDate&amp;#34;: &amp;#34;2022-06-15T00:00:00Z&amp;#34;, &amp;#34;Tags&amp;#34;: [ &amp;#34;latest&amp;#34;, &amp;#34;test&amp;#34;, &amp;#34;dev&amp;#34; ] } ] } Install InfluxDB Run longhorn/upgrade-responder with the command: go run main.go --debug start --upgrade-response-config config/response.json --influxdb-url http://localhost:8086 --geodb geodb/GeoLite2-City.mmdb --application-name harvester Check the local upgrade responder is running curl -X POST http://localhost:8314/v1/checkupgrade \ -d &amp;#39;{ &amp;#34;appVersion&amp;#34;: &amp;#34;v1.</description>
    </item>
    
    <item>
      <title>template with EFI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</guid>
      <description>Related issues: #2577 [BUG] Boot in EFI mode not selected when creating multiple VM instances using VM template with EFI mode selected. Category: Template Verification Steps Go to Template, create a VM template with Boot in EFI mode selected. Go to Virtual Machines, click Create, select Multiple instance, type in a random name prefix, and select the VM template we just created. Go to Advanced Options, for now this EFI checkbox should be checked without any issue.</description>
    </item>
    
    <item>
      <title>Terraform import VLAN</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</guid>
      <description>Related issues: #2261 [FEATURE] enhance terraform network to not pruge route_cidr and route_gateway Category: Terraform Verification Steps Install Harvester with any nodes Install terraform-harvester-provider (using master-head for testing) Execute terraform init Create the file network.tf as following snippets, then execute terraform import harvester_clusternetwork.vlan vlan to import default vlan settings resource &amp;#34;harvester_clusternetwork&amp;#34; &amp;#34;vlan&amp;#34; { name = &amp;#34;vlan&amp;#34; enable = true default_physical_nic = &amp;#34;harvester-mgmt&amp;#34; } resource &amp;#34;harvester_network&amp;#34; &amp;#34;vlan1&amp;#34; { name = &amp;#34;vlan1&amp;#34; namespace = &amp;#34;harvester-public&amp;#34; vlan_id = 1 route_mode = &amp;#34;auto&amp;#34; } execute terraform apply Login to dashboard then navigate to Advanced/Networks, make sure the Route Connectivity becomes Active Execute terraform apply again and many more times Expected Results Resources should not be changed or added or destroyed.</description>
    </item>
    
    <item>
      <title>Terraformer import KUBECONFIG</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</guid>
      <description>Related issues: #2604 [BUG] Terraformer imported VLAN always be 0 Category: Terraformer Verification Steps Install Harvester with any nodes Login to dashboard, navigate to: Advanced/Settings -&amp;gt; then enabledvlan` Navigate to Advanced/Networks and Create a Network which Vlan ID is not 0 Navigate to Support Page and Download KubeConfig file Initialize a terraform environment, download Harvester Terraformer Execute command terraformer import harvester -r network to generate terraform configuration from the cluster Generated file generated/harvester/network/network.</description>
    </item>
    
    <item>
      <title>Testing Harvester Storage Tiering</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2147-testing-storage-tiering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2147-testing-storage-tiering/</guid>
      <description>Related issues: #2147 [[FEATURE] Storage Tiering Category: Images Volumes VirtualMachines Test Setup Steps Have a Harvester Node with 3 Disks in total (one main disk, two additional disks), ideally the two additional disks should be roughly 20/30Gi for testing Add the additional disks to the harvester node (you may first need to be on the node itself and do a sudo gdisk /dev/sda and then w and y to write the disk identifier so that Harvester can recogonize the disk, note you shouldn&amp;rsquo;t need to build partitions) Add the disks to the Harvester node via: Hosts -&amp;gt; Edit Config -&amp;gt; Storage -&amp;gt; &amp;ldquo;Add Disk&amp;rdquo; (call-to-action), they should auto populate with available disks that you can add Save Navigate back to Hosts -&amp;gt; Host -&amp;gt; Edit Config -&amp;gt; Storage, then add a Host Tag, and a unique disk tag for every disk (including the main disk/default-disk) Verification Steps with Checks Navigate to Advanced -&amp;gt; Storage Classes -&amp;gt; Create (Call-To-Action), create a storageClass &amp;ldquo;sc-a&amp;rdquo;, specify nodeSelector (choose host), diskSelector (choose one of the unique disk tags), number of replicas (1-12) Also create a storageClass &amp;ldquo;sc-b&amp;rdquo;, specify nodeSelector (choose host), diskSelector (choose one of the unique disk tags), number of replicas (1-12) Create a new image img-a, specify storageClassName to sc-a Create a new vm vm1 use the image img-a Check the replicas number and location of rootdisk volume in longhorn UI Create a new volume volume-a by choose source=image img-a Add the volume volume-a to vm vm1 Check the replicas number and location of volume volume-a in longhorn UI: volume-a, should also be seen in kubectl get pv --all-namespaces (where &amp;ldquo;Claim&amp;rdquo; is volume-a) with the appropriate storage class also with something like kubectl describe pv/pvc-your-uuid-from-get-pv-call-with-volume-a --all-namespaces: can audit volume attributes like: VolumeAttributes: diskSelector=second migratable=true nodeSelector=node-2 numberOfReplicas=1 share=true staleReplicaTimeout=30 storage.</description>
    </item>
    
    <item>
      <title>Topology aware scheduling of guest cluster workloads</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1418-2383-topology-scheduling-of-guest-cluster-workloads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1418-2383-topology-scheduling-of-guest-cluster-workloads/</guid>
      <description> Related issues: #1418 [FEATURE] Support topology aware scheduling of guest cluster workloads Related issues: #2383 [backport v1.0.3] [FEATURE] Support topology aware scheduling of guest cluster workloads Category: Rancher integration Verification Steps Environment preparation as above steps Access Harvester node config page Add the following node labels with values topology.kubernetes.io/zone topology.kubernetes.io/region Provision an RKE2 cluster Wait for the provisioning complete Access RKE2 guest cluster Access the RKE2 cluster in Cluster Management page Click + to add another node Access the RKE2 cluster node page Wait until the second node created Edit yaml of the second node Check the harvester node label have propagated to the guest cluster node Expected Results The topology encoded in the Harvester cluster node labels Can be correctly propagated to the additional node of the RKE2 guest cluster </description>
    </item>
    
    <item>
      <title>Upgrade guest cluster kubernetes version can also update the cloud provider chart version</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2546-upgrade-guest-k8s-version-upgrade-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2546-upgrade-guest-k8s-version-upgrade-cloud-provider/</guid>
      <description>Related issues: #2546 [BUG] Harvester Cloud Provider is not able to deploy upgraded container after upgrading the cluster Category: Rancher integration Verification Steps Prepare the previous stable Rancher rc version and Harvester Update rke-metadata-config to {&amp;quot;refresh-interval-minutes&amp;quot;:&amp;quot;1440&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;https://yufa-dev.s3.ap-east-1.amazonaws.com/data.json&amp;quot;} in global settings Update the ui-dashboard-index to https://releases.rancher.com/dashboard/latest/index.html Set ui-offline-preferred to Remote Refresh web page (ctrl + r) Open Create RKE2 cluster page Check the show deprecated kubernetes patched versions Select v1.23.8+rke2r1 Finish the RKE2 cluster provision Check the current cloud provider version in workload page Edit RKE2 cluster, upgrade the kubernetes version to 1.</description>
    </item>
    
    <item>
      <title>VM IP addresses should be labeled per network interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2032-2370-vm-ip-lableled-per-network-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2032-2370-vm-ip-lableled-per-network-interface/</guid>
      <description>Related issues: #2032 [BUG] VM IP addresses should be labeled per network interface Related issues: #2370 [backport v1.0.3] VM IP addresses should be labeled per network interface Category: Virtual Machine Verification Steps Enable network with magement-mgmt interface Create vlan network vlan1 with id 1 Check the IP address on the VM page Create a VM with harvester-mgmt network Import Harvester in Rancher Provision a RKE2 cluster from Rancher Check the IP address on the VM page Expected Results Now the VM list only show IP which related to user access.</description>
    </item>
    
    <item>
      <title>Zero downtime upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</guid>
      <description> Related issues: #1707 [BUG] Zero downtime upgrade stuck in &amp;ldquo;Waiting for VM live-migration or shutdown&amp;hellip;&amp;rdquo; Category: Upgrade Verification Steps Create a ubuntu image from URL Enable Network with management-mgmt Create a virtual network vlan1 with id 1 Setup backup target Create a VM backup Follow the guide to do upgrade test Expected Results Can upgrade correctly with all VMs remain in running </description>
    </item>
    
  </channel>
</rss>
