<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>harvester_e2e_tests.scenarios.test_vm_actions API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.scenarios.test_vm_actions</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) 2021 SUSE LLC
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of version 3 of the GNU General Public License as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.   See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, contact SUSE LLC.
#
# To contact SUSE about this file by physical or electronic mail,
# you may find current contact information at www.suse.com

from harvester_e2e_tests import utils
import polling2
import time
import json
import pytest

pytest_plugins = [
    &#39;harvester_e2e_tests.fixtures.keypair&#39;,
    &#39;harvester_e2e_tests.fixtures.vm&#39;,
    &#39;harvester_e2e_tests.fixtures.volume&#39;,
    &#39;harvester_e2e_tests.fixtures.backuptarget&#39;
]


def backup_restore_migrated_vm(request, admin_session,
                               harvester_api_endpoints,
                               vm_with_volume,
                               backuptarget):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           backup-and-restore-13-Restore Backup for VM that was live migrated
           backup-and-restore-14-Backup Single VM that has been live migrated
            before
    &#34;&#34;&#34;
    backup_json = utils.random_name()
    try:
        vm_name = vm_with_volume[&#39;metadata&#39;][&#39;name&#39;]
        vm_instance_json = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm_with_volume)
        vm_node_before_migrate = vm_instance_json[&#39;status&#39;][&#39;nodeName&#39;]

        resp = admin_session.get(harvester_api_endpoints.get_node % (
            vm_node_before_migrate))

        resp = admin_session.get(harvester_api_endpoints.list_nodes)
        assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (
            resp.content)
        nodes_json = resp.json()[&#39;data&#39;]
        for node in nodes_json:
            if node[&#39;metadata&#39;][&#39;name&#39;] != vm_node_before_migrate:
                node_to_migrate = node[&#39;metadata&#39;][&#39;name&#39;]

        resp = admin_session.put(harvester_api_endpoints.migrate_vm % (
            vm_name),
            json={&#34;nodeName&#34;: node_to_migrate})
        assert resp.status_code == 202, &#39;Failed to migrat VM to host %s&#39; % (
            node_to_migrate)
        # give it some time for the VM to migrate
        time.sleep(120)

        def _check_vm_instance_migrated():
            resp = admin_session.get(
                harvester_api_endpoints.get_vm_instance % (
                    vm_name))
            if resp.status_code == 200:
                resp_json = resp.json()
                if (&#39;status&#39; in resp_json and
                        &#39;migrationState&#39; in resp_json[&#39;status&#39;] and
                        resp_json[&#39;status&#39;][&#39;migrationState&#39;][&#39;completed&#39;]):
                    return True
            return False

        success = polling2.poll(
            _check_vm_instance_migrated,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out as waiting for VM to migrate : %s&#39; % (
            vm_name)
        vmi_json_after_migrate = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm_with_volume)
        vm_node_after_migrate = vmi_json_after_migrate[&#39;status&#39;][&#39;nodeName&#39;]
        assert vm_node_after_migrate != vm_node_before_migrate, (
            &#39;Failed to Migrate as Host remains same. &#39;
            &#39;Node Before Migrate: %s; Node after Migrate: %s&#39; % (
                vm_node_before_migrate, vm_node_after_migrate))
        # Create backup of Live migrated VM
        backup_name = utils.random_name()
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget,
                                             name=backup_name,
                                             vm_name=vm_name)
        # Stop VM
        utils.stop_vm(request, admin_session,
                      harvester_api_endpoints, vm_name)
        # Restore existing VM from backup
        restore_name = utils.random_name()
        utils.restore_vm_backup(request, admin_session,
                                harvester_api_endpoints,
                                name=restore_name,
                                vm_name=vm_name,
                                backup_name=backup_name)
        utils.assert_vm_ready(request, admin_session,
                              harvester_api_endpoints,
                              vm_name, running=True)
        resp = admin_session.get(harvester_api_endpoints.get_vm % (
            vm_name))
        assert resp.status_code == 200, &#39;Failed to get restor VM %s: %s&#39; % (
            vm_name, resp.content)

        restored_vm_json = resp.json()
        restored_vm_instance_json = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, restored_vm_json)
        restored_vm_node = restored_vm_instance_json[&#39;status&#39;][&#39;nodeName&#39;]
        assert restored_vm_node == vm_node_after_migrate, (
            &#39;Node of restored VM not same as Node after VM migration &#39;
            &#39;Node Of Restored VM: %s; VM Node after Migrate: %s&#39; % (
                restored_vm_node, vm_node_after_migrate))
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if vm_with_volume:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, vm_with_volume)
            if backup_json:
                utils.delete_vm_backup(request, admin_session,
                                       harvester_api_endpoints,
                                       backuptarget, backup_json)


def update_backup_yaml(request, admin_session,
                       harvester_api_endpoints,
                       basic_vm,
                       backuptarget):
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    utils.lookup_vm_instance(
        admin_session, harvester_api_endpoints, basic_vm)
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget,
                                             name=backup_name,
                                             vm_name=vm_name)
        backup_json[&#39;metadata&#39;][&#39;annotations&#39;] = {
            &#39;test.harvesterhci.io&#39;: &#39;for-test-update&#39;
        }
        resp = utils.poll_for_update_resource(
            request, admin_session,
            harvester_api_endpoints.update_vm_backup % (
                backup_json[&#39;metadata&#39;][&#39;name&#39;]),
            backup_json,
            harvester_api_endpoints.get_vm_backup % (
                backup_json[&#39;metadata&#39;][&#39;name&#39;]),
            use_yaml=True)
        updated_backup_data = resp.json()
        assert updated_backup_data[&#39;metadata&#39;][&#39;annotations&#39;].get(
            &#39;test.harvesterhci.io&#39;) == &#39;for-test-update&#39;
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget, backup_json)


@pytest.mark.virtual_machines_p1
@pytest.mark.p1
class TestVMActions:
    &#34;&#34;&#34;
    Test Virtual Machines opertions like restart,stop,start,pause,
    unpause
    Covers:
        virtual-machines-55-VM operations stop,start,restart,pause,unpause
        virtual-machines-50-VM Edit VM via YAML with CPU
    &#34;&#34;&#34;
    def test_create_vm(self, admin_session, harvester_api_endpoints, basic_vm):
        # make sure the VM instance is successfully created
        utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, basic_vm)
        # make sure it has a cdrom device
        devices = basic_vm[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;][&#39;devices&#39;]
        disks = devices[&#39;disks&#39;]
        found_cdrom = False
        for disk in disks:
            if &#39;cdrom&#39; in disk:
                found_cdrom = True
                break
        assert found_cdrom, &#39;Expecting &#34;cdrom&#34; in the disks list.&#39;

    def test_restart_vm(self, request, admin_session, harvester_api_endpoints,
                        basic_vm):
        vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
        previous_uid = basic_vm[&#39;metadata&#39;][&#39;uid&#39;]
        utils.restart_vm(admin_session, harvester_api_endpoints, previous_uid,
                         vm_name, request.config.getoption(&#39;--wait-timeout&#39;))

    def test_stop_vm(self, request, admin_session, harvester_api_endpoints,
                     basic_vm):
        vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
        utils.stop_vm(request, admin_session, harvester_api_endpoints,
                      vm_name)

    def test_start_vm(self, request, admin_session, harvester_api_endpoints,
                      basic_vm):
        # NOTE: this step must be done after VM has stopped
        resp = admin_session.put(harvester_api_endpoints.start_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 202, (
            &#39;Failed to start VM instance %s: %s&#39; % (
                basic_vm[&#39;metadata&#39;][&#39;name&#39;], resp.content))

        # give it some time for the VM to start
        time.sleep(120)

        def _check_vm_instance_started():
            resp = admin_session.get(
                harvester_api_endpoints.get_vm_instance % (
                    basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
            if resp.status_code == 200:
                resp_json = resp.json()
                if (&#39;status&#39; in resp_json and
                        resp_json[&#39;status&#39;][&#39;phase&#39;] == &#39;Running&#39;):
                    return True
            return False

        success = polling2.poll(
            _check_vm_instance_started,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Failed to get VM instance for: %s&#39; % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;])

    def test_pause_vm(self, request, admin_session, harvester_api_endpoints,
                      basic_vm):
        resp = admin_session.put(harvester_api_endpoints.pause_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to pause VM instance %s&#39; % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;])

        # give it some time for the VM to pause
        time.sleep(60)

        def _check_vm_instance_paused():
            resp = admin_session.get(harvester_api_endpoints.get_vm % (
                basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
            if resp.status_code == 200:
                resp_json = resp.json()
                if &#39;status&#39; in resp_json:
                    for condition in resp_json[&#39;status&#39;][&#39;conditions&#39;]:
                        if (condition[&#39;type&#39;] == &#39;Paused&#39; and
                                condition[&#39;status&#39;] == &#39;True&#39;):
                            return True
            return False

        success = polling2.poll(
            _check_vm_instance_paused,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out while waiting for VM to be paused.&#39;

    def test_unpause_vm(self, request, admin_session, harvester_api_endpoints,
                        basic_vm):
        # NOTE: make sure to execute this step after _paused_vm()
        resp = admin_session.put(harvester_api_endpoints.unpause_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to unpause VM instance %s&#39; % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;])

        # give it some time to unpause
        time.sleep(10)

        def _check_vm_instance_unpaused():
            resp = admin_session.get(harvester_api_endpoints.get_vm % (
                basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
            if resp.status_code == 200:
                resp_json = resp.json()
                if (&#39;status&#39; in resp_json and
                        &#39;ready&#39; in resp_json[&#39;status&#39;] and
                        resp_json[&#39;status&#39;][&#39;ready&#39;]):
                    return True
            return False

        success = polling2.poll(
            _check_vm_instance_unpaused,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out while waiting for VM to be unpaused.&#39;

    def test_update_vm_cpu(self, request, admin_session,
                           harvester_api_endpoints, basic_vm):
        vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
        vm_instance_json = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, basic_vm)
        previous_uid = vm_instance_json[&#39;metadata&#39;][&#39;uid&#39;]
        domain_data = basic_vm[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;]
        updated_cores = domain_data[&#39;cpu&#39;][&#39;cores&#39;] + 1
        domain_data[&#39;cpu&#39;][&#39;cores&#39;] = updated_cores

        resp = utils.poll_for_update_resource(
            request, admin_session,
            harvester_api_endpoints.update_vm % (vm_name),
            basic_vm,
            harvester_api_endpoints.get_vm % (vm_name))
        updated_vm_data = resp.json()
        updated_domain_data = (
            updated_vm_data[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;])
        assert updated_domain_data[&#39;cpu&#39;][&#39;cores&#39;] == updated_cores
        # restart the VM instance for the changes to take effect
        utils.restart_vm(admin_session, harvester_api_endpoints, previous_uid,
                         vm_name, request.config.getoption(&#39;--wait-timeout&#39;))


@pytest.mark.volumes_p2
@pytest.mark.volumes_p1
@pytest.mark.p2
@pytest.mark.p1
class TestVMVolumes:

    def test_create_vm_with_external_volume(self, admin_session,
                                            harvester_api_endpoints,
                                            vm_with_volume):
        &#34;&#34;&#34;
        Test virtual machines
        Covers:
        virtual-machines-11-Create VM with two disk volumes
        &#34;&#34;&#34;
        # make sure the VM instance is successfully created
        utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm_with_volume)
        # make sure it&#39;s data volumes are in-use
        volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for volume in volumes:
            resp = admin_session.get(harvester_api_endpoints.get_volume % (
                volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))
            assert resp.status_code == 200, (
                &#39;Failed to lookup volume %s: %s&#39; % (
                    volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;],
                    resp.content))
            volume_json = resp.json()
            owned_by = json.loads(
                volume_json[&#39;metadata&#39;][&#39;annotations&#39;].get(
                    &#39;harvesterhci.io/owned-by&#39;))
            expected_owner = &#39;%s/%s&#39; % (
                vm_with_volume[&#39;metadata&#39;][&#39;namespace&#39;],
                vm_with_volume[&#39;metadata&#39;][&#39;name&#39;])
            # make sure VM is one of the owners
            found = False
            for owner in owned_by:
                if (owner[&#39;schema&#39;] == &#39;kubevirt.io.virtualmachine&#39; and
                        expected_owner in owner[&#39;refs&#39;]):
                    found = True
                    break
            assert found, (&#39;Expecting %s to be in volume %s owners list&#39; % (
                expected_owner, volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))

    def test_delete_volume_in_use(self, request, admin_session,
                                  harvester_api_endpoints, vm_with_volume):
        &#34;&#34;&#34;
        Volume testing
        Covers:
            Negative vol-01-Delete Volume that is in use
            vol-13-Validate volume shows as in use when attached
        &#34;&#34;&#34;
        volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for volume in volumes:
            # try to delete a volume in &#39;in-use&#39; state and it should
            # fail
            resp = admin_session.delete(
                harvester_api_endpoints.delete_volume % (
                    volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))
            assert resp.status_code not in [200, 201], (
                &#39;Deleting &#34;in-use&#34; volumes should not be permitted: %s&#39; % (
                    resp.content))

    def test_delete_vm_then_volumes(self, request, admin_session,
                                    harvester_api_endpoints,
                                    vm_with_volume, volume):
        &#34;&#34;&#34;
        Volume testing
        Covers:
            vol-15-Delete volume that was attached to VM but now is not
        &#34;&#34;&#34;
        # delete the VM but keep the volumes
        utils.delete_vm(request, admin_session, harvester_api_endpoints,
                        vm_with_volume, remove_all_disks=False)
        volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for data_vol in volumes:
            volume_name = data_vol[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            resp = admin_session.get(harvester_api_endpoints.get_volume % (
                volume_name))
            assert resp.status_code == 200, (
                &#39;Failed to lookup data volume %s: %s&#39; % (
                    volume_name, resp.content))
            # now cleanup the volume
            utils.delete_volume_by_name(request, admin_session,
                                        harvester_api_endpoints, volume_name)


@pytest.mark.backups3
@pytest.mark.backup_and_restore_p1
@pytest.mark.p1
def test_backup_single_vm_s3(request, admin_session,
                             harvester_api_endpoints, basic_vm,
                             backuptarget_s3):
    &#34;&#34;&#34;
    Backup and Restore
        Covers:
       backup-and-restore-02-Backup Single VM s3
       backup-and-restore-07-Delete single Backup
       backup-and-restore-01-create backup target
    &#34;&#34;&#34;
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget_s3,
                                             name=backup_name,
                                             vm_name=vm_name)
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget_s3, backup_json)


@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p1
@pytest.mark.p1
def test_backup_single_vm_nfs(request, admin_session,
                              harvester_api_endpoints, basic_vm,
                              backuptarget_nfs):
    &#34;&#34;&#34;
    Backup and Restore
        Covers:
        vol-02-Backup Single VM nfs
    &#34;&#34;&#34;
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget_nfs,
                                             name=backup_name,
                                             vm_name=vm_name)
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget_nfs, backup_json)


@pytest.mark.skip(&#34;https://github.com/harvester/harvester/issues/1473&#34;)
@pytest.mark.backups3
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_backup_restore_migrated_vm_s3(request, admin_session,
                                       harvester_api_endpoints,
                                       vm_with_volume,
                                       backuptarget_s3):
    &#34;&#34;&#34;
    Backup and Restore
    Covers:
       backup-and-restore-13-Restore Backup S3 for VM that was
       live migrated
       backup-and-restore-14-Backup single vm S3 for VM that was
       live migrated before
    &#34;&#34;&#34;
    backup_restore_migrated_vm(request, admin_session,
                               harvester_api_endpoints,
                               vm_with_volume,
                               backuptarget_s3)


@pytest.mark.skip(&#34;https://github.com/harvester/harvester/issues/1473&#34;)
@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_backup_restore_migrated_vm_nfs(request, admin_session,
                                        harvester_api_endpoints,
                                        vm_with_volume,
                                        backuptarget_nfs):
    &#34;&#34;&#34;
    Backup and Restore
    Covers:
       backup-and-restore-13-Restore Backup nfs for VM that was
       live migrated
       backup-and-restore-14-Backup single vm nfs for VM that was
       live migrated before
    &#34;&#34;&#34;
    backup_restore_migrated_vm(request, admin_session,
                               harvester_api_endpoints,
                               vm_with_volume,
                               backuptarget_nfs)


@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_update_backup_yaml_nfs(request, admin_session,
                                harvester_api_endpoints, basic_vm,
                                backuptarget_nfs):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           backup-and-restore-11-Edit Backup nfs
    &#34;&#34;&#34;
    update_backup_yaml(request, admin_session,
                       harvester_api_endpoints,
                       basic_vm,
                       backuptarget_nfs)


@pytest.mark.backups3
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_update_backup_yaml_s3(request, admin_session,
                               harvester_api_endpoints, basic_vm,
                               backuptarget_s3):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           backup-and-restore-11-Edit Backup s3
    &#34;&#34;&#34;
    update_backup_yaml(request, admin_session,
                       harvester_api_endpoints,
                       basic_vm,
                       backuptarget_s3)


@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p1
@pytest.mark.p1
def test_restore_backup_vm_on(request, admin_session,
                              harvester_api_endpoints,
                              basic_vm, backuptarget_nfs):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           Negative backup-and-restore-08-Restore Backup Negative
    &#34;&#34;&#34;
    # make sure the VM instance is successfully created
    vm_instance_json = utils.lookup_vm_instance(
        admin_session, harvester_api_endpoints, basic_vm)
    vm_name = vm_instance_json[&#39;metadata&#39;][&#39;name&#39;]
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget_nfs,
                                             name=backup_name,
                                             vm_name=vm_name)
        restore_name = utils.random_name()
        request_json = utils.get_json_object_from_template(
            &#39;basic_vm_restore&#39;,
            name=restore_name,
            vm_name=vm_name,
            backup_name=backup_name
        )
        resp = admin_session.post(
            harvester_api_endpoints.create_vm_restore,
            json=request_json)
        content = resp.json()
        assert &#39;please stop the VM&#39; in content[&#39;message&#39;]
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget_nfs, backup_json)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.backup_restore_migrated_vm"><code class="name flex">
<span>def <span class="ident">backup_restore_migrated_vm</span></span>(<span>request, admin_session, harvester_api_endpoints, vm_with_volume, backuptarget)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup Restore Testing</p>
<h2 id="covers">Covers</h2>
<p>backup-and-restore-13-Restore Backup for VM that was live migrated
backup-and-restore-14-Backup Single VM that has been live migrated
before</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_restore_migrated_vm(request, admin_session,
                               harvester_api_endpoints,
                               vm_with_volume,
                               backuptarget):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           backup-and-restore-13-Restore Backup for VM that was live migrated
           backup-and-restore-14-Backup Single VM that has been live migrated
            before
    &#34;&#34;&#34;
    backup_json = utils.random_name()
    try:
        vm_name = vm_with_volume[&#39;metadata&#39;][&#39;name&#39;]
        vm_instance_json = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm_with_volume)
        vm_node_before_migrate = vm_instance_json[&#39;status&#39;][&#39;nodeName&#39;]

        resp = admin_session.get(harvester_api_endpoints.get_node % (
            vm_node_before_migrate))

        resp = admin_session.get(harvester_api_endpoints.list_nodes)
        assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (
            resp.content)
        nodes_json = resp.json()[&#39;data&#39;]
        for node in nodes_json:
            if node[&#39;metadata&#39;][&#39;name&#39;] != vm_node_before_migrate:
                node_to_migrate = node[&#39;metadata&#39;][&#39;name&#39;]

        resp = admin_session.put(harvester_api_endpoints.migrate_vm % (
            vm_name),
            json={&#34;nodeName&#34;: node_to_migrate})
        assert resp.status_code == 202, &#39;Failed to migrat VM to host %s&#39; % (
            node_to_migrate)
        # give it some time for the VM to migrate
        time.sleep(120)

        def _check_vm_instance_migrated():
            resp = admin_session.get(
                harvester_api_endpoints.get_vm_instance % (
                    vm_name))
            if resp.status_code == 200:
                resp_json = resp.json()
                if (&#39;status&#39; in resp_json and
                        &#39;migrationState&#39; in resp_json[&#39;status&#39;] and
                        resp_json[&#39;status&#39;][&#39;migrationState&#39;][&#39;completed&#39;]):
                    return True
            return False

        success = polling2.poll(
            _check_vm_instance_migrated,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out as waiting for VM to migrate : %s&#39; % (
            vm_name)
        vmi_json_after_migrate = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm_with_volume)
        vm_node_after_migrate = vmi_json_after_migrate[&#39;status&#39;][&#39;nodeName&#39;]
        assert vm_node_after_migrate != vm_node_before_migrate, (
            &#39;Failed to Migrate as Host remains same. &#39;
            &#39;Node Before Migrate: %s; Node after Migrate: %s&#39; % (
                vm_node_before_migrate, vm_node_after_migrate))
        # Create backup of Live migrated VM
        backup_name = utils.random_name()
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget,
                                             name=backup_name,
                                             vm_name=vm_name)
        # Stop VM
        utils.stop_vm(request, admin_session,
                      harvester_api_endpoints, vm_name)
        # Restore existing VM from backup
        restore_name = utils.random_name()
        utils.restore_vm_backup(request, admin_session,
                                harvester_api_endpoints,
                                name=restore_name,
                                vm_name=vm_name,
                                backup_name=backup_name)
        utils.assert_vm_ready(request, admin_session,
                              harvester_api_endpoints,
                              vm_name, running=True)
        resp = admin_session.get(harvester_api_endpoints.get_vm % (
            vm_name))
        assert resp.status_code == 200, &#39;Failed to get restor VM %s: %s&#39; % (
            vm_name, resp.content)

        restored_vm_json = resp.json()
        restored_vm_instance_json = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, restored_vm_json)
        restored_vm_node = restored_vm_instance_json[&#39;status&#39;][&#39;nodeName&#39;]
        assert restored_vm_node == vm_node_after_migrate, (
            &#39;Node of restored VM not same as Node after VM migration &#39;
            &#39;Node Of Restored VM: %s; VM Node after Migrate: %s&#39; % (
                restored_vm_node, vm_node_after_migrate))
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if vm_with_volume:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, vm_with_volume)
            if backup_json:
                utils.delete_vm_backup(request, admin_session,
                                       harvester_api_endpoints,
                                       backuptarget, backup_json)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_restore_migrated_vm_nfs"><code class="name flex">
<span>def <span class="ident">test_backup_restore_migrated_vm_nfs</span></span>(<span>request, admin_session, harvester_api_endpoints, vm_with_volume, backuptarget_nfs)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup and Restore</p>
<h2 id="covers">Covers</h2>
<p>backup-and-restore-13-Restore Backup nfs for VM that was
live migrated
backup-and-restore-14-Backup single vm nfs for VM that was
live migrated before</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(&#34;https://github.com/harvester/harvester/issues/1473&#34;)
@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_backup_restore_migrated_vm_nfs(request, admin_session,
                                        harvester_api_endpoints,
                                        vm_with_volume,
                                        backuptarget_nfs):
    &#34;&#34;&#34;
    Backup and Restore
    Covers:
       backup-and-restore-13-Restore Backup nfs for VM that was
       live migrated
       backup-and-restore-14-Backup single vm nfs for VM that was
       live migrated before
    &#34;&#34;&#34;
    backup_restore_migrated_vm(request, admin_session,
                               harvester_api_endpoints,
                               vm_with_volume,
                               backuptarget_nfs)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_restore_migrated_vm_s3"><code class="name flex">
<span>def <span class="ident">test_backup_restore_migrated_vm_s3</span></span>(<span>request, admin_session, harvester_api_endpoints, vm_with_volume, backuptarget_s3)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup and Restore</p>
<h2 id="covers">Covers</h2>
<p>backup-and-restore-13-Restore Backup S3 for VM that was
live migrated
backup-and-restore-14-Backup single vm S3 for VM that was
live migrated before</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(&#34;https://github.com/harvester/harvester/issues/1473&#34;)
@pytest.mark.backups3
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_backup_restore_migrated_vm_s3(request, admin_session,
                                       harvester_api_endpoints,
                                       vm_with_volume,
                                       backuptarget_s3):
    &#34;&#34;&#34;
    Backup and Restore
    Covers:
       backup-and-restore-13-Restore Backup S3 for VM that was
       live migrated
       backup-and-restore-14-Backup single vm S3 for VM that was
       live migrated before
    &#34;&#34;&#34;
    backup_restore_migrated_vm(request, admin_session,
                               harvester_api_endpoints,
                               vm_with_volume,
                               backuptarget_s3)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_single_vm_nfs"><code class="name flex">
<span>def <span class="ident">test_backup_single_vm_nfs</span></span>(<span>request, admin_session, harvester_api_endpoints, basic_vm, backuptarget_nfs)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup and Restore
Covers:
vol-02-Backup Single VM nfs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p1
@pytest.mark.p1
def test_backup_single_vm_nfs(request, admin_session,
                              harvester_api_endpoints, basic_vm,
                              backuptarget_nfs):
    &#34;&#34;&#34;
    Backup and Restore
        Covers:
        vol-02-Backup Single VM nfs
    &#34;&#34;&#34;
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget_nfs,
                                             name=backup_name,
                                             vm_name=vm_name)
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget_nfs, backup_json)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_single_vm_s3"><code class="name flex">
<span>def <span class="ident">test_backup_single_vm_s3</span></span>(<span>request, admin_session, harvester_api_endpoints, basic_vm, backuptarget_s3)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup and Restore
Covers:
backup-and-restore-02-Backup Single VM s3
backup-and-restore-07-Delete single Backup
backup-and-restore-01-create backup target</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.backups3
@pytest.mark.backup_and_restore_p1
@pytest.mark.p1
def test_backup_single_vm_s3(request, admin_session,
                             harvester_api_endpoints, basic_vm,
                             backuptarget_s3):
    &#34;&#34;&#34;
    Backup and Restore
        Covers:
       backup-and-restore-02-Backup Single VM s3
       backup-and-restore-07-Delete single Backup
       backup-and-restore-01-create backup target
    &#34;&#34;&#34;
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget_s3,
                                             name=backup_name,
                                             vm_name=vm_name)
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget_s3, backup_json)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.test_restore_backup_vm_on"><code class="name flex">
<span>def <span class="ident">test_restore_backup_vm_on</span></span>(<span>request, admin_session, harvester_api_endpoints, basic_vm, backuptarget_nfs)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup Restore Testing</p>
<h2 id="covers">Covers</h2>
<p>Negative backup-and-restore-08-Restore Backup Negative</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p1
@pytest.mark.p1
def test_restore_backup_vm_on(request, admin_session,
                              harvester_api_endpoints,
                              basic_vm, backuptarget_nfs):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           Negative backup-and-restore-08-Restore Backup Negative
    &#34;&#34;&#34;
    # make sure the VM instance is successfully created
    vm_instance_json = utils.lookup_vm_instance(
        admin_session, harvester_api_endpoints, basic_vm)
    vm_name = vm_instance_json[&#39;metadata&#39;][&#39;name&#39;]
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget_nfs,
                                             name=backup_name,
                                             vm_name=vm_name)
        restore_name = utils.random_name()
        request_json = utils.get_json_object_from_template(
            &#39;basic_vm_restore&#39;,
            name=restore_name,
            vm_name=vm_name,
            backup_name=backup_name
        )
        resp = admin_session.post(
            harvester_api_endpoints.create_vm_restore,
            json=request_json)
        content = resp.json()
        assert &#39;please stop the VM&#39; in content[&#39;message&#39;]
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget_nfs, backup_json)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.test_update_backup_yaml_nfs"><code class="name flex">
<span>def <span class="ident">test_update_backup_yaml_nfs</span></span>(<span>request, admin_session, harvester_api_endpoints, basic_vm, backuptarget_nfs)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup Restore Testing</p>
<h2 id="covers">Covers</h2>
<p>backup-and-restore-11-Edit Backup nfs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.backupnfs
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_update_backup_yaml_nfs(request, admin_session,
                                harvester_api_endpoints, basic_vm,
                                backuptarget_nfs):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           backup-and-restore-11-Edit Backup nfs
    &#34;&#34;&#34;
    update_backup_yaml(request, admin_session,
                       harvester_api_endpoints,
                       basic_vm,
                       backuptarget_nfs)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.test_update_backup_yaml_s3"><code class="name flex">
<span>def <span class="ident">test_update_backup_yaml_s3</span></span>(<span>request, admin_session, harvester_api_endpoints, basic_vm, backuptarget_s3)</span>
</code></dt>
<dd>
<div class="desc"><p>Backup Restore Testing</p>
<h2 id="covers">Covers</h2>
<p>backup-and-restore-11-Edit Backup s3</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.backups3
@pytest.mark.backup_and_restore_p2
@pytest.mark.p2
def test_update_backup_yaml_s3(request, admin_session,
                               harvester_api_endpoints, basic_vm,
                               backuptarget_s3):
    &#34;&#34;&#34;
        Backup Restore Testing
        Covers:
           backup-and-restore-11-Edit Backup s3
    &#34;&#34;&#34;
    update_backup_yaml(request, admin_session,
                       harvester_api_endpoints,
                       basic_vm,
                       backuptarget_s3)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.update_backup_yaml"><code class="name flex">
<span>def <span class="ident">update_backup_yaml</span></span>(<span>request, admin_session, harvester_api_endpoints, basic_vm, backuptarget)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_backup_yaml(request, admin_session,
                       harvester_api_endpoints,
                       basic_vm,
                       backuptarget):
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    utils.lookup_vm_instance(
        admin_session, harvester_api_endpoints, basic_vm)
    backup_name = utils.random_name()
    backup_json = None
    try:
        backup_json = utils.create_vm_backup(request, admin_session,
                                             harvester_api_endpoints,
                                             backuptarget,
                                             name=backup_name,
                                             vm_name=vm_name)
        backup_json[&#39;metadata&#39;][&#39;annotations&#39;] = {
            &#39;test.harvesterhci.io&#39;: &#39;for-test-update&#39;
        }
        resp = utils.poll_for_update_resource(
            request, admin_session,
            harvester_api_endpoints.update_vm_backup % (
                backup_json[&#39;metadata&#39;][&#39;name&#39;]),
            backup_json,
            harvester_api_endpoints.get_vm_backup % (
                backup_json[&#39;metadata&#39;][&#39;name&#39;]),
            use_yaml=True)
        updated_backup_data = resp.json()
        assert updated_backup_data[&#39;metadata&#39;][&#39;annotations&#39;].get(
            &#39;test.harvesterhci.io&#39;) == &#39;for-test-update&#39;
    finally:
        if not request.config.getoption(&#39;--do-not-cleanup&#39;):
            if backup_json:
                utils.delete_vm(request, admin_session,
                                harvester_api_endpoints, basic_vm)
                utils.delete_vm_backup(
                    request, admin_session, harvester_api_endpoints,
                    backuptarget, backup_json)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions"><code class="flex name class">
<span>class <span class="ident">TestVMActions</span></span>
</code></dt>
<dd>
<div class="desc"><p>Test Virtual Machines opertions like restart,stop,start,pause,
unpause</p>
<h2 id="covers">Covers</h2>
<p>virtual-machines-55-VM operations stop,start,restart,pause,unpause
virtual-machines-50-VM Edit VM via YAML with CPU</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.virtual_machines_p1
@pytest.mark.p1
class TestVMActions:
    &#34;&#34;&#34;
    Test Virtual Machines opertions like restart,stop,start,pause,
    unpause
    Covers:
        virtual-machines-55-VM operations stop,start,restart,pause,unpause
        virtual-machines-50-VM Edit VM via YAML with CPU
    &#34;&#34;&#34;
    def test_create_vm(self, admin_session, harvester_api_endpoints, basic_vm):
        # make sure the VM instance is successfully created
        utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, basic_vm)
        # make sure it has a cdrom device
        devices = basic_vm[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;][&#39;devices&#39;]
        disks = devices[&#39;disks&#39;]
        found_cdrom = False
        for disk in disks:
            if &#39;cdrom&#39; in disk:
                found_cdrom = True
                break
        assert found_cdrom, &#39;Expecting &#34;cdrom&#34; in the disks list.&#39;

    def test_restart_vm(self, request, admin_session, harvester_api_endpoints,
                        basic_vm):
        vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
        previous_uid = basic_vm[&#39;metadata&#39;][&#39;uid&#39;]
        utils.restart_vm(admin_session, harvester_api_endpoints, previous_uid,
                         vm_name, request.config.getoption(&#39;--wait-timeout&#39;))

    def test_stop_vm(self, request, admin_session, harvester_api_endpoints,
                     basic_vm):
        vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
        utils.stop_vm(request, admin_session, harvester_api_endpoints,
                      vm_name)

    def test_start_vm(self, request, admin_session, harvester_api_endpoints,
                      basic_vm):
        # NOTE: this step must be done after VM has stopped
        resp = admin_session.put(harvester_api_endpoints.start_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 202, (
            &#39;Failed to start VM instance %s: %s&#39; % (
                basic_vm[&#39;metadata&#39;][&#39;name&#39;], resp.content))

        # give it some time for the VM to start
        time.sleep(120)

        def _check_vm_instance_started():
            resp = admin_session.get(
                harvester_api_endpoints.get_vm_instance % (
                    basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
            if resp.status_code == 200:
                resp_json = resp.json()
                if (&#39;status&#39; in resp_json and
                        resp_json[&#39;status&#39;][&#39;phase&#39;] == &#39;Running&#39;):
                    return True
            return False

        success = polling2.poll(
            _check_vm_instance_started,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Failed to get VM instance for: %s&#39; % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;])

    def test_pause_vm(self, request, admin_session, harvester_api_endpoints,
                      basic_vm):
        resp = admin_session.put(harvester_api_endpoints.pause_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to pause VM instance %s&#39; % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;])

        # give it some time for the VM to pause
        time.sleep(60)

        def _check_vm_instance_paused():
            resp = admin_session.get(harvester_api_endpoints.get_vm % (
                basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
            if resp.status_code == 200:
                resp_json = resp.json()
                if &#39;status&#39; in resp_json:
                    for condition in resp_json[&#39;status&#39;][&#39;conditions&#39;]:
                        if (condition[&#39;type&#39;] == &#39;Paused&#39; and
                                condition[&#39;status&#39;] == &#39;True&#39;):
                            return True
            return False

        success = polling2.poll(
            _check_vm_instance_paused,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out while waiting for VM to be paused.&#39;

    def test_unpause_vm(self, request, admin_session, harvester_api_endpoints,
                        basic_vm):
        # NOTE: make sure to execute this step after _paused_vm()
        resp = admin_session.put(harvester_api_endpoints.unpause_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to unpause VM instance %s&#39; % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;])

        # give it some time to unpause
        time.sleep(10)

        def _check_vm_instance_unpaused():
            resp = admin_session.get(harvester_api_endpoints.get_vm % (
                basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
            if resp.status_code == 200:
                resp_json = resp.json()
                if (&#39;status&#39; in resp_json and
                        &#39;ready&#39; in resp_json[&#39;status&#39;] and
                        resp_json[&#39;status&#39;][&#39;ready&#39;]):
                    return True
            return False

        success = polling2.poll(
            _check_vm_instance_unpaused,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out while waiting for VM to be unpaused.&#39;

    def test_update_vm_cpu(self, request, admin_session,
                           harvester_api_endpoints, basic_vm):
        vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
        vm_instance_json = utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, basic_vm)
        previous_uid = vm_instance_json[&#39;metadata&#39;][&#39;uid&#39;]
        domain_data = basic_vm[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;]
        updated_cores = domain_data[&#39;cpu&#39;][&#39;cores&#39;] + 1
        domain_data[&#39;cpu&#39;][&#39;cores&#39;] = updated_cores

        resp = utils.poll_for_update_resource(
            request, admin_session,
            harvester_api_endpoints.update_vm % (vm_name),
            basic_vm,
            harvester_api_endpoints.get_vm % (vm_name))
        updated_vm_data = resp.json()
        updated_domain_data = (
            updated_vm_data[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;])
        assert updated_domain_data[&#39;cpu&#39;][&#39;cores&#39;] == updated_cores
        # restart the VM instance for the changes to take effect
        utils.restart_vm(admin_session, harvester_api_endpoints, previous_uid,
                         vm_name, request.config.getoption(&#39;--wait-timeout&#39;))</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_create_vm"><code class="name flex">
<span>def <span class="ident">test_create_vm</span></span>(<span>self, admin_session, harvester_api_endpoints, basic_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_create_vm(self, admin_session, harvester_api_endpoints, basic_vm):
    # make sure the VM instance is successfully created
    utils.lookup_vm_instance(
        admin_session, harvester_api_endpoints, basic_vm)
    # make sure it has a cdrom device
    devices = basic_vm[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;][&#39;devices&#39;]
    disks = devices[&#39;disks&#39;]
    found_cdrom = False
    for disk in disks:
        if &#39;cdrom&#39; in disk:
            found_cdrom = True
            break
    assert found_cdrom, &#39;Expecting &#34;cdrom&#34; in the disks list.&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_pause_vm"><code class="name flex">
<span>def <span class="ident">test_pause_vm</span></span>(<span>self, request, admin_session, harvester_api_endpoints, basic_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_pause_vm(self, request, admin_session, harvester_api_endpoints,
                  basic_vm):
    resp = admin_session.put(harvester_api_endpoints.pause_vm % (
        basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code == 200, &#39;Failed to pause VM instance %s&#39; % (
        basic_vm[&#39;metadata&#39;][&#39;name&#39;])

    # give it some time for the VM to pause
    time.sleep(60)

    def _check_vm_instance_paused():
        resp = admin_session.get(harvester_api_endpoints.get_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        if resp.status_code == 200:
            resp_json = resp.json()
            if &#39;status&#39; in resp_json:
                for condition in resp_json[&#39;status&#39;][&#39;conditions&#39;]:
                    if (condition[&#39;type&#39;] == &#39;Paused&#39; and
                            condition[&#39;status&#39;] == &#39;True&#39;):
                        return True
        return False

    success = polling2.poll(
        _check_vm_instance_paused,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for VM to be paused.&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_restart_vm"><code class="name flex">
<span>def <span class="ident">test_restart_vm</span></span>(<span>self, request, admin_session, harvester_api_endpoints, basic_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_restart_vm(self, request, admin_session, harvester_api_endpoints,
                    basic_vm):
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    previous_uid = basic_vm[&#39;metadata&#39;][&#39;uid&#39;]
    utils.restart_vm(admin_session, harvester_api_endpoints, previous_uid,
                     vm_name, request.config.getoption(&#39;--wait-timeout&#39;))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_start_vm"><code class="name flex">
<span>def <span class="ident">test_start_vm</span></span>(<span>self, request, admin_session, harvester_api_endpoints, basic_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_start_vm(self, request, admin_session, harvester_api_endpoints,
                  basic_vm):
    # NOTE: this step must be done after VM has stopped
    resp = admin_session.put(harvester_api_endpoints.start_vm % (
        basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code == 202, (
        &#39;Failed to start VM instance %s: %s&#39; % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;], resp.content))

    # give it some time for the VM to start
    time.sleep(120)

    def _check_vm_instance_started():
        resp = admin_session.get(
            harvester_api_endpoints.get_vm_instance % (
                basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        if resp.status_code == 200:
            resp_json = resp.json()
            if (&#39;status&#39; in resp_json and
                    resp_json[&#39;status&#39;][&#39;phase&#39;] == &#39;Running&#39;):
                return True
        return False

    success = polling2.poll(
        _check_vm_instance_started,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Failed to get VM instance for: %s&#39; % (
        basic_vm[&#39;metadata&#39;][&#39;name&#39;])</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_stop_vm"><code class="name flex">
<span>def <span class="ident">test_stop_vm</span></span>(<span>self, request, admin_session, harvester_api_endpoints, basic_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_stop_vm(self, request, admin_session, harvester_api_endpoints,
                 basic_vm):
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    utils.stop_vm(request, admin_session, harvester_api_endpoints,
                  vm_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_unpause_vm"><code class="name flex">
<span>def <span class="ident">test_unpause_vm</span></span>(<span>self, request, admin_session, harvester_api_endpoints, basic_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_unpause_vm(self, request, admin_session, harvester_api_endpoints,
                    basic_vm):
    # NOTE: make sure to execute this step after _paused_vm()
    resp = admin_session.put(harvester_api_endpoints.unpause_vm % (
        basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code == 200, &#39;Failed to unpause VM instance %s&#39; % (
        basic_vm[&#39;metadata&#39;][&#39;name&#39;])

    # give it some time to unpause
    time.sleep(10)

    def _check_vm_instance_unpaused():
        resp = admin_session.get(harvester_api_endpoints.get_vm % (
            basic_vm[&#39;metadata&#39;][&#39;name&#39;]))
        if resp.status_code == 200:
            resp_json = resp.json()
            if (&#39;status&#39; in resp_json and
                    &#39;ready&#39; in resp_json[&#39;status&#39;] and
                    resp_json[&#39;status&#39;][&#39;ready&#39;]):
                return True
        return False

    success = polling2.poll(
        _check_vm_instance_unpaused,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for VM to be unpaused.&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_update_vm_cpu"><code class="name flex">
<span>def <span class="ident">test_update_vm_cpu</span></span>(<span>self, request, admin_session, harvester_api_endpoints, basic_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_update_vm_cpu(self, request, admin_session,
                       harvester_api_endpoints, basic_vm):
    vm_name = basic_vm[&#39;metadata&#39;][&#39;name&#39;]
    vm_instance_json = utils.lookup_vm_instance(
        admin_session, harvester_api_endpoints, basic_vm)
    previous_uid = vm_instance_json[&#39;metadata&#39;][&#39;uid&#39;]
    domain_data = basic_vm[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;]
    updated_cores = domain_data[&#39;cpu&#39;][&#39;cores&#39;] + 1
    domain_data[&#39;cpu&#39;][&#39;cores&#39;] = updated_cores

    resp = utils.poll_for_update_resource(
        request, admin_session,
        harvester_api_endpoints.update_vm % (vm_name),
        basic_vm,
        harvester_api_endpoints.get_vm % (vm_name))
    updated_vm_data = resp.json()
    updated_domain_data = (
        updated_vm_data[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;domain&#39;])
    assert updated_domain_data[&#39;cpu&#39;][&#39;cores&#39;] == updated_cores
    # restart the VM instance for the changes to take effect
    utils.restart_vm(admin_session, harvester_api_endpoints, previous_uid,
                     vm_name, request.config.getoption(&#39;--wait-timeout&#39;))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes"><code class="flex name class">
<span>class <span class="ident">TestVMVolumes</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.volumes_p2
@pytest.mark.volumes_p1
@pytest.mark.p2
@pytest.mark.p1
class TestVMVolumes:

    def test_create_vm_with_external_volume(self, admin_session,
                                            harvester_api_endpoints,
                                            vm_with_volume):
        &#34;&#34;&#34;
        Test virtual machines
        Covers:
        virtual-machines-11-Create VM with two disk volumes
        &#34;&#34;&#34;
        # make sure the VM instance is successfully created
        utils.lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm_with_volume)
        # make sure it&#39;s data volumes are in-use
        volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for volume in volumes:
            resp = admin_session.get(harvester_api_endpoints.get_volume % (
                volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))
            assert resp.status_code == 200, (
                &#39;Failed to lookup volume %s: %s&#39; % (
                    volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;],
                    resp.content))
            volume_json = resp.json()
            owned_by = json.loads(
                volume_json[&#39;metadata&#39;][&#39;annotations&#39;].get(
                    &#39;harvesterhci.io/owned-by&#39;))
            expected_owner = &#39;%s/%s&#39; % (
                vm_with_volume[&#39;metadata&#39;][&#39;namespace&#39;],
                vm_with_volume[&#39;metadata&#39;][&#39;name&#39;])
            # make sure VM is one of the owners
            found = False
            for owner in owned_by:
                if (owner[&#39;schema&#39;] == &#39;kubevirt.io.virtualmachine&#39; and
                        expected_owner in owner[&#39;refs&#39;]):
                    found = True
                    break
            assert found, (&#39;Expecting %s to be in volume %s owners list&#39; % (
                expected_owner, volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))

    def test_delete_volume_in_use(self, request, admin_session,
                                  harvester_api_endpoints, vm_with_volume):
        &#34;&#34;&#34;
        Volume testing
        Covers:
            Negative vol-01-Delete Volume that is in use
            vol-13-Validate volume shows as in use when attached
        &#34;&#34;&#34;
        volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for volume in volumes:
            # try to delete a volume in &#39;in-use&#39; state and it should
            # fail
            resp = admin_session.delete(
                harvester_api_endpoints.delete_volume % (
                    volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))
            assert resp.status_code not in [200, 201], (
                &#39;Deleting &#34;in-use&#34; volumes should not be permitted: %s&#39; % (
                    resp.content))

    def test_delete_vm_then_volumes(self, request, admin_session,
                                    harvester_api_endpoints,
                                    vm_with_volume, volume):
        &#34;&#34;&#34;
        Volume testing
        Covers:
            vol-15-Delete volume that was attached to VM but now is not
        &#34;&#34;&#34;
        # delete the VM but keep the volumes
        utils.delete_vm(request, admin_session, harvester_api_endpoints,
                        vm_with_volume, remove_all_disks=False)
        volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for data_vol in volumes:
            volume_name = data_vol[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            resp = admin_session.get(harvester_api_endpoints.get_volume % (
                volume_name))
            assert resp.status_code == 200, (
                &#39;Failed to lookup data volume %s: %s&#39; % (
                    volume_name, resp.content))
            # now cleanup the volume
            utils.delete_volume_by_name(request, admin_session,
                                        harvester_api_endpoints, volume_name)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_create_vm_with_external_volume"><code class="name flex">
<span>def <span class="ident">test_create_vm_with_external_volume</span></span>(<span>self, admin_session, harvester_api_endpoints, vm_with_volume)</span>
</code></dt>
<dd>
<div class="desc"><p>Test virtual machines
Covers:
virtual-machines-11-Create VM with two disk volumes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_create_vm_with_external_volume(self, admin_session,
                                        harvester_api_endpoints,
                                        vm_with_volume):
    &#34;&#34;&#34;
    Test virtual machines
    Covers:
    virtual-machines-11-Create VM with two disk volumes
    &#34;&#34;&#34;
    # make sure the VM instance is successfully created
    utils.lookup_vm_instance(
        admin_session, harvester_api_endpoints, vm_with_volume)
    # make sure it&#39;s data volumes are in-use
    volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
    for volume in volumes:
        resp = admin_session.get(harvester_api_endpoints.get_volume % (
            volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))
        assert resp.status_code == 200, (
            &#39;Failed to lookup volume %s: %s&#39; % (
                volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;],
                resp.content))
        volume_json = resp.json()
        owned_by = json.loads(
            volume_json[&#39;metadata&#39;][&#39;annotations&#39;].get(
                &#39;harvesterhci.io/owned-by&#39;))
        expected_owner = &#39;%s/%s&#39; % (
            vm_with_volume[&#39;metadata&#39;][&#39;namespace&#39;],
            vm_with_volume[&#39;metadata&#39;][&#39;name&#39;])
        # make sure VM is one of the owners
        found = False
        for owner in owned_by:
            if (owner[&#39;schema&#39;] == &#39;kubevirt.io.virtualmachine&#39; and
                    expected_owner in owner[&#39;refs&#39;]):
                found = True
                break
        assert found, (&#39;Expecting %s to be in volume %s owners list&#39; % (
            expected_owner, volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_delete_vm_then_volumes"><code class="name flex">
<span>def <span class="ident">test_delete_vm_then_volumes</span></span>(<span>self, request, admin_session, harvester_api_endpoints, vm_with_volume, volume)</span>
</code></dt>
<dd>
<div class="desc"><p>Volume testing</p>
<h2 id="covers">Covers</h2>
<p>vol-15-Delete volume that was attached to VM but now is not</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_delete_vm_then_volumes(self, request, admin_session,
                                harvester_api_endpoints,
                                vm_with_volume, volume):
    &#34;&#34;&#34;
    Volume testing
    Covers:
        vol-15-Delete volume that was attached to VM but now is not
    &#34;&#34;&#34;
    # delete the VM but keep the volumes
    utils.delete_vm(request, admin_session, harvester_api_endpoints,
                    vm_with_volume, remove_all_disks=False)
    volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
    for data_vol in volumes:
        volume_name = data_vol[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        resp = admin_session.get(harvester_api_endpoints.get_volume % (
            volume_name))
        assert resp.status_code == 200, (
            &#39;Failed to lookup data volume %s: %s&#39; % (
                volume_name, resp.content))
        # now cleanup the volume
        utils.delete_volume_by_name(request, admin_session,
                                    harvester_api_endpoints, volume_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_delete_volume_in_use"><code class="name flex">
<span>def <span class="ident">test_delete_volume_in_use</span></span>(<span>self, request, admin_session, harvester_api_endpoints, vm_with_volume)</span>
</code></dt>
<dd>
<div class="desc"><p>Volume testing</p>
<h2 id="covers">Covers</h2>
<p>Negative vol-01-Delete Volume that is in use
vol-13-Validate volume shows as in use when attached</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_delete_volume_in_use(self, request, admin_session,
                              harvester_api_endpoints, vm_with_volume):
    &#34;&#34;&#34;
    Volume testing
    Covers:
        Negative vol-01-Delete Volume that is in use
        vol-13-Validate volume shows as in use when attached
    &#34;&#34;&#34;
    volumes = vm_with_volume[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
    for volume in volumes:
        # try to delete a volume in &#39;in-use&#39; state and it should
        # fail
        resp = admin_session.delete(
            harvester_api_endpoints.delete_volume % (
                volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]))
        assert resp.status_code not in [200, 201], (
            &#39;Deleting &#34;in-use&#34; volumes should not be permitted: %s&#39; % (
                resp.content))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests.scenarios" href="index.html">harvester_e2e_tests.scenarios</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.backup_restore_migrated_vm" href="#harvester_e2e_tests.scenarios.test_vm_actions.backup_restore_migrated_vm">backup_restore_migrated_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_restore_migrated_vm_nfs" href="#harvester_e2e_tests.scenarios.test_vm_actions.test_backup_restore_migrated_vm_nfs">test_backup_restore_migrated_vm_nfs</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_restore_migrated_vm_s3" href="#harvester_e2e_tests.scenarios.test_vm_actions.test_backup_restore_migrated_vm_s3">test_backup_restore_migrated_vm_s3</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_single_vm_nfs" href="#harvester_e2e_tests.scenarios.test_vm_actions.test_backup_single_vm_nfs">test_backup_single_vm_nfs</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.test_backup_single_vm_s3" href="#harvester_e2e_tests.scenarios.test_vm_actions.test_backup_single_vm_s3">test_backup_single_vm_s3</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.test_restore_backup_vm_on" href="#harvester_e2e_tests.scenarios.test_vm_actions.test_restore_backup_vm_on">test_restore_backup_vm_on</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.test_update_backup_yaml_nfs" href="#harvester_e2e_tests.scenarios.test_vm_actions.test_update_backup_yaml_nfs">test_update_backup_yaml_nfs</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.test_update_backup_yaml_s3" href="#harvester_e2e_tests.scenarios.test_vm_actions.test_update_backup_yaml_s3">test_update_backup_yaml_s3</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.update_backup_yaml" href="#harvester_e2e_tests.scenarios.test_vm_actions.update_backup_yaml">update_backup_yaml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions">TestVMActions</a></code></h4>
<ul class="two-column">
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.pytestmark" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_create_vm" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_create_vm">test_create_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_pause_vm" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_pause_vm">test_pause_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_restart_vm" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_restart_vm">test_restart_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_start_vm" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_start_vm">test_start_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_stop_vm" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_stop_vm">test_stop_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_unpause_vm" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_unpause_vm">test_unpause_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_update_vm_cpu" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMActions.test_update_vm_cpu">test_update_vm_cpu</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes">TestVMVolumes</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.pytestmark" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_create_vm_with_external_volume" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_create_vm_with_external_volume">test_create_vm_with_external_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_delete_vm_then_volumes" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_delete_vm_then_volumes">test_delete_vm_then_volumes</a></code></li>
<li><code><a title="harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_delete_volume_in_use" href="#harvester_e2e_tests.scenarios.test_vm_actions.TestVMVolumes.test_delete_volume_in_use">test_delete_volume_in_use</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>