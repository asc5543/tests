<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>harvester_e2e_tests.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) 2021 SUSE LLC
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of version 3 of the GNU General Public License as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.   See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, contact SUSE LLC.
#
# To contact SUSE about this file by physical or electronic mail,
# you may find current contact information at www.suse.com

from io import StringIO
from paramiko import SSHClient, AutoAddPolicy, RSAKey
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from scp import SCPClient
import boto3
import jinja2
import json
import math
import os
import polling2
import random
import requests
import shutil
import string
import subprocess
import tempfile
import time
import uuid
import yaml


def retry_session():
    &#34;&#34;&#34;Create a session that will retry on connection errors&#34;&#34;&#34;
    # TODO(gyee): should we make retries and backoff_factor configurable?
    # We should retry on connection error only.
    # See https://urllib3.readthedocs.io/en/latest/reference/
    # urllib3.util.html#urllib3.util.Retry for more information.
    allowed_methods = frozenset({&#39;DELETE&#39;, &#39;GET&#39;, &#39;HEAD&#39;, &#39;OPTIONS&#39;, &#39;PUT&#39;,
                                 &#39;TRACE&#39;, &#39;POST&#39;})
    retry_strategy = Retry(total=5, backoff_factor=10.0,
                           status_forcelist=[500],
                           allowed_methods=allowed_methods)
    adapter = HTTPAdapter(max_retries=retry_strategy)
    s = requests.Session()
    # TODO(gyee): do we need to support other auth methods?
    # NOTE(gyee): ignore SSL certificate validation for now
    s.verify = False
    s.mount(&#39;https://&#39;, adapter)
    s.mount(&#39;http://&#39;, adapter)
    return s


def random_name():
    &#34;&#34;&#34;Generate a random alphanumeric name using uuid.uuid4()&#34;&#34;&#34;
    return uuid.uuid4().hex


def random_alphanumeric(length=5, upper_case=False):
    &#34;&#34;&#34;Generate a random alphanumeric string of given length

    :param length: the size of the string
    :param upper_case: whether to return the upper case string
    &#34;&#34;&#34;
    if upper_case:
        return &#39;&#39;.join(random.choice(
            string.ascii_uppercase + string.digits) for _ in range(length))
    else:
        return &#39;&#39;.join(random.choice(
            string.ascii_lowercase + string.digits) for _ in range(length))


def get_json_object_from_template(template_name, **template_args):
    &#34;&#34;&#34;Load template from template file

    :param template_name: the name of the template. It is the filename of
                          template without the file extension. For example, if
                          you want to load &#39;./templates/foo.json.j2&#39;, then the
                          template_name should be &#39;foo&#39;.
    :param template_args: dictionary of template argument values for the given
                          Jinja2 template
    &#34;&#34;&#34;
    # get the current path relative to the ./templates/ directory
    my_path = os.path.dirname(os.path.realpath(__file__))
    # NOTE: the templates directory must be at the same level as
    # utilities.py, and all the templates must have the &#39;.yaml.j2&#39; extension
    templates_path = os.path.join(my_path, &#39;templates&#39;)
    template_file = f&#39;{templates_path}/{template_name}.json.j2&#39;
    # now load the template
    with open(template_file) as tempfile:
        template = jinja2.Template(tempfile.read())
    template.globals[&#39;random_name&#39;] = random_name
    template.globals[&#39;random_alphanumeric&#39;] = random_alphanumeric
    # now render the template
    rendered = template.render(template_args)
    return json.loads(rendered)


def poll_for_resource_ready(request, admin_session, endpoint,
                            expected_code=200):
    ready = polling2.poll(
        lambda: admin_session.get(endpoint).status_code == expected_code,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert ready, &#39;Timed out while waiting for %s to yield %s&#39; % (
        endpoint, expected_code)


def get_latest_resource_version(request, admin_session, lookup_endpoint):
    poll_for_resource_ready(request, admin_session, lookup_endpoint)
    resp = admin_session.get(lookup_endpoint)
    assert resp.status_code == 200, &#39;Failed to lookup resource: %s&#39; % (
        resp.content)
    return resp.json()[&#39;metadata&#39;][&#39;resourceVersion&#39;]


def poll_for_update_resource(request, admin_session, update_endpoint,
                             request_json, lookup_endpoint, use_yaml=None):

    resp = None

    def _update_resource():
        # we want the update response to return back to the caller
        nonlocal resp

        # first we need to get the latest resourceVersion and fill that in
        # the request_json as it is a required field and must be the latest.
        request_json[&#39;metadata&#39;][&#39;resourceVersion&#39;] = (
            get_latest_resource_version(
                request, admin_session, lookup_endpoint))
        if use_yaml:
            resp = admin_session.put(update_endpoint,
                                     data=yaml.dump(
                                         request_json, sort_keys=False),
                                     headers={
                                         &#39;Content-Type&#39;: &#39;application/yaml&#39;})
        else:
            resp = admin_session.put(update_endpoint, json=request_json)
        if resp.status_code in [409, 500]:
            return False
        else:
            assert resp.status_code == 200, &#39;Failed to update resource: %s&#39; % (
                resp.content)
            return True

    # NOTE(gyee): we need to do retries because kubenetes cluster does not
    # guarantee freshness when updating resources because of the way it handles
    # queuing. See
    # https://github.com/kubernetes/kubernetes/issues/84430
    # Therefore, we must do fetch-retry when updating resources.
    # Apparently this is way of life in Kubernetes world.
    updated = polling2.poll(
        _update_resource,
        step=3,
        timeout=120)
    assert updated, &#39;Timed out while waiting to update resource: %s&#39; % (
        update_endpoint)
    return resp


def lookup_vm_instance(admin_session, harvester_api_endpoints, vm_json):
    # NOTE(gyee): seem like the corresponding VM instance has the same name as
    # the VM. If this assumption is not true, we need to fix this code.
    resp = admin_session.get(harvester_api_endpoints.get_vm_instance % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code == 200, &#39;Failed to lookup VM instance %s: %s&#39; % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
    return resp.json()


def lookup_hosts_with_most_available_cpu(admin_session,
                                         harvester_api_endpoints):
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    most_available_cpu_nodes = None
    most_available_cpu = 0
    for node in nodes_json:
        # look up CPU usage for the given node
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, (
            &#39;Failed to lookup metrices for node %s: %s&#39; % (
                node[&#39;metadata&#39;][&#39;name&#39;], resp.content))
        metrics_json = resp.json()
        # NOTE: Kubernets CPU metrics are expressed in nanocores, or
        # 1 billionth of a CPU. We need to convert it to a whole CPU core.
        cpu_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;cpu&#39;][:-1]) / 1000000000)
        available_cpu = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;cpu&#39;]) - cpu_usage
        if available_cpu &gt; most_available_cpu:
            most_available_cpu = available_cpu
            most_available_cpu_nodes = [node[&#39;metadata&#39;][&#39;name&#39;]]
        elif available_cpu == most_available_cpu:
            most_available_cpu_nodes.append(node[&#39;metadata&#39;][&#39;name&#39;])
    return (most_available_cpu_nodes, most_available_cpu)


def lookup_hosts_with_most_available_memory(admin_session,
                                            harvester_api_endpoints):
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    most_available_memory_nodes = None
    most_available_memory = 0
    for node in nodes_json:
        # look up CPU usage for the given node
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, (
            &#39;Failed to lookup metrices for node %s: %s&#39; % (
                node[&#39;metadata&#39;][&#39;name&#39;], resp.content))
        metrics_json = resp.json()
        # NOTE: Kubernets memory metrics are expressed Kibibyte so convert it
        # back to Gigabytes
        memory_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;memory&#39;][:-2]) * 1.024e-06)
        # NOTE: we want the floor here so we don&#39;t over commit
        allocatable_memory = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;memory&#39;][:-2])
        allocatable_memory = math.floor(
            allocatable_memory * 1.024e-06)
        available_memory = allocatable_memory - memory_usage
        if available_memory &gt; most_available_memory:
            most_available_memory = available_memory
            most_available_memory_nodes = [node[&#39;metadata&#39;][&#39;name&#39;]]
        elif available_memory == most_available_memory:
            most_available_memory_nodes.append(node[&#39;metadata&#39;][&#39;name&#39;])
    return (most_available_memory_nodes, most_available_memory)


def lookup_hosts_with_cpu_and_memory(admin_session, harvester_api_endpoints,
                                     cpu, memory):
    &#34;&#34;&#34;Lookup nodes that satisfies the given CPU and memory requirements&#34;&#34;&#34;
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    nodes = []
    for node in nodes_json:
        # look up CPU usage for the given node
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, (
            &#39;Failed to lookup metrices for node %s: %s&#39; % (
                node[&#39;metadata&#39;][&#39;name&#39;], resp.content))
        metrics_json = resp.json()
        # NOTE: Kubernets CPU metrics are expressed in nanocores, or
        # 1 billionth of a CPU. We need to convert it to a whole CPU core.
        cpu_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;cpu&#39;][:-1]) / 1000000000)
        available_cpu = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;cpu&#39;]) - cpu_usage
        # NOTE: Kubernets memory metrics are expressed Kibibyte so convert it
        # back to Gigabytes
        memory_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;memory&#39;][:-2]) * 1.024e-06)
        # NOTE: we want the floor here so we don&#39;t over commit
        allocatable_memory = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;memory&#39;][:-2])
        allocatable_memory = math.floor(
            allocatable_memory * 1.024e-06)
        available_memory = allocatable_memory - memory_usage
        if available_cpu &gt;= cpu and available_memory &gt;= memory:
            nodes.append(node[&#39;metadata&#39;][&#39;name&#39;])
    return nodes


def restart_vm(admin_session, harvester_api_endpoints, previous_uid, vm_name,
               wait_timeout):
    resp = admin_session.put(harvester_api_endpoints.restart_vm % (
        vm_name))
    assert resp.status_code == 202, &#39;Failed to restart VM instance %s: %s&#39; % (
        vm_name, resp.content)
    assert_vm_restarted(admin_session, harvester_api_endpoints, previous_uid,
                        vm_name, wait_timeout)


def stop_vm(request, admin_session, harvester_api_endpoints,
            vm_name):
    resp = admin_session.put(harvester_api_endpoints.stop_vm % (
        vm_name))
    assert resp.status_code == 202, &#39;Failed to stop VM instance %s&#39; % (
        vm_name)

    # give it some time for the VM instance to stop
    time.sleep(120)

    def _check_vm_instance_stopped():
        resp = admin_session.get(
            harvester_api_endpoints.get_vm_instance % (
                vm_name))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _check_vm_instance_stopped,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Failed to stop VM: %s&#39; % (
        vm_name)


def assert_vm_restarted(admin_session, harvester_api_endpoints,
                        previous_uid, vm_name, wait_timeout):
    # give it some time for the VM instance to restart
    time.sleep(180)

    def _check_vm_instance_restarted():
        resp = admin_session.get(
            harvester_api_endpoints.get_vm_instance % (vm_name))
        if resp.status_code == 200:
            resp_json = resp.json()
            if (&#39;status&#39; in resp_json and
                    &#39;phase&#39; in resp_json[&#39;status&#39;] and
                    resp_json[&#39;status&#39;][&#39;phase&#39;] == &#39;Running&#39; and
                    resp_json[&#39;metadata&#39;][&#39;uid&#39;] != previous_uid):
                return True
        return False

    success = polling2.poll(
        _check_vm_instance_restarted,
        step=5,
        timeout=wait_timeout)
    assert success, &#39;Failed to restart VM %s&#39; % (vm_name)


def delete_image(request, admin_session, harvester_api_endpoints, image_json):
    delete_image_by_name(request,
                         admin_session,
                         harvester_api_endpoints,
                         image_json[&#39;metadata&#39;][&#39;name&#39;])


def delete_image_by_name(request, admin_session,
                         harvester_api_endpoints, image_name):
    # see if the image exist first
    resp = admin_session.get(harvester_api_endpoints.get_image % (image_name))
    if resp.status_code == 404:
        # image doesn&#39;t exist so nothing to be done
        return

    def _wait_for_image_to_be_deleted():
        # retry delete
        admin_session.delete(harvester_api_endpoints.delete_image %
                             (image_name))
        time.sleep(15)
        resp = admin_session.get(harvester_api_endpoints.get_image %
                                 (image_name))
        if resp.status_code == 404:
            return True
        return False

    try:
        polling2.poll(
            _wait_for_image_to_be_deleted,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    except polling2.TimeoutException:
        errmsg = &#39;Timed out while waiting for image to be deleted&#39;
        raise AssertionError(errmsg)


def assert_image_ready(request, admin_session,
                       harvester_api_endpoints, image_name):

    resp = admin_session.get(harvester_api_endpoints.get_image % (image_name))
    if resp.status_code == 404:
        raise AssertionError(f&#34;Image ${image_name} not exists&#34;)

    resp_json = dict()

    def _check_image_ready():
        resp = admin_session.get(harvester_api_endpoints.get_image %
                                 (image_name))
        nonlocal resp_json
        resp_json = resp.json()
        if resp_json[&#39;status&#39;].get(&#34;progress&#34;, 0) == 100:
            return True
        return False

    try:
        polling2.poll(
            _check_image_ready,
            step=5,
            timeout=request.config.getoption(&#34;--wait-timeout&#34;))
    except polling2.TimeoutException:
        errmsg = (&#34;Timed out while waiting for image to be ready\n&#34;
                  f&#34;Stucking in the status {resp_json[&#39;status&#39;]}&#34;)
        raise AssertionError(errmsg)


def create_image(request, admin_session, harvester_api_endpoints, url,
                 name=None, description=&#39;&#39;, source_type=&#39;download&#39;):
    request_json = get_json_object_from_template(
        &#39;basic_image&#39;,
        name=name,
        source_type=source_type,
        description=description,
        url=url
    )
    resp = admin_session.post(harvester_api_endpoints.create_image,
                              json=request_json)
    assert resp.status_code in [200, 201], &#39;Failed to create image %s: %s&#39; % (
        name, resp.content)
    image_json = resp.json()

    # wait for the image to get ready
    time.sleep(50)

    def _wait_for_image_become_active():
        # we want the update response to return back to the caller
        nonlocal image_json

        resp = admin_session.get(harvester_api_endpoints.get_image % (
            image_json[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to get image %s: %s&#39; % (
            image_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
        image_json = resp.json()
        if (&#39;status&#39; in image_json and
                &#39;storageClassName&#39; in image_json[&#39;status&#39;]):
            return True
        return False

    success = polling2.poll(
        _wait_for_image_become_active,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for image to be active.&#39;

    return image_json


def assert_vm_unschedulable(request, admin_session, harvester_api_endpoints,
                            vm_name):
    # give it some time for the scheduler to find a host
    time.sleep(120)

    def _check_vm_instance_unschedulable():
        resp = admin_session.get(
            harvester_api_endpoints.get_vm_instance % (vm_name))
        if resp.status_code == 200:
            resp_json = resp.json()
            if (&#39;status&#39; in resp_json and
                    &#39;conditions&#39; in resp_json[&#39;status&#39;]):
                for condition in resp_json[&#39;status&#39;][&#39;conditions&#39;]:
                    if (&#39;reason&#39; in condition and
                            condition[&#39;reason&#39;] == &#39;Unschedulable&#39;):
                        return True
        return False

    success = polling2.poll(
        _check_vm_instance_unschedulable,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, (
        &#39;Timed out while waiting for the %s instance to become &#39;
        &#39;unscheduable&#39; % (vm_name))


def assert_vm_ready(request, admin_session, harvester_api_endpoints,
                    vm_name, running):
    # give it some time for the VM to boot up
    time.sleep(180)
    resp_json = dict()

    def _check_vm_ready():
        resp = admin_session.get(harvester_api_endpoints.get_vm_instance %
                                 (vm_name))
        if resp.status_code == 200:
            nonlocal resp_json
            resp_json = resp.json()
            if running:
                if (&#39;status&#39; in resp_json and
                        &#39;phase&#39; in resp_json[&#39;status&#39;] and
                        &#39;Running&#39; in resp_json[&#39;status&#39;][&#39;phase&#39;] and
                        &#39;nodeName&#39; in resp_json[&#39;status&#39;]):
                    return True
            else:
                if (&#39;status&#39; in resp_json and
                        &#39;Running&#39; not in resp_json[&#39;status&#39;][&#39;phase&#39;]):
                    return True
        return False

    try:
        polling2.poll(
            _check_vm_ready,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    except polling2.TimeoutException:
        errmsg = (&#39;Timed out while waiting for VM to be ready.\n&#39;
                  f&#34;Stucking in Phase {resp_json[&#39;status&#39;][&#39;phase&#39;]}&#34;)
        raise AssertionError(errmsg)


def create_vm(request, admin_session, image, harvester_api_endpoints,
              template=&#39;basic_vm&#39;, keypair=None, volume=None, network=None,
              cpu=1, disk_size_gb=10, memory_gb=1, network_data=None,
              user_data=None, running=True, machine_type=&#39;q35&#39;,
              include_usb=True):
    volume_name = None
    ssh_public_key = None
    network_name = None
    if network:
        network_name = network[&#39;metadata&#39;][&#39;name&#39;]
    if volume:
        volume_name = volume[&#39;metadata&#39;][&#39;name&#39;]
    if keypair:
        ssh_public_key = keypair[&#39;spec&#39;][&#39;publicKey&#39;]
    request_json = get_json_object_from_template(
        template,
        image_namespace=image[&#39;metadata&#39;][&#39;namespace&#39;],
        image_name=image[&#39;metadata&#39;][&#39;name&#39;],
        image_storage_class=image[&#39;status&#39;][&#39;storageClassName&#39;],
        volume_name=volume_name,
        network_name=network_name,
        disk_size_gb=disk_size_gb,
        cpu=cpu,
        memory_gb=memory_gb,
        ssh_public_key=ssh_public_key,
        network_data=network_data,
        user_data=user_data,
        machine_type=machine_type,
        include_usb=include_usb
    )
    request_json[&#39;spec&#39;][&#39;running&#39;] = running
    resp = admin_session.post(harvester_api_endpoints.create_vm,
                              json=request_json)
    assert resp.status_code == 201, (
        &#39;Failed to create VM %s: %s&#39; % (resp.status_code, resp.content))
    vm_resp_json = resp.json()
    if running:
        assert_vm_ready(request, admin_session, harvester_api_endpoints,
                        vm_resp_json[&#39;metadata&#39;][&#39;name&#39;], running)
    return vm_resp_json


def delete_vm(request, admin_session, harvester_api_endpoints, vm_json,
              remove_all_disks=True):
    resp = admin_session.delete(harvester_api_endpoints.delete_vm % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code in [200, 201], &#39;Failed to delete VM %s: %s&#39; % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)

    def _check_vm_deleted():
        resp = admin_session.get(harvester_api_endpoints.get_vm % (
            vm_json[&#39;metadata&#39;][&#39;name&#39;]))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _check_vm_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for VM to be terminated.&#39;
    if remove_all_disks:
        # NOTE: for PVC, we must explicitly delete the volumes after the
        # VM is deleted
        volumes = vm_json[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for volume in volumes:
            if &#39;persistentVolumeClaim&#39; in volume:
                delete_volume_by_name(
                    request, admin_session, harvester_api_endpoints,
                    volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;],
                    owned_by=vm_json[&#39;metadata&#39;][&#39;name&#39;])


def delete_volume(request, admin_session, harvester_api_endpoints,
                  volume_json):
    delete_volume_by_name(request, admin_session, harvester_api_endpoints,
                          volume_json[&#39;metadata&#39;][&#39;name&#39;])


def delete_volume_by_name(request, admin_session, harvester_api_endpoints,
                          volume_name, owned_by=None):
    # see if the volume exist first
    resp = admin_session.get(harvester_api_endpoints.get_volume % (
        volume_name))
    if resp.status_code == 404:
        # volume doesn&#39;t exist so nothing to be done
        return

    def _wait_for_vm_remove_owned_by():
        if owned_by is None:
            return True
        resp = admin_session.get(harvester_api_endpoints.get_volume % (
            volume_name))
        volume_json = resp.json()
        annotations = volume_json[&#39;metadata&#39;][&#39;annotations&#39;]
        if &#39;harvesterhci.io/owned-by&#39; in annotations:
            if owned_by in annotations[&#39;harvesterhci.io/owned-by&#39;]:
                return False
            return True
        else:
            return True

    success = polling2.poll(
        _wait_for_vm_remove_owned_by,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))

    resp = admin_session.delete(harvester_api_endpoints.delete_volume % (
        volume_name))
    assert resp.status_code in [200, 201], (
        &#39;Failed to delete volume %s: %s&#39; % (volume_name, resp.content))

    def _check_volume_deleted():
        resp = admin_session.get(harvester_api_endpoints.delete_volume % (
            volume_name))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _check_volume_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for volume to be terminated.&#39;


def delete_host(request, admin_session, harvester_api_endpoints, host_json):
    resp = admin_session.delete(harvester_api_endpoints.delete_node % (
        host_json[&#39;id&#39;]))
    assert resp.status_code in [200, 201], &#39;Unable to delete host %s: %s&#39; % (
        host_json[&#39;id&#39;], resp.content)
    # wait for host to be deleted
    time.sleep(180)

    def _wait_for_host_to_be_deleted():
        resp = admin_session.get(harvester_api_endpoints.get_node % (
            host_json[&#39;id&#39;]))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _wait_for_host_to_be_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for host to be deleted&#39;


def _get_node_script_path(request, script_name=None, script_type=None):
    if script_type == &#39;terraform&#39;:
        scripts_dir = request.config.getoption(&#39;--terraform-scripts-location&#39;)
    elif script_type == &#39;backup&#39;:
        scripts_dir = request.config.getoption(&#39;--backup-scripts-location&#39;)
    else:
        scripts_dir = request.config.getoption(&#39;--node-scripts-location&#39;)
    assert scripts_dir, (&#39;Node scripts location not provided. Please use &#39;
                         &#39;the --node-scripts-location parameter to specify &#39;
                         &#39;the location of the node scripts.&#39;)
    assert os.path.isdir(scripts_dir), &#39;Invalid node scripts location: %s&#39; % (
        scripts_dir)
    script = scripts_dir
    if script_name:
        script = os.path.join(scripts_dir, script_name)
        assert os.path.isfile(script), &#39;Node script %s not found&#39; % (script)
        assert os.access(script, os.X_OK), &#39;Node script %s not executable&#39; % (
            script)
    return script


def _lookup_node_ip(admin_session, harvester_api_endpoints, node_name):
    resp = admin_session.get(harvester_api_endpoints.get_node % (node_name))
    assert resp.status_code == 200, &#39;Failed to lookup host %s: %s&#39; % (
        node_name, resp.content)
    node_json = resp.json()
    for address in node_json[&#39;status&#39;][&#39;addresses&#39;]:
        if address[&#39;type&#39;] == &#39;InternalIP&#39;:
            return address[&#39;address&#39;]
    assert False, &#39;Failed to lookup host IP: %s&#39; % (
        node_json[&#39;status&#39;][&#39;addresses&#39;])


def power_off_node(request, admin_session, harvester_api_endpoints, node_name,
                   node_ip=None):
    power_off_script = _get_node_script_path(request, &#39;power_off.sh&#39;)
    if node_ip is None:
        node_ip = _lookup_node_ip(admin_session, harvester_api_endpoints,
                                  node_name)
    result = subprocess.run([power_off_script, node_name, node_ip],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to power-off node %s: rc: %s, stdout: %s, stderr: %s&#39; % (
            node_name, result.returncode, result.stderr, result.stdout))

    # wait for the node to disappear
    time.sleep(120)

    def _wait_for_node_to_disappear():
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node_name))
        metrics_json = resp.json()
        if &#39;status&#39; in metrics_json and metrics_json[&#39;status&#39;] == 404:
            return True
        return False

    success = polling2.poll(
        _wait_for_node_to_disappear,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for node to shutdown&#39;


def power_on_node(request, admin_session, harvester_api_endpoints, node_name,
                  node_ip=None):
    power_on_script = _get_node_script_path(request, &#39;power_on.sh&#39;)
    if node_ip is None:
        node_ip = _lookup_node_ip(admin_session, harvester_api_endpoints,
                                  node_name)
    result = subprocess.run([power_on_script, node_name, node_ip],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to power-on node %s: rc: %s, stdout: %s, stderr: %s&#39; % (
            node_name, result.returncode, result.stderr, result.stdout))

    # wait for the node to power-on
    time.sleep(180)

    def _wait_for_node_to_appear():
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node_name))
        metrics_json = resp.json()
        if (&#39;metadata&#39; in metrics_json and
                &#39;state&#39; in metrics_json[&#39;metadata&#39;] and
                metrics_json[&#39;metadata&#39;][&#39;state&#39;][&#39;error&#39;] is False):
            return True
        return False

    success = polling2.poll(
        _wait_for_node_to_appear,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for node to power-on&#39;


def lookup_host_not_harvester_endpoint(request, admin_session,
                                       harvester_api_endpoints):
    cur_endpoint = (request.config.getoption(&#39;--endpoint&#39;).split(&#34;:&#34;)[1])[2:]
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    for node in nodes_json:
        # look up CPU usage for the given node
        if node[&#39;metadata&#39;][&#39;annotations&#39;].get(
                &#39;etcd.k3s.cattle.io/node-address&#39;) != cur_endpoint:
            node_data = node
    return node_data


def reboot_node(request, admin_session, harvester_api_endpoints, node_name,
                node_ip=None):
    reboot_script = _get_node_script_path(request, &#39;reboot.sh&#39;)
    if node_ip is None:
        node_ip = _lookup_node_ip(admin_session, harvester_api_endpoints,
                                  node_name)
    result = subprocess.run([reboot_script, node_name, node_ip],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to reboot node %s: rc: %s, stdout: %s, stderr: %s&#39; % (
            node_name, result.returncode, result.stderr, result.stdout))

    # wait for the node to power-on
    time.sleep(180)

    def _wait_for_node_to_appear():
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node_name))
        metrics_json = resp.json()
        if (&#39;metadata&#39; in metrics_json and
                &#39;state&#39; in metrics_json[&#39;metadata&#39;] and
                metrics_json[&#39;metadata&#39;][&#39;state&#39;][&#39;error&#39;] is False):
            return True
        return False

    success = polling2.poll(
        _wait_for_node_to_appear,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for node to reboot&#39;


def poweroff_host_maintenance_mode(request, admin_session,
                                   harvester_api_endpoints):
    host_poweroff = lookup_host_not_harvester_endpoint(request, admin_session,
                                                       harvester_api_endpoints)
    # Enable Maintenance Mode
    enable_maintenance_mode(request, admin_session,
                            harvester_api_endpoints, host_poweroff)
    node_name = host_poweroff[&#39;id&#39;]
    poll_for_resource_ready(request, admin_session,
                            harvester_api_endpoints.get_node % (node_name))
    resp = admin_session.get(
        harvester_api_endpoints.get_node % (node_name))
    resp.status_code == 200, &#39;Failed to get host: %s&#39; % (resp.content)
    ret_data = resp.json()
    assert ret_data[&#34;spec&#34;][&#34;unschedulable&#34;]
    s = ret_data[&#34;metadata&#34;][&#34;annotations&#34;][&#34;harvesterhci.io/maintain-status&#34;]
    assert s in [&#34;running&#34;, &#34;completed&#34;]
    node_name = host_poweroff[&#39;id&#39;]
    # Power Off Node
    power_off_node(request, admin_session, harvester_api_endpoints,
                   node_name)
    resp = admin_session.get(
        harvester_api_endpoints.get_node % (node_name))
    resp.status_code == 200, &#39;Failed to get host: %s&#39; % (resp.content)
    ret_data = resp.json()
    assert &#34;NotReady,SchedulingDisabled&#34; in ret_data[&#34;metadata&#34;][&#34;fields&#34;]
    return host_poweroff


def enable_maintenance_mode(request, admin_session, harvester_api_endpoints,
                            node_json):

    def _add_drain_taint(node_json):
        if &#39;taints&#39; in node_json[&#39;spec&#39;]:
            for taint in node_json[&#39;spec&#39;][&#39;taints&#39;]:
                if taint[&#39;key&#39;] == &#39;kubevirt.io/drain&#39;:
                    return
        else:
            node_json[&#39;spec&#39;][&#39;taints&#39;] = []
        node_json[&#39;spec&#39;][&#39;taints&#39;].append(
            {
                &#39;key&#39;: &#39;kubevirt.io/drain&#39;,
                &#39;value&#39;: &#39;scheduling&#39;,
                &#39;effect&#39;: &#39;NoSchedule&#39;
            }
        )

    # NOTE(gyee): implemention of
    # https://github.com/harvester/harvester/blob/
    # 3edc82a7ae6a5de6e8114901058dc573938093e8/pkg/api/node/formatter.go#L92
    node_json[&#39;spec&#39;][&#39;unschedulable&#39;] = True
    _add_drain_taint(node_json)
    if &#39;annotations&#39; not in node_json[&#39;metadata&#39;]:
        node_json[&#39;metadata&#39;][&#39;annotations&#39;] = {}
    node_json[&#39;metadata&#39;][&#39;annotations&#39;][&#39;harvesterhci.io/maintain-status&#39;] = (
        &#39;running&#39;)
    poll_for_update_resource(request, admin_session,
                             node_json[&#39;links&#39;][&#39;update&#39;],
                             node_json,
                             harvester_api_endpoints.get_node % (
                                 node_json[&#39;metadata&#39;][&#39;name&#39;]))


def disable_maintenance_mode(request, admin_session, harvester_api_endpoints,
                             node_json):
    # NOTE(gyee): implementation of
    # https://github.com/harvester/harvester/blob/
    # 3edc82a7ae6a5de6e8114901058dc573938093e8/pkg/api/node/formatter.go#L118
    node_json[&#39;spec&#39;][&#39;unschedulable&#39;] = False
    if &#39;taints&#39; in node_json[&#39;spec&#39;]:
        node_json[&#39;spec&#39;][&#39;taints&#39;] = [
            t for t in node_json[&#39;spec&#39;][&#39;taints&#39;] if not (
                t[&#39;key&#39;] == &#39;kubevirt.io/drain&#39;)]
    del node_json[&#39;metadata&#39;][&#39;annotations&#39;][&#39;harvesterhci.io/maintain-status&#39;]
    poll_for_update_resource(request, admin_session,
                             node_json[&#39;links&#39;][&#39;update&#39;],
                             node_json,
                             harvester_api_endpoints.get_node % (
                                 node_json[&#39;metadata&#39;][&#39;name&#39;]))


def create_tf_from_template(request, template_name, **template_args):
    # get the current path relative to the ./templates/ directory
    my_path = os.path.dirname(os.path.realpath(__file__))
    # NOTE: the templates directory must be at the same level as
    # utilities.py, and all the templates must have the &#39;.yaml.j2&#39; extension
    templates_path = os.path.join(my_path, &#39;templates&#39;)
    template_file = f&#39;{templates_path}/{template_name}.tf.j2&#39;
    # now load the template
    with open(template_file) as tempfile:
        template = jinja2.Template(tempfile.read())
    # now render the template
#    template.globals[&#39;random_name&#39;] = random_name
    rendered = template.render(template_args)
    tf_file_dir = _get_node_script_path(request, script_type=&#39;terraform&#39;)
    tf_file_path = os.path.join(tf_file_dir, f&#39;{template_name}.tf&#39;)
    with open(tf_file_path, &#39;w&#39;) as f:
        f.write(rendered)


def create_kubeconfig_from_template(request, template_name, **template_args):
    # Create/Update kubeconfig
    my_path = os.path.dirname(os.path.realpath(__file__))
    # NOTE: the templates directory must be at the same level as
    # utilities.py, and all the templates must have the &#39;.yaml.j2&#39; extension
    templates_path = os.path.join(my_path, &#39;templates&#39;)
    template_file = f&#39;{templates_path}/{template_name}.j2&#39;
    with open(template_file) as tempfile:
        template = jinja2.Template(tempfile.read())
    # now render the template
    rendered = template.render(template_args)
    terraform_path = _get_node_script_path(
        request, script_type=&#39;terraform&#39;)
    kubeconfig_dir = os.path.join(terraform_path, &#39;.kube&#39;)
    if not os.path.exists(kubeconfig_dir):
        os.makedirs(kubeconfig_dir)
    config_file_path = os.path.join(kubeconfig_dir, &#39;config&#39;)
    with open(config_file_path, &#39;w&#39;) as f:
        f.write(rendered)
    create_tf_from_template(
        request,
        &#39;provider&#39;,
        kubeconfig=os.path.abspath(config_file_path))


def create_image_terraform(request, admin_session, harvester_api_endpoints,
                           url):
    name = &#34;t-&#34; + random_name()
    create_tf_from_template(
        request,
        &#39;resource_image&#39;,
        name=name,
        url=url)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))
    # wait for the image to get ready
    time.sleep(50)

    def _wait_for_image_become_active():
        # we want the update response to return back to the caller

        resp = admin_session.get(harvester_api_endpoints.get_image % (
            name))
        assert resp.status_code == 200, &#39;Failed to get image %s: %s&#39; % (
            name, resp.content)
        image_json = resp.json()
        if (&#39;status&#39; in image_json and
                &#39;storageClassName&#39; in image_json[&#39;status&#39;] and
                &#39;progress&#39; in image_json[&#39;status&#39;] and
                image_json[&#39;status&#39;][&#39;progress&#39;] == 100):
            return True
        return False

    success = polling2.poll(
        _wait_for_image_become_active,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for image to be active.&#39;

    resp = admin_session.get(harvester_api_endpoints.get_image % (
        name))
    image_json = resp.json()
    return image_json


def destroy_resource(request, admin_session, destroy_type=None):
    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_path = _get_node_script_path(
        request, script_type=&#39;terraform&#39;)
    if os.path.isdir(os.path.join(terraform_path, &#39;terraformharvester&#39;)):
        terraform_script = _get_node_script_path(
            request, &#39;terraform_destroy.sh&#39;, &#39;terraform&#39;) + &#39; &#39; + destroy_type
        result = subprocess.run([terraform_script], shell=True,
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        assert result.returncode == 0, (
            &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
                result.returncode, result.stderr, result.stdout))

    if os.path.isdir(os.path.join(terraform_path, &#39;.kube&#39;)):
        shutil.rmtree(os.path.join(terraform_path, &#39;.kube&#39;))

    if os.path.isfile(os.path.join(terraform_path, &#39;provider.tf&#39;)):
        os.remove(os.path.join(terraform_path, &#39;provider.tf&#39;))


def create_volume_terraform(request, admin_session, harvester_api_endpoints,
                            template_name, size, image=None):
    name = &#34;t-&#34; + random_name()
    create_tf_from_template(
        request,
        template_name,
        name=name,
        size=size,
        image=image)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    resp = admin_session.get(harvester_api_endpoints.get_volume % (
        name))
    assert resp.status_code == 200, &#39;Failed to get Volume %s: %s&#39; % (
        name, resp.content)

    vol_data = resp.json()
    return vol_data


def create_keypair_terraform(request, admin_session, harvester_api_endpoints,
                             template_name, public_key):
    name = &#34;t-&#34; + random_name()
    create_tf_from_template(
        request,
        template_name,
        name=name,
        public_key=public_key)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    resp = admin_session.get(harvester_api_endpoints.get_keypair % (
        name))
    assert resp.status_code == 200, &#39;Failed to get Volume %s: %s&#39; % (
        name, resp.content)

    keypair_data = resp.json()
    return keypair_data


def create_network_terraform(request, admin_session, harvester_api_endpoints,
                             template_name, vlan_id, import_flag):

    # NOTE(gyee): will name the network with the following convention as
    # VLAN ID must be unique. vlan_network_&lt;VLAN ID&gt;
    name = f&#39;vlan-network-{vlan_id}&#39;
    create_tf_from_template(
        request,
        template_name,
        name=name,
        vlan_id=vlan_id)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    if import_flag:
        terraform_script = _get_node_script_path(
            request, &#39;terraform.sh&#39;, &#39;terraform&#39;) + \
                           &#39; &#39; + &#39;network&#39; + &#39; &#39; + name
    else:
        terraform_script = _get_node_script_path(
            request, &#39;terraform.sh&#39;, &#39;terraform&#39;)

    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    poll_for_resource_ready(request, admin_session,
                            harvester_api_endpoints.get_network % (name))
    resp = admin_session.get(harvester_api_endpoints.get_network % (
        name))
    assert resp.status_code == 200, &#39;Failed to get Network %s: %s&#39; % (
        name, resp.content)

    network_data = resp.json()
    return network_data


def create_clusternetworks_terraform(request, admin_session,
                                     harvester_api_endpoints,
                                     template_name,
                                     vlan_nic=None):

    create_tf_from_template(
        request,
        template_name,
        vlan_nic=vlan_nic)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;) + &#39; &#39; + &#39;cluster&#39;
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    poll_for_resource_ready(request, admin_session,
                            harvester_api_endpoints.get_vlan)
    resp = admin_session.get(harvester_api_endpoints.get_vlan)
    assert resp.status_code == 200, &#39;Failed to get vlan: %s&#39; % (resp.content)
    network_data = resp.json()
    return network_data


def create_vm_terraform(request, admin_session, harvester_api_endpoints,
                        template_name,
                        keypair=None,
                        image=None,
                        volume=None,
                        net=None,
                        user_data=None,
                        net_data=None):
    name = &#39;t-&#39; + random_name()
    create_tf_from_template(
        request,
        template_name,
        name=name,
        image_name=image[&#39;metadata&#39;][&#39;name&#39;],
        vol_name=volume[&#39;metadata&#39;][&#39;name&#39;],
        net_name=net[&#39;metadata&#39;][&#39;name&#39;],
        keypair=keypair[&#39;metadata&#39;][&#39;name&#39;],
        public_key=keypair[&#39;spec&#39;][&#39;publicKey&#39;],
        user_data=user_data,
        net_data=net_data)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    assert_vm_ready(request, admin_session, harvester_api_endpoints,
                    name, running=True)
    resp = admin_session.get(harvester_api_endpoints.get_vm % (
        name))
    assert resp.status_code == 200, &#39;Failed to get VirtualMachine %s: %s&#39; % (
        name, resp.content)

    vm_data = resp.json()
    return vm_data


def create_image_upload(request, admin_session, harvester_api_endpoints,
                        name=None):

    cache_url = request.config.getoption(&#39;--image-cache-url&#39;)

    base_url = (&#39;http://download.opensuse.org/repositories/Cloud:/Images:/&#39;
                &#39;Leap_15.2/images&#39;)
    if cache_url:
        base_url = cache_url
    url = os.path.join(base_url, &#39;openSUSE-Leap-15.2.x86_64-NoCloud.qcow2&#39;)

    with tempfile.TemporaryDirectory() as tmpdir:
        image_path = os.path.join(
            tmpdir, &#39;openSUSE-Leap-15.2.x86_64-NoCloud.qcow2&#39;)
        # first download the file
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(image_path, &#39;wb&#39;) as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        # create an image for upload
        image_json = create_image(request, admin_session,
                                  harvester_api_endpoints,
                                  &#39;&#39;, source_type=&#39;upload&#39;)
        image_name = image_json[&#39;metadata&#39;][&#39;name&#39;]
        # now upload the file in one shot
        # TODO(gyee): need to check with Harvester team to see if the API
        # supports streaming
        image_size = os.stat(image_path).st_size
        files = {&#39;chunk&#39;: open(image_path, &#39;rb&#39;)}
        params = {&#39;action&#39;: &#39;upload&#39;,
                  &#39;size&#39;: image_size}
        resp = admin_session.post(
            harvester_api_endpoints.upload_image % (image_name),
            files=files,
            params=params)
        assert resp.status_code in [200, 201], (
            &#39;Failed to upload image %s: %s: %s&#39; % (
                image_name, resp.status_code, resp.content))

        def _wait_for_image_upload_complete():
            # we want the update response to return back to the caller
            nonlocal image_json

            resp = admin_session.get(harvester_api_endpoints.get_image % (
                image_json[&#39;metadata&#39;][&#39;name&#39;]))
            assert resp.status_code == 200, &#39;Failed to get image %s: %s&#39; % (
                image_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
            image_json = resp.json()
            if (&#39;status&#39; in image_json and
                    &#39;progress&#39; in image_json[&#39;status&#39;] and
                    &#39;size&#39; in image_json[&#39;status&#39;] and
                    image_json[&#39;status&#39;][&#39;progress&#39;] == 100 and
                    image_json[&#39;status&#39;][&#39;size&#39;] == image_size):
                return True
            return False

        success = polling2.poll(
            _wait_for_image_upload_complete,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out while waiting for image upload to finish.&#39;

    return image_json


def is_marker_enabled(request, marker_name):
    for item in request.session.items:
        if item.get_closest_marker(marker_name) is not None:
            return True
    return False


def create_vm_backup(request, admin_session, harvester_api_endpoints,
                     backuptarget, name=None, vm_name=None):
    request_json = get_json_object_from_template(
        &#39;basic_vm_backup&#39;,
        name=name,
        vm_name=vm_name
    )
    backuptarget_value = json.loads(backuptarget[&#39;value&#39;])
    backuptarget_type = backuptarget_value[&#39;type&#39;]
    if backuptarget_type == &#39;s3&#39;:
        total_objects_before_backup = get_total_objects_s3_bucket(request)
    else:
        total_objects_before_backup = get_total_objects_nfs_share(request)

    resp = admin_session.post(harvester_api_endpoints.create_vm_backup,
                              json=request_json)
    assert resp.status_code in [200, 201], &#39;Failed to create backup %s: %s&#39; % (
        name, resp.content)
    backup_json = resp.json()

    # wait for the backup to get ready
    time.sleep(30)

    def _wait_for_backup_become_active():
        # we want the update response to return back to the caller

        resp = admin_session.get(harvester_api_endpoints.get_vm_backup % (
            backup_json[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to get backup %s: %s&#39; % (
            backup_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
        resp_json = resp.json()
        if (&#39;status&#39; in resp_json and
                &#39;readyToUse&#39; in resp_json[&#39;status&#39;] and
                resp_json[&#39;status&#39;][&#39;readyToUse&#39;]):
            return True
        return False

    success = polling2.poll(
        _wait_for_backup_become_active,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Failed to Backup  VM: %s&#39; % (
        vm_name)

    if backuptarget_type == &#39;s3&#39;:
        total_objects_after_backup = get_total_objects_s3_bucket(request)
    else:
        total_objects_after_backup = get_total_objects_nfs_share(request)

    time.sleep(50)

    assert total_objects_before_backup &lt; total_objects_after_backup, (
        &#39;Failed to add any objects in %s target. &#39;
        &#39;Before backup object count: %s; after backup object count: %s&#39; % (
            backuptarget_type, total_objects_before_backup,
            total_objects_after_backup))
    return backup_json


def delete_vm_backup(request, admin_session,
                     harvester_api_endpoints, backuptarget, backup_json):

    backuptarget_value = json.loads(backuptarget[&#39;value&#39;])
    backuptarget_type = backuptarget_value[&#39;type&#39;]
    if backuptarget_type == &#39;s3&#39;:
        total_objects_before_delete = get_total_objects_s3_bucket(request)
    else:
        total_objects_before_delete = get_total_objects_nfs_share(request)
    resp = admin_session.delete(harvester_api_endpoints.delete_vm_backup % (

        backup_json[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code in [200, 201], &#39;Unable to del backup %s: %s&#39; % (
        backup_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)

    def _wait_for_backup_to_be_deleted():
        resp = admin_session.get(harvester_api_endpoints.get_vm_backup % (
            backup_json[&#39;metadata&#39;][&#39;name&#39;]))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _wait_for_backup_to_be_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for backup to be deleted&#39;

    time.sleep(50)
    if backuptarget_type == &#39;s3&#39;:
        total_objects_after_delete = get_total_objects_s3_bucket(request)
    else:
        total_objects_after_delete = get_total_objects_nfs_share(request)

    assert total_objects_before_delete &gt; total_objects_after_delete, (
        &#39;Failed to delete any objects from %s target. &#39;
        &#39;Before delete object count: %s; after delete object count: %s&#39; % (
            backuptarget_type, total_objects_before_delete,
            total_objects_after_delete))


def get_total_objects_s3_bucket(request):
    accesskey = request.config.getoption(&#39;--accessKeyId&#39;)
    secretaccesskey = request.config.getoption(&#39;--secretAccessKey&#39;)
    bucket = request.config.getoption(&#39;--bucketName&#39;)
    region = request.config.getoption(&#39;--region&#39;)
    totalCount = 0
    s3 = boto3.resource(&#34;s3&#34;,
                        region_name=region,
                        aws_access_key_id=accesskey,
                        aws_secret_access_key=secretaccesskey)
    s3bucket = s3.Bucket(bucket)
    for key in s3bucket.objects.all():
        totalCount += 1

    return totalCount


def get_total_objects_nfs_share(request):
    backup_script = get_backup_create_files_script(
        request, &#39;mountnfs.sh&#39;, &#39;backup&#39;)
    nfsendpoint = (request.config.getoption(&#39;--nfs-endpoint&#39;).split(&#34;//&#34;)[1])
    nfsmountdir = request.config.getoption(&#39;--nfs-mount-dir&#39;)
    total_objects = 0
    result = subprocess.run([backup_script, nfsendpoint, nfsmountdir],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run mountnfs : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))
    total_objects = int(result.stdout.decode(&#34;utf-8&#34;))
    return total_objects


def restore_vm_backup(request, admin_session, harvester_api_endpoints,
                      name=None, vm_name=None,
                      backup_name=None, vm_new=None):
    request_json = get_json_object_from_template(
        &#39;basic_vm_restore&#39;,
        name=name,
        vm_name=vm_name,
        backup_name=backup_name
    )
    if vm_new:
        request_json[&#39;spec&#39;][&#39;newVM&#39;] = vm_new
    resp = admin_session.post(harvester_api_endpoints.create_vm_restore,
                              json=request_json)
    assert resp.status_code in [200, 201], &#39;Failed to restore bakup %s: %s&#39; % (
        backup_name, resp.content)
    restore_json = resp.json()

    # wait for the restore to get ready
    time.sleep(30)

    def _wait_for_restore_to_finish():
        # we want the update response to return back to the caller

        resp = admin_session.get(harvester_api_endpoints.get_vm_restore % (
            restore_json[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to restore %s: %s&#39; % (
            restore_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
        resp_json = resp.json()
        if (&#39;status&#39; in resp_json and
                &#39;complete&#39; in resp_json[&#39;status&#39;] and
                resp_json[&#39;status&#39;][&#39;complete&#39;]):
            return True
        return False

    success = polling2.poll(
        _wait_for_restore_to_finish,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Failed to Restore  VM: %s&#39; % (
        vm_name)

    return restore_json


def get_backup_create_files_script(request,
                                   script_name=None,
                                   script_type=None):
    script = _get_node_script_path(request, script_name, script_type)
    return script


def wait_for_ssh_client(ip, timeout, keypair=None):
    client = SSHClient()
    # automatically add host since we only care about connectivity
    client.set_missing_host_key_policy(AutoAddPolicy)

    def _wait_for_connect():
        try:
            # NOTE: for the default openSUSE Leap image, the root user
            # password is &#39;linux&#39;
            if keypair is not None:
                private_key = RSAKey.from_private_key(
                    StringIO(keypair[&#39;spec&#39;][&#39;privateKey&#39;]))
                client.connect(ip, username=&#39;root&#39;, pkey=private_key)
            else:
                client.connect(ip, username=&#39;root&#39;, password=&#39;linux&#39;)
        except Exception as e:
            print(&#39;Unable to connect to %s: %s&#39; % (ip, e))
            return False
        return True

    ready = polling2.poll(
        _wait_for_connect,
        step=5,
        timeout=timeout)
    assert ready, &#39;Timed out while waiting for SSH server to be ready&#39;
    return client


def get_vm_ip_address(admin_session, harvester_api_endpoints, vm, timeout,
                      nic_name=&#39;default&#39;):
    vm_instance_json = None

    def _wait_for_ip():
        nonlocal vm_instance_json
        vm_instance_json = lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm)
        for interface in vm_instance_json[&#39;status&#39;][&#39;interfaces&#39;]:
            # NOTE: by default, the second NIC name is &#39;nic-1&#39;
            if (interface[&#39;name&#39;] == nic_name and
                    &#39;ipAddress&#39; in interface):
                return interface[&#39;ipAddress&#39;]

    try:
        ip = polling2.poll(_wait_for_ip, step=5, timeout=timeout)
    except polling2.TimeoutException:
        errmsg = (&#39;Timed out while waiting for IP address for NIC %s to be &#39;
                  &#39; assigned: %s&#39; % (nic_name, vm_instance_json))
        raise AssertionError(errmsg)

    return (vm_instance_json, ip)


def execute_script_on_vm(ip, timeout, script, keypair=None,
                         script_params=None):
    ssh_client = wait_for_ssh_client(ip, timeout, keypair)
    # first copy the script to the /tmp dir on the VM
    with SCPClient(ssh_client.get_transport()) as scp:
        scp.put(script, &#39;/tmp&#39;)
    # now execute the script on the VM and return stdout as string
    command = &#39;bash /tmp/%s&#39; % (os.path.basename(script))
    if script_params:
        command += &#39; &#39; + script_params
    stdin, stdout, stderr = ssh_client.exec_command(command)
    ssh_client.close()
    errors = stderr.read().strip().decode(&#39;utf-8&#39;)
    assert not errors, (
        &#39;Failed to execute %s on %s: %s&#39; % (script, ip, errors))
    return stdout.read().strip().decode(&#39;utf-8&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.utils.assert_image_ready"><code class="name flex">
<span>def <span class="ident">assert_image_ready</span></span>(<span>request, admin_session, harvester_api_endpoints, image_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_image_ready(request, admin_session,
                       harvester_api_endpoints, image_name):

    resp = admin_session.get(harvester_api_endpoints.get_image % (image_name))
    if resp.status_code == 404:
        raise AssertionError(f&#34;Image ${image_name} not exists&#34;)

    resp_json = dict()

    def _check_image_ready():
        resp = admin_session.get(harvester_api_endpoints.get_image %
                                 (image_name))
        nonlocal resp_json
        resp_json = resp.json()
        if resp_json[&#39;status&#39;].get(&#34;progress&#34;, 0) == 100:
            return True
        return False

    try:
        polling2.poll(
            _check_image_ready,
            step=5,
            timeout=request.config.getoption(&#34;--wait-timeout&#34;))
    except polling2.TimeoutException:
        errmsg = (&#34;Timed out while waiting for image to be ready\n&#34;
                  f&#34;Stucking in the status {resp_json[&#39;status&#39;]}&#34;)
        raise AssertionError(errmsg)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.assert_vm_ready"><code class="name flex">
<span>def <span class="ident">assert_vm_ready</span></span>(<span>request, admin_session, harvester_api_endpoints, vm_name, running)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_vm_ready(request, admin_session, harvester_api_endpoints,
                    vm_name, running):
    # give it some time for the VM to boot up
    time.sleep(180)
    resp_json = dict()

    def _check_vm_ready():
        resp = admin_session.get(harvester_api_endpoints.get_vm_instance %
                                 (vm_name))
        if resp.status_code == 200:
            nonlocal resp_json
            resp_json = resp.json()
            if running:
                if (&#39;status&#39; in resp_json and
                        &#39;phase&#39; in resp_json[&#39;status&#39;] and
                        &#39;Running&#39; in resp_json[&#39;status&#39;][&#39;phase&#39;] and
                        &#39;nodeName&#39; in resp_json[&#39;status&#39;]):
                    return True
            else:
                if (&#39;status&#39; in resp_json and
                        &#39;Running&#39; not in resp_json[&#39;status&#39;][&#39;phase&#39;]):
                    return True
        return False

    try:
        polling2.poll(
            _check_vm_ready,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    except polling2.TimeoutException:
        errmsg = (&#39;Timed out while waiting for VM to be ready.\n&#39;
                  f&#34;Stucking in Phase {resp_json[&#39;status&#39;][&#39;phase&#39;]}&#34;)
        raise AssertionError(errmsg)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.assert_vm_restarted"><code class="name flex">
<span>def <span class="ident">assert_vm_restarted</span></span>(<span>admin_session, harvester_api_endpoints, previous_uid, vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_vm_restarted(admin_session, harvester_api_endpoints,
                        previous_uid, vm_name, wait_timeout):
    # give it some time for the VM instance to restart
    time.sleep(180)

    def _check_vm_instance_restarted():
        resp = admin_session.get(
            harvester_api_endpoints.get_vm_instance % (vm_name))
        if resp.status_code == 200:
            resp_json = resp.json()
            if (&#39;status&#39; in resp_json and
                    &#39;phase&#39; in resp_json[&#39;status&#39;] and
                    resp_json[&#39;status&#39;][&#39;phase&#39;] == &#39;Running&#39; and
                    resp_json[&#39;metadata&#39;][&#39;uid&#39;] != previous_uid):
                return True
        return False

    success = polling2.poll(
        _check_vm_instance_restarted,
        step=5,
        timeout=wait_timeout)
    assert success, &#39;Failed to restart VM %s&#39; % (vm_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.assert_vm_unschedulable"><code class="name flex">
<span>def <span class="ident">assert_vm_unschedulable</span></span>(<span>request, admin_session, harvester_api_endpoints, vm_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_vm_unschedulable(request, admin_session, harvester_api_endpoints,
                            vm_name):
    # give it some time for the scheduler to find a host
    time.sleep(120)

    def _check_vm_instance_unschedulable():
        resp = admin_session.get(
            harvester_api_endpoints.get_vm_instance % (vm_name))
        if resp.status_code == 200:
            resp_json = resp.json()
            if (&#39;status&#39; in resp_json and
                    &#39;conditions&#39; in resp_json[&#39;status&#39;]):
                for condition in resp_json[&#39;status&#39;][&#39;conditions&#39;]:
                    if (&#39;reason&#39; in condition and
                            condition[&#39;reason&#39;] == &#39;Unschedulable&#39;):
                        return True
        return False

    success = polling2.poll(
        _check_vm_instance_unschedulable,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, (
        &#39;Timed out while waiting for the %s instance to become &#39;
        &#39;unscheduable&#39; % (vm_name))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_clusternetworks_terraform"><code class="name flex">
<span>def <span class="ident">create_clusternetworks_terraform</span></span>(<span>request, admin_session, harvester_api_endpoints, template_name, vlan_nic=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_clusternetworks_terraform(request, admin_session,
                                     harvester_api_endpoints,
                                     template_name,
                                     vlan_nic=None):

    create_tf_from_template(
        request,
        template_name,
        vlan_nic=vlan_nic)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;) + &#39; &#39; + &#39;cluster&#39;
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    poll_for_resource_ready(request, admin_session,
                            harvester_api_endpoints.get_vlan)
    resp = admin_session.get(harvester_api_endpoints.get_vlan)
    assert resp.status_code == 200, &#39;Failed to get vlan: %s&#39; % (resp.content)
    network_data = resp.json()
    return network_data</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_image"><code class="name flex">
<span>def <span class="ident">create_image</span></span>(<span>request, admin_session, harvester_api_endpoints, url, name=None, description='', source_type='download')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_image(request, admin_session, harvester_api_endpoints, url,
                 name=None, description=&#39;&#39;, source_type=&#39;download&#39;):
    request_json = get_json_object_from_template(
        &#39;basic_image&#39;,
        name=name,
        source_type=source_type,
        description=description,
        url=url
    )
    resp = admin_session.post(harvester_api_endpoints.create_image,
                              json=request_json)
    assert resp.status_code in [200, 201], &#39;Failed to create image %s: %s&#39; % (
        name, resp.content)
    image_json = resp.json()

    # wait for the image to get ready
    time.sleep(50)

    def _wait_for_image_become_active():
        # we want the update response to return back to the caller
        nonlocal image_json

        resp = admin_session.get(harvester_api_endpoints.get_image % (
            image_json[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to get image %s: %s&#39; % (
            image_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
        image_json = resp.json()
        if (&#39;status&#39; in image_json and
                &#39;storageClassName&#39; in image_json[&#39;status&#39;]):
            return True
        return False

    success = polling2.poll(
        _wait_for_image_become_active,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for image to be active.&#39;

    return image_json</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_image_terraform"><code class="name flex">
<span>def <span class="ident">create_image_terraform</span></span>(<span>request, admin_session, harvester_api_endpoints, url)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_image_terraform(request, admin_session, harvester_api_endpoints,
                           url):
    name = &#34;t-&#34; + random_name()
    create_tf_from_template(
        request,
        &#39;resource_image&#39;,
        name=name,
        url=url)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))
    # wait for the image to get ready
    time.sleep(50)

    def _wait_for_image_become_active():
        # we want the update response to return back to the caller

        resp = admin_session.get(harvester_api_endpoints.get_image % (
            name))
        assert resp.status_code == 200, &#39;Failed to get image %s: %s&#39; % (
            name, resp.content)
        image_json = resp.json()
        if (&#39;status&#39; in image_json and
                &#39;storageClassName&#39; in image_json[&#39;status&#39;] and
                &#39;progress&#39; in image_json[&#39;status&#39;] and
                image_json[&#39;status&#39;][&#39;progress&#39;] == 100):
            return True
        return False

    success = polling2.poll(
        _wait_for_image_become_active,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for image to be active.&#39;

    resp = admin_session.get(harvester_api_endpoints.get_image % (
        name))
    image_json = resp.json()
    return image_json</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_image_upload"><code class="name flex">
<span>def <span class="ident">create_image_upload</span></span>(<span>request, admin_session, harvester_api_endpoints, name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_image_upload(request, admin_session, harvester_api_endpoints,
                        name=None):

    cache_url = request.config.getoption(&#39;--image-cache-url&#39;)

    base_url = (&#39;http://download.opensuse.org/repositories/Cloud:/Images:/&#39;
                &#39;Leap_15.2/images&#39;)
    if cache_url:
        base_url = cache_url
    url = os.path.join(base_url, &#39;openSUSE-Leap-15.2.x86_64-NoCloud.qcow2&#39;)

    with tempfile.TemporaryDirectory() as tmpdir:
        image_path = os.path.join(
            tmpdir, &#39;openSUSE-Leap-15.2.x86_64-NoCloud.qcow2&#39;)
        # first download the file
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(image_path, &#39;wb&#39;) as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        # create an image for upload
        image_json = create_image(request, admin_session,
                                  harvester_api_endpoints,
                                  &#39;&#39;, source_type=&#39;upload&#39;)
        image_name = image_json[&#39;metadata&#39;][&#39;name&#39;]
        # now upload the file in one shot
        # TODO(gyee): need to check with Harvester team to see if the API
        # supports streaming
        image_size = os.stat(image_path).st_size
        files = {&#39;chunk&#39;: open(image_path, &#39;rb&#39;)}
        params = {&#39;action&#39;: &#39;upload&#39;,
                  &#39;size&#39;: image_size}
        resp = admin_session.post(
            harvester_api_endpoints.upload_image % (image_name),
            files=files,
            params=params)
        assert resp.status_code in [200, 201], (
            &#39;Failed to upload image %s: %s: %s&#39; % (
                image_name, resp.status_code, resp.content))

        def _wait_for_image_upload_complete():
            # we want the update response to return back to the caller
            nonlocal image_json

            resp = admin_session.get(harvester_api_endpoints.get_image % (
                image_json[&#39;metadata&#39;][&#39;name&#39;]))
            assert resp.status_code == 200, &#39;Failed to get image %s: %s&#39; % (
                image_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
            image_json = resp.json()
            if (&#39;status&#39; in image_json and
                    &#39;progress&#39; in image_json[&#39;status&#39;] and
                    &#39;size&#39; in image_json[&#39;status&#39;] and
                    image_json[&#39;status&#39;][&#39;progress&#39;] == 100 and
                    image_json[&#39;status&#39;][&#39;size&#39;] == image_size):
                return True
            return False

        success = polling2.poll(
            _wait_for_image_upload_complete,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
        assert success, &#39;Timed out while waiting for image upload to finish.&#39;

    return image_json</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_keypair_terraform"><code class="name flex">
<span>def <span class="ident">create_keypair_terraform</span></span>(<span>request, admin_session, harvester_api_endpoints, template_name, public_key)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_keypair_terraform(request, admin_session, harvester_api_endpoints,
                             template_name, public_key):
    name = &#34;t-&#34; + random_name()
    create_tf_from_template(
        request,
        template_name,
        name=name,
        public_key=public_key)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    resp = admin_session.get(harvester_api_endpoints.get_keypair % (
        name))
    assert resp.status_code == 200, &#39;Failed to get Volume %s: %s&#39; % (
        name, resp.content)

    keypair_data = resp.json()
    return keypair_data</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_kubeconfig_from_template"><code class="name flex">
<span>def <span class="ident">create_kubeconfig_from_template</span></span>(<span>request, template_name, **template_args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_kubeconfig_from_template(request, template_name, **template_args):
    # Create/Update kubeconfig
    my_path = os.path.dirname(os.path.realpath(__file__))
    # NOTE: the templates directory must be at the same level as
    # utilities.py, and all the templates must have the &#39;.yaml.j2&#39; extension
    templates_path = os.path.join(my_path, &#39;templates&#39;)
    template_file = f&#39;{templates_path}/{template_name}.j2&#39;
    with open(template_file) as tempfile:
        template = jinja2.Template(tempfile.read())
    # now render the template
    rendered = template.render(template_args)
    terraform_path = _get_node_script_path(
        request, script_type=&#39;terraform&#39;)
    kubeconfig_dir = os.path.join(terraform_path, &#39;.kube&#39;)
    if not os.path.exists(kubeconfig_dir):
        os.makedirs(kubeconfig_dir)
    config_file_path = os.path.join(kubeconfig_dir, &#39;config&#39;)
    with open(config_file_path, &#39;w&#39;) as f:
        f.write(rendered)
    create_tf_from_template(
        request,
        &#39;provider&#39;,
        kubeconfig=os.path.abspath(config_file_path))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_network_terraform"><code class="name flex">
<span>def <span class="ident">create_network_terraform</span></span>(<span>request, admin_session, harvester_api_endpoints, template_name, vlan_id, import_flag)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_network_terraform(request, admin_session, harvester_api_endpoints,
                             template_name, vlan_id, import_flag):

    # NOTE(gyee): will name the network with the following convention as
    # VLAN ID must be unique. vlan_network_&lt;VLAN ID&gt;
    name = f&#39;vlan-network-{vlan_id}&#39;
    create_tf_from_template(
        request,
        template_name,
        name=name,
        vlan_id=vlan_id)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    if import_flag:
        terraform_script = _get_node_script_path(
            request, &#39;terraform.sh&#39;, &#39;terraform&#39;) + \
                           &#39; &#39; + &#39;network&#39; + &#39; &#39; + name
    else:
        terraform_script = _get_node_script_path(
            request, &#39;terraform.sh&#39;, &#39;terraform&#39;)

    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    poll_for_resource_ready(request, admin_session,
                            harvester_api_endpoints.get_network % (name))
    resp = admin_session.get(harvester_api_endpoints.get_network % (
        name))
    assert resp.status_code == 200, &#39;Failed to get Network %s: %s&#39; % (
        name, resp.content)

    network_data = resp.json()
    return network_data</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_tf_from_template"><code class="name flex">
<span>def <span class="ident">create_tf_from_template</span></span>(<span>request, template_name, **template_args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_tf_from_template(request, template_name, **template_args):
    # get the current path relative to the ./templates/ directory
    my_path = os.path.dirname(os.path.realpath(__file__))
    # NOTE: the templates directory must be at the same level as
    # utilities.py, and all the templates must have the &#39;.yaml.j2&#39; extension
    templates_path = os.path.join(my_path, &#39;templates&#39;)
    template_file = f&#39;{templates_path}/{template_name}.tf.j2&#39;
    # now load the template
    with open(template_file) as tempfile:
        template = jinja2.Template(tempfile.read())
    # now render the template
#    template.globals[&#39;random_name&#39;] = random_name
    rendered = template.render(template_args)
    tf_file_dir = _get_node_script_path(request, script_type=&#39;terraform&#39;)
    tf_file_path = os.path.join(tf_file_dir, f&#39;{template_name}.tf&#39;)
    with open(tf_file_path, &#39;w&#39;) as f:
        f.write(rendered)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_vm"><code class="name flex">
<span>def <span class="ident">create_vm</span></span>(<span>request, admin_session, image, harvester_api_endpoints, template='basic_vm', keypair=None, volume=None, network=None, cpu=1, disk_size_gb=10, memory_gb=1, network_data=None, user_data=None, running=True, machine_type='q35', include_usb=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_vm(request, admin_session, image, harvester_api_endpoints,
              template=&#39;basic_vm&#39;, keypair=None, volume=None, network=None,
              cpu=1, disk_size_gb=10, memory_gb=1, network_data=None,
              user_data=None, running=True, machine_type=&#39;q35&#39;,
              include_usb=True):
    volume_name = None
    ssh_public_key = None
    network_name = None
    if network:
        network_name = network[&#39;metadata&#39;][&#39;name&#39;]
    if volume:
        volume_name = volume[&#39;metadata&#39;][&#39;name&#39;]
    if keypair:
        ssh_public_key = keypair[&#39;spec&#39;][&#39;publicKey&#39;]
    request_json = get_json_object_from_template(
        template,
        image_namespace=image[&#39;metadata&#39;][&#39;namespace&#39;],
        image_name=image[&#39;metadata&#39;][&#39;name&#39;],
        image_storage_class=image[&#39;status&#39;][&#39;storageClassName&#39;],
        volume_name=volume_name,
        network_name=network_name,
        disk_size_gb=disk_size_gb,
        cpu=cpu,
        memory_gb=memory_gb,
        ssh_public_key=ssh_public_key,
        network_data=network_data,
        user_data=user_data,
        machine_type=machine_type,
        include_usb=include_usb
    )
    request_json[&#39;spec&#39;][&#39;running&#39;] = running
    resp = admin_session.post(harvester_api_endpoints.create_vm,
                              json=request_json)
    assert resp.status_code == 201, (
        &#39;Failed to create VM %s: %s&#39; % (resp.status_code, resp.content))
    vm_resp_json = resp.json()
    if running:
        assert_vm_ready(request, admin_session, harvester_api_endpoints,
                        vm_resp_json[&#39;metadata&#39;][&#39;name&#39;], running)
    return vm_resp_json</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_vm_backup"><code class="name flex">
<span>def <span class="ident">create_vm_backup</span></span>(<span>request, admin_session, harvester_api_endpoints, backuptarget, name=None, vm_name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_vm_backup(request, admin_session, harvester_api_endpoints,
                     backuptarget, name=None, vm_name=None):
    request_json = get_json_object_from_template(
        &#39;basic_vm_backup&#39;,
        name=name,
        vm_name=vm_name
    )
    backuptarget_value = json.loads(backuptarget[&#39;value&#39;])
    backuptarget_type = backuptarget_value[&#39;type&#39;]
    if backuptarget_type == &#39;s3&#39;:
        total_objects_before_backup = get_total_objects_s3_bucket(request)
    else:
        total_objects_before_backup = get_total_objects_nfs_share(request)

    resp = admin_session.post(harvester_api_endpoints.create_vm_backup,
                              json=request_json)
    assert resp.status_code in [200, 201], &#39;Failed to create backup %s: %s&#39; % (
        name, resp.content)
    backup_json = resp.json()

    # wait for the backup to get ready
    time.sleep(30)

    def _wait_for_backup_become_active():
        # we want the update response to return back to the caller

        resp = admin_session.get(harvester_api_endpoints.get_vm_backup % (
            backup_json[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to get backup %s: %s&#39; % (
            backup_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
        resp_json = resp.json()
        if (&#39;status&#39; in resp_json and
                &#39;readyToUse&#39; in resp_json[&#39;status&#39;] and
                resp_json[&#39;status&#39;][&#39;readyToUse&#39;]):
            return True
        return False

    success = polling2.poll(
        _wait_for_backup_become_active,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Failed to Backup  VM: %s&#39; % (
        vm_name)

    if backuptarget_type == &#39;s3&#39;:
        total_objects_after_backup = get_total_objects_s3_bucket(request)
    else:
        total_objects_after_backup = get_total_objects_nfs_share(request)

    time.sleep(50)

    assert total_objects_before_backup &lt; total_objects_after_backup, (
        &#39;Failed to add any objects in %s target. &#39;
        &#39;Before backup object count: %s; after backup object count: %s&#39; % (
            backuptarget_type, total_objects_before_backup,
            total_objects_after_backup))
    return backup_json</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_vm_terraform"><code class="name flex">
<span>def <span class="ident">create_vm_terraform</span></span>(<span>request, admin_session, harvester_api_endpoints, template_name, keypair=None, image=None, volume=None, net=None, user_data=None, net_data=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_vm_terraform(request, admin_session, harvester_api_endpoints,
                        template_name,
                        keypair=None,
                        image=None,
                        volume=None,
                        net=None,
                        user_data=None,
                        net_data=None):
    name = &#39;t-&#39; + random_name()
    create_tf_from_template(
        request,
        template_name,
        name=name,
        image_name=image[&#39;metadata&#39;][&#39;name&#39;],
        vol_name=volume[&#39;metadata&#39;][&#39;name&#39;],
        net_name=net[&#39;metadata&#39;][&#39;name&#39;],
        keypair=keypair[&#39;metadata&#39;][&#39;name&#39;],
        public_key=keypair[&#39;spec&#39;][&#39;publicKey&#39;],
        user_data=user_data,
        net_data=net_data)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    assert_vm_ready(request, admin_session, harvester_api_endpoints,
                    name, running=True)
    resp = admin_session.get(harvester_api_endpoints.get_vm % (
        name))
    assert resp.status_code == 200, &#39;Failed to get VirtualMachine %s: %s&#39; % (
        name, resp.content)

    vm_data = resp.json()
    return vm_data</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.create_volume_terraform"><code class="name flex">
<span>def <span class="ident">create_volume_terraform</span></span>(<span>request, admin_session, harvester_api_endpoints, template_name, size, image=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_volume_terraform(request, admin_session, harvester_api_endpoints,
                            template_name, size, image=None):
    name = &#34;t-&#34; + random_name()
    create_tf_from_template(
        request,
        template_name,
        name=name,
        size=size,
        image=image)

    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_script = _get_node_script_path(
        request, &#39;terraform.sh&#39;, &#39;terraform&#39;)
    result = subprocess.run([terraform_script], shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))

    resp = admin_session.get(harvester_api_endpoints.get_volume % (
        name))
    assert resp.status_code == 200, &#39;Failed to get Volume %s: %s&#39; % (
        name, resp.content)

    vol_data = resp.json()
    return vol_data</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.delete_host"><code class="name flex">
<span>def <span class="ident">delete_host</span></span>(<span>request, admin_session, harvester_api_endpoints, host_json)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_host(request, admin_session, harvester_api_endpoints, host_json):
    resp = admin_session.delete(harvester_api_endpoints.delete_node % (
        host_json[&#39;id&#39;]))
    assert resp.status_code in [200, 201], &#39;Unable to delete host %s: %s&#39; % (
        host_json[&#39;id&#39;], resp.content)
    # wait for host to be deleted
    time.sleep(180)

    def _wait_for_host_to_be_deleted():
        resp = admin_session.get(harvester_api_endpoints.get_node % (
            host_json[&#39;id&#39;]))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _wait_for_host_to_be_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for host to be deleted&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.delete_image"><code class="name flex">
<span>def <span class="ident">delete_image</span></span>(<span>request, admin_session, harvester_api_endpoints, image_json)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_image(request, admin_session, harvester_api_endpoints, image_json):
    delete_image_by_name(request,
                         admin_session,
                         harvester_api_endpoints,
                         image_json[&#39;metadata&#39;][&#39;name&#39;])</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.delete_image_by_name"><code class="name flex">
<span>def <span class="ident">delete_image_by_name</span></span>(<span>request, admin_session, harvester_api_endpoints, image_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_image_by_name(request, admin_session,
                         harvester_api_endpoints, image_name):
    # see if the image exist first
    resp = admin_session.get(harvester_api_endpoints.get_image % (image_name))
    if resp.status_code == 404:
        # image doesn&#39;t exist so nothing to be done
        return

    def _wait_for_image_to_be_deleted():
        # retry delete
        admin_session.delete(harvester_api_endpoints.delete_image %
                             (image_name))
        time.sleep(15)
        resp = admin_session.get(harvester_api_endpoints.get_image %
                                 (image_name))
        if resp.status_code == 404:
            return True
        return False

    try:
        polling2.poll(
            _wait_for_image_to_be_deleted,
            step=5,
            timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    except polling2.TimeoutException:
        errmsg = &#39;Timed out while waiting for image to be deleted&#39;
        raise AssertionError(errmsg)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.delete_vm"><code class="name flex">
<span>def <span class="ident">delete_vm</span></span>(<span>request, admin_session, harvester_api_endpoints, vm_json, remove_all_disks=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_vm(request, admin_session, harvester_api_endpoints, vm_json,
              remove_all_disks=True):
    resp = admin_session.delete(harvester_api_endpoints.delete_vm % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code in [200, 201], &#39;Failed to delete VM %s: %s&#39; % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)

    def _check_vm_deleted():
        resp = admin_session.get(harvester_api_endpoints.get_vm % (
            vm_json[&#39;metadata&#39;][&#39;name&#39;]))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _check_vm_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for VM to be terminated.&#39;
    if remove_all_disks:
        # NOTE: for PVC, we must explicitly delete the volumes after the
        # VM is deleted
        volumes = vm_json[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;volumes&#39;]
        for volume in volumes:
            if &#39;persistentVolumeClaim&#39; in volume:
                delete_volume_by_name(
                    request, admin_session, harvester_api_endpoints,
                    volume[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;],
                    owned_by=vm_json[&#39;metadata&#39;][&#39;name&#39;])</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.delete_vm_backup"><code class="name flex">
<span>def <span class="ident">delete_vm_backup</span></span>(<span>request, admin_session, harvester_api_endpoints, backuptarget, backup_json)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_vm_backup(request, admin_session,
                     harvester_api_endpoints, backuptarget, backup_json):

    backuptarget_value = json.loads(backuptarget[&#39;value&#39;])
    backuptarget_type = backuptarget_value[&#39;type&#39;]
    if backuptarget_type == &#39;s3&#39;:
        total_objects_before_delete = get_total_objects_s3_bucket(request)
    else:
        total_objects_before_delete = get_total_objects_nfs_share(request)
    resp = admin_session.delete(harvester_api_endpoints.delete_vm_backup % (

        backup_json[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code in [200, 201], &#39;Unable to del backup %s: %s&#39; % (
        backup_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)

    def _wait_for_backup_to_be_deleted():
        resp = admin_session.get(harvester_api_endpoints.get_vm_backup % (
            backup_json[&#39;metadata&#39;][&#39;name&#39;]))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _wait_for_backup_to_be_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for backup to be deleted&#39;

    time.sleep(50)
    if backuptarget_type == &#39;s3&#39;:
        total_objects_after_delete = get_total_objects_s3_bucket(request)
    else:
        total_objects_after_delete = get_total_objects_nfs_share(request)

    assert total_objects_before_delete &gt; total_objects_after_delete, (
        &#39;Failed to delete any objects from %s target. &#39;
        &#39;Before delete object count: %s; after delete object count: %s&#39; % (
            backuptarget_type, total_objects_before_delete,
            total_objects_after_delete))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.delete_volume"><code class="name flex">
<span>def <span class="ident">delete_volume</span></span>(<span>request, admin_session, harvester_api_endpoints, volume_json)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_volume(request, admin_session, harvester_api_endpoints,
                  volume_json):
    delete_volume_by_name(request, admin_session, harvester_api_endpoints,
                          volume_json[&#39;metadata&#39;][&#39;name&#39;])</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.delete_volume_by_name"><code class="name flex">
<span>def <span class="ident">delete_volume_by_name</span></span>(<span>request, admin_session, harvester_api_endpoints, volume_name, owned_by=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_volume_by_name(request, admin_session, harvester_api_endpoints,
                          volume_name, owned_by=None):
    # see if the volume exist first
    resp = admin_session.get(harvester_api_endpoints.get_volume % (
        volume_name))
    if resp.status_code == 404:
        # volume doesn&#39;t exist so nothing to be done
        return

    def _wait_for_vm_remove_owned_by():
        if owned_by is None:
            return True
        resp = admin_session.get(harvester_api_endpoints.get_volume % (
            volume_name))
        volume_json = resp.json()
        annotations = volume_json[&#39;metadata&#39;][&#39;annotations&#39;]
        if &#39;harvesterhci.io/owned-by&#39; in annotations:
            if owned_by in annotations[&#39;harvesterhci.io/owned-by&#39;]:
                return False
            return True
        else:
            return True

    success = polling2.poll(
        _wait_for_vm_remove_owned_by,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))

    resp = admin_session.delete(harvester_api_endpoints.delete_volume % (
        volume_name))
    assert resp.status_code in [200, 201], (
        &#39;Failed to delete volume %s: %s&#39; % (volume_name, resp.content))

    def _check_volume_deleted():
        resp = admin_session.get(harvester_api_endpoints.delete_volume % (
            volume_name))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _check_volume_deleted,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for volume to be terminated.&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.destroy_resource"><code class="name flex">
<span>def <span class="ident">destroy_resource</span></span>(<span>request, admin_session, destroy_type=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def destroy_resource(request, admin_session, destroy_type=None):
    create_kubeconfig_from_template(
        request,
        &#39;kube_config&#39;,
        harvester_endpoint=request.config.getoption(&#39;--endpoint&#39;),
        token=(admin_session.headers[&#39;authorization&#39;]).split()[1]
    )

    terraform_path = _get_node_script_path(
        request, script_type=&#39;terraform&#39;)
    if os.path.isdir(os.path.join(terraform_path, &#39;terraformharvester&#39;)):
        terraform_script = _get_node_script_path(
            request, &#39;terraform_destroy.sh&#39;, &#39;terraform&#39;) + &#39; &#39; + destroy_type
        result = subprocess.run([terraform_script], shell=True,
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        assert result.returncode == 0, (
            &#39;Failed to run terraform : rc: %s, stdout: %s, stderr: %s&#39; % (
                result.returncode, result.stderr, result.stdout))

    if os.path.isdir(os.path.join(terraform_path, &#39;.kube&#39;)):
        shutil.rmtree(os.path.join(terraform_path, &#39;.kube&#39;))

    if os.path.isfile(os.path.join(terraform_path, &#39;provider.tf&#39;)):
        os.remove(os.path.join(terraform_path, &#39;provider.tf&#39;))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.disable_maintenance_mode"><code class="name flex">
<span>def <span class="ident">disable_maintenance_mode</span></span>(<span>request, admin_session, harvester_api_endpoints, node_json)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def disable_maintenance_mode(request, admin_session, harvester_api_endpoints,
                             node_json):
    # NOTE(gyee): implementation of
    # https://github.com/harvester/harvester/blob/
    # 3edc82a7ae6a5de6e8114901058dc573938093e8/pkg/api/node/formatter.go#L118
    node_json[&#39;spec&#39;][&#39;unschedulable&#39;] = False
    if &#39;taints&#39; in node_json[&#39;spec&#39;]:
        node_json[&#39;spec&#39;][&#39;taints&#39;] = [
            t for t in node_json[&#39;spec&#39;][&#39;taints&#39;] if not (
                t[&#39;key&#39;] == &#39;kubevirt.io/drain&#39;)]
    del node_json[&#39;metadata&#39;][&#39;annotations&#39;][&#39;harvesterhci.io/maintain-status&#39;]
    poll_for_update_resource(request, admin_session,
                             node_json[&#39;links&#39;][&#39;update&#39;],
                             node_json,
                             harvester_api_endpoints.get_node % (
                                 node_json[&#39;metadata&#39;][&#39;name&#39;]))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.enable_maintenance_mode"><code class="name flex">
<span>def <span class="ident">enable_maintenance_mode</span></span>(<span>request, admin_session, harvester_api_endpoints, node_json)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enable_maintenance_mode(request, admin_session, harvester_api_endpoints,
                            node_json):

    def _add_drain_taint(node_json):
        if &#39;taints&#39; in node_json[&#39;spec&#39;]:
            for taint in node_json[&#39;spec&#39;][&#39;taints&#39;]:
                if taint[&#39;key&#39;] == &#39;kubevirt.io/drain&#39;:
                    return
        else:
            node_json[&#39;spec&#39;][&#39;taints&#39;] = []
        node_json[&#39;spec&#39;][&#39;taints&#39;].append(
            {
                &#39;key&#39;: &#39;kubevirt.io/drain&#39;,
                &#39;value&#39;: &#39;scheduling&#39;,
                &#39;effect&#39;: &#39;NoSchedule&#39;
            }
        )

    # NOTE(gyee): implemention of
    # https://github.com/harvester/harvester/blob/
    # 3edc82a7ae6a5de6e8114901058dc573938093e8/pkg/api/node/formatter.go#L92
    node_json[&#39;spec&#39;][&#39;unschedulable&#39;] = True
    _add_drain_taint(node_json)
    if &#39;annotations&#39; not in node_json[&#39;metadata&#39;]:
        node_json[&#39;metadata&#39;][&#39;annotations&#39;] = {}
    node_json[&#39;metadata&#39;][&#39;annotations&#39;][&#39;harvesterhci.io/maintain-status&#39;] = (
        &#39;running&#39;)
    poll_for_update_resource(request, admin_session,
                             node_json[&#39;links&#39;][&#39;update&#39;],
                             node_json,
                             harvester_api_endpoints.get_node % (
                                 node_json[&#39;metadata&#39;][&#39;name&#39;]))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.execute_script_on_vm"><code class="name flex">
<span>def <span class="ident">execute_script_on_vm</span></span>(<span>ip, timeout, script, keypair=None, script_params=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute_script_on_vm(ip, timeout, script, keypair=None,
                         script_params=None):
    ssh_client = wait_for_ssh_client(ip, timeout, keypair)
    # first copy the script to the /tmp dir on the VM
    with SCPClient(ssh_client.get_transport()) as scp:
        scp.put(script, &#39;/tmp&#39;)
    # now execute the script on the VM and return stdout as string
    command = &#39;bash /tmp/%s&#39; % (os.path.basename(script))
    if script_params:
        command += &#39; &#39; + script_params
    stdin, stdout, stderr = ssh_client.exec_command(command)
    ssh_client.close()
    errors = stderr.read().strip().decode(&#39;utf-8&#39;)
    assert not errors, (
        &#39;Failed to execute %s on %s: %s&#39; % (script, ip, errors))
    return stdout.read().strip().decode(&#39;utf-8&#39;)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.get_backup_create_files_script"><code class="name flex">
<span>def <span class="ident">get_backup_create_files_script</span></span>(<span>request, script_name=None, script_type=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_backup_create_files_script(request,
                                   script_name=None,
                                   script_type=None):
    script = _get_node_script_path(request, script_name, script_type)
    return script</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.get_json_object_from_template"><code class="name flex">
<span>def <span class="ident">get_json_object_from_template</span></span>(<span>template_name, **template_args)</span>
</code></dt>
<dd>
<div class="desc"><p>Load template from template file</p>
<p>:param template_name: the name of the template. It is the filename of
template without the file extension. For example, if
you want to load './templates/foo.json.j2', then the
template_name should be 'foo'.
:param template_args: dictionary of template argument values for the given
Jinja2 template</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_json_object_from_template(template_name, **template_args):
    &#34;&#34;&#34;Load template from template file

    :param template_name: the name of the template. It is the filename of
                          template without the file extension. For example, if
                          you want to load &#39;./templates/foo.json.j2&#39;, then the
                          template_name should be &#39;foo&#39;.
    :param template_args: dictionary of template argument values for the given
                          Jinja2 template
    &#34;&#34;&#34;
    # get the current path relative to the ./templates/ directory
    my_path = os.path.dirname(os.path.realpath(__file__))
    # NOTE: the templates directory must be at the same level as
    # utilities.py, and all the templates must have the &#39;.yaml.j2&#39; extension
    templates_path = os.path.join(my_path, &#39;templates&#39;)
    template_file = f&#39;{templates_path}/{template_name}.json.j2&#39;
    # now load the template
    with open(template_file) as tempfile:
        template = jinja2.Template(tempfile.read())
    template.globals[&#39;random_name&#39;] = random_name
    template.globals[&#39;random_alphanumeric&#39;] = random_alphanumeric
    # now render the template
    rendered = template.render(template_args)
    return json.loads(rendered)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.get_latest_resource_version"><code class="name flex">
<span>def <span class="ident">get_latest_resource_version</span></span>(<span>request, admin_session, lookup_endpoint)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_latest_resource_version(request, admin_session, lookup_endpoint):
    poll_for_resource_ready(request, admin_session, lookup_endpoint)
    resp = admin_session.get(lookup_endpoint)
    assert resp.status_code == 200, &#39;Failed to lookup resource: %s&#39; % (
        resp.content)
    return resp.json()[&#39;metadata&#39;][&#39;resourceVersion&#39;]</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.get_total_objects_nfs_share"><code class="name flex">
<span>def <span class="ident">get_total_objects_nfs_share</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_total_objects_nfs_share(request):
    backup_script = get_backup_create_files_script(
        request, &#39;mountnfs.sh&#39;, &#39;backup&#39;)
    nfsendpoint = (request.config.getoption(&#39;--nfs-endpoint&#39;).split(&#34;//&#34;)[1])
    nfsmountdir = request.config.getoption(&#39;--nfs-mount-dir&#39;)
    total_objects = 0
    result = subprocess.run([backup_script, nfsendpoint, nfsmountdir],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to run mountnfs : rc: %s, stdout: %s, stderr: %s&#39; % (
            result.returncode, result.stderr, result.stdout))
    total_objects = int(result.stdout.decode(&#34;utf-8&#34;))
    return total_objects</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.get_total_objects_s3_bucket"><code class="name flex">
<span>def <span class="ident">get_total_objects_s3_bucket</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_total_objects_s3_bucket(request):
    accesskey = request.config.getoption(&#39;--accessKeyId&#39;)
    secretaccesskey = request.config.getoption(&#39;--secretAccessKey&#39;)
    bucket = request.config.getoption(&#39;--bucketName&#39;)
    region = request.config.getoption(&#39;--region&#39;)
    totalCount = 0
    s3 = boto3.resource(&#34;s3&#34;,
                        region_name=region,
                        aws_access_key_id=accesskey,
                        aws_secret_access_key=secretaccesskey)
    s3bucket = s3.Bucket(bucket)
    for key in s3bucket.objects.all():
        totalCount += 1

    return totalCount</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.get_vm_ip_address"><code class="name flex">
<span>def <span class="ident">get_vm_ip_address</span></span>(<span>admin_session, harvester_api_endpoints, vm, timeout, nic_name='default')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vm_ip_address(admin_session, harvester_api_endpoints, vm, timeout,
                      nic_name=&#39;default&#39;):
    vm_instance_json = None

    def _wait_for_ip():
        nonlocal vm_instance_json
        vm_instance_json = lookup_vm_instance(
            admin_session, harvester_api_endpoints, vm)
        for interface in vm_instance_json[&#39;status&#39;][&#39;interfaces&#39;]:
            # NOTE: by default, the second NIC name is &#39;nic-1&#39;
            if (interface[&#39;name&#39;] == nic_name and
                    &#39;ipAddress&#39; in interface):
                return interface[&#39;ipAddress&#39;]

    try:
        ip = polling2.poll(_wait_for_ip, step=5, timeout=timeout)
    except polling2.TimeoutException:
        errmsg = (&#39;Timed out while waiting for IP address for NIC %s to be &#39;
                  &#39; assigned: %s&#39; % (nic_name, vm_instance_json))
        raise AssertionError(errmsg)

    return (vm_instance_json, ip)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.is_marker_enabled"><code class="name flex">
<span>def <span class="ident">is_marker_enabled</span></span>(<span>request, marker_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_marker_enabled(request, marker_name):
    for item in request.session.items:
        if item.get_closest_marker(marker_name) is not None:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.lookup_host_not_harvester_endpoint"><code class="name flex">
<span>def <span class="ident">lookup_host_not_harvester_endpoint</span></span>(<span>request, admin_session, harvester_api_endpoints)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lookup_host_not_harvester_endpoint(request, admin_session,
                                       harvester_api_endpoints):
    cur_endpoint = (request.config.getoption(&#39;--endpoint&#39;).split(&#34;:&#34;)[1])[2:]
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    for node in nodes_json:
        # look up CPU usage for the given node
        if node[&#39;metadata&#39;][&#39;annotations&#39;].get(
                &#39;etcd.k3s.cattle.io/node-address&#39;) != cur_endpoint:
            node_data = node
    return node_data</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.lookup_hosts_with_cpu_and_memory"><code class="name flex">
<span>def <span class="ident">lookup_hosts_with_cpu_and_memory</span></span>(<span>admin_session, harvester_api_endpoints, cpu, memory)</span>
</code></dt>
<dd>
<div class="desc"><p>Lookup nodes that satisfies the given CPU and memory requirements</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lookup_hosts_with_cpu_and_memory(admin_session, harvester_api_endpoints,
                                     cpu, memory):
    &#34;&#34;&#34;Lookup nodes that satisfies the given CPU and memory requirements&#34;&#34;&#34;
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    nodes = []
    for node in nodes_json:
        # look up CPU usage for the given node
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, (
            &#39;Failed to lookup metrices for node %s: %s&#39; % (
                node[&#39;metadata&#39;][&#39;name&#39;], resp.content))
        metrics_json = resp.json()
        # NOTE: Kubernets CPU metrics are expressed in nanocores, or
        # 1 billionth of a CPU. We need to convert it to a whole CPU core.
        cpu_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;cpu&#39;][:-1]) / 1000000000)
        available_cpu = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;cpu&#39;]) - cpu_usage
        # NOTE: Kubernets memory metrics are expressed Kibibyte so convert it
        # back to Gigabytes
        memory_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;memory&#39;][:-2]) * 1.024e-06)
        # NOTE: we want the floor here so we don&#39;t over commit
        allocatable_memory = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;memory&#39;][:-2])
        allocatable_memory = math.floor(
            allocatable_memory * 1.024e-06)
        available_memory = allocatable_memory - memory_usage
        if available_cpu &gt;= cpu and available_memory &gt;= memory:
            nodes.append(node[&#39;metadata&#39;][&#39;name&#39;])
    return nodes</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.lookup_hosts_with_most_available_cpu"><code class="name flex">
<span>def <span class="ident">lookup_hosts_with_most_available_cpu</span></span>(<span>admin_session, harvester_api_endpoints)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lookup_hosts_with_most_available_cpu(admin_session,
                                         harvester_api_endpoints):
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    most_available_cpu_nodes = None
    most_available_cpu = 0
    for node in nodes_json:
        # look up CPU usage for the given node
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, (
            &#39;Failed to lookup metrices for node %s: %s&#39; % (
                node[&#39;metadata&#39;][&#39;name&#39;], resp.content))
        metrics_json = resp.json()
        # NOTE: Kubernets CPU metrics are expressed in nanocores, or
        # 1 billionth of a CPU. We need to convert it to a whole CPU core.
        cpu_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;cpu&#39;][:-1]) / 1000000000)
        available_cpu = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;cpu&#39;]) - cpu_usage
        if available_cpu &gt; most_available_cpu:
            most_available_cpu = available_cpu
            most_available_cpu_nodes = [node[&#39;metadata&#39;][&#39;name&#39;]]
        elif available_cpu == most_available_cpu:
            most_available_cpu_nodes.append(node[&#39;metadata&#39;][&#39;name&#39;])
    return (most_available_cpu_nodes, most_available_cpu)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.lookup_hosts_with_most_available_memory"><code class="name flex">
<span>def <span class="ident">lookup_hosts_with_most_available_memory</span></span>(<span>admin_session, harvester_api_endpoints)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lookup_hosts_with_most_available_memory(admin_session,
                                            harvester_api_endpoints):
    resp = admin_session.get(harvester_api_endpoints.list_nodes)
    assert resp.status_code == 200, &#39;Failed to list nodes: %s&#39; % (resp.content)
    nodes_json = resp.json()[&#39;data&#39;]
    most_available_memory_nodes = None
    most_available_memory = 0
    for node in nodes_json:
        # look up CPU usage for the given node
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, (
            &#39;Failed to lookup metrices for node %s: %s&#39; % (
                node[&#39;metadata&#39;][&#39;name&#39;], resp.content))
        metrics_json = resp.json()
        # NOTE: Kubernets memory metrics are expressed Kibibyte so convert it
        # back to Gigabytes
        memory_usage = math.ceil(
            int(metrics_json[&#39;usage&#39;][&#39;memory&#39;][:-2]) * 1.024e-06)
        # NOTE: we want the floor here so we don&#39;t over commit
        allocatable_memory = int(node[&#39;status&#39;][&#39;allocatable&#39;][&#39;memory&#39;][:-2])
        allocatable_memory = math.floor(
            allocatable_memory * 1.024e-06)
        available_memory = allocatable_memory - memory_usage
        if available_memory &gt; most_available_memory:
            most_available_memory = available_memory
            most_available_memory_nodes = [node[&#39;metadata&#39;][&#39;name&#39;]]
        elif available_memory == most_available_memory:
            most_available_memory_nodes.append(node[&#39;metadata&#39;][&#39;name&#39;])
    return (most_available_memory_nodes, most_available_memory)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.lookup_vm_instance"><code class="name flex">
<span>def <span class="ident">lookup_vm_instance</span></span>(<span>admin_session, harvester_api_endpoints, vm_json)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lookup_vm_instance(admin_session, harvester_api_endpoints, vm_json):
    # NOTE(gyee): seem like the corresponding VM instance has the same name as
    # the VM. If this assumption is not true, we need to fix this code.
    resp = admin_session.get(harvester_api_endpoints.get_vm_instance % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;]))
    assert resp.status_code == 200, &#39;Failed to lookup VM instance %s: %s&#39; % (
        vm_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
    return resp.json()</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.poll_for_resource_ready"><code class="name flex">
<span>def <span class="ident">poll_for_resource_ready</span></span>(<span>request, admin_session, endpoint, expected_code=200)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def poll_for_resource_ready(request, admin_session, endpoint,
                            expected_code=200):
    ready = polling2.poll(
        lambda: admin_session.get(endpoint).status_code == expected_code,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert ready, &#39;Timed out while waiting for %s to yield %s&#39; % (
        endpoint, expected_code)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.poll_for_update_resource"><code class="name flex">
<span>def <span class="ident">poll_for_update_resource</span></span>(<span>request, admin_session, update_endpoint, request_json, lookup_endpoint, use_yaml=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def poll_for_update_resource(request, admin_session, update_endpoint,
                             request_json, lookup_endpoint, use_yaml=None):

    resp = None

    def _update_resource():
        # we want the update response to return back to the caller
        nonlocal resp

        # first we need to get the latest resourceVersion and fill that in
        # the request_json as it is a required field and must be the latest.
        request_json[&#39;metadata&#39;][&#39;resourceVersion&#39;] = (
            get_latest_resource_version(
                request, admin_session, lookup_endpoint))
        if use_yaml:
            resp = admin_session.put(update_endpoint,
                                     data=yaml.dump(
                                         request_json, sort_keys=False),
                                     headers={
                                         &#39;Content-Type&#39;: &#39;application/yaml&#39;})
        else:
            resp = admin_session.put(update_endpoint, json=request_json)
        if resp.status_code in [409, 500]:
            return False
        else:
            assert resp.status_code == 200, &#39;Failed to update resource: %s&#39; % (
                resp.content)
            return True

    # NOTE(gyee): we need to do retries because kubenetes cluster does not
    # guarantee freshness when updating resources because of the way it handles
    # queuing. See
    # https://github.com/kubernetes/kubernetes/issues/84430
    # Therefore, we must do fetch-retry when updating resources.
    # Apparently this is way of life in Kubernetes world.
    updated = polling2.poll(
        _update_resource,
        step=3,
        timeout=120)
    assert updated, &#39;Timed out while waiting to update resource: %s&#39; % (
        update_endpoint)
    return resp</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.power_off_node"><code class="name flex">
<span>def <span class="ident">power_off_node</span></span>(<span>request, admin_session, harvester_api_endpoints, node_name, node_ip=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def power_off_node(request, admin_session, harvester_api_endpoints, node_name,
                   node_ip=None):
    power_off_script = _get_node_script_path(request, &#39;power_off.sh&#39;)
    if node_ip is None:
        node_ip = _lookup_node_ip(admin_session, harvester_api_endpoints,
                                  node_name)
    result = subprocess.run([power_off_script, node_name, node_ip],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to power-off node %s: rc: %s, stdout: %s, stderr: %s&#39; % (
            node_name, result.returncode, result.stderr, result.stdout))

    # wait for the node to disappear
    time.sleep(120)

    def _wait_for_node_to_disappear():
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node_name))
        metrics_json = resp.json()
        if &#39;status&#39; in metrics_json and metrics_json[&#39;status&#39;] == 404:
            return True
        return False

    success = polling2.poll(
        _wait_for_node_to_disappear,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for node to shutdown&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.power_on_node"><code class="name flex">
<span>def <span class="ident">power_on_node</span></span>(<span>request, admin_session, harvester_api_endpoints, node_name, node_ip=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def power_on_node(request, admin_session, harvester_api_endpoints, node_name,
                  node_ip=None):
    power_on_script = _get_node_script_path(request, &#39;power_on.sh&#39;)
    if node_ip is None:
        node_ip = _lookup_node_ip(admin_session, harvester_api_endpoints,
                                  node_name)
    result = subprocess.run([power_on_script, node_name, node_ip],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to power-on node %s: rc: %s, stdout: %s, stderr: %s&#39; % (
            node_name, result.returncode, result.stderr, result.stdout))

    # wait for the node to power-on
    time.sleep(180)

    def _wait_for_node_to_appear():
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node_name))
        metrics_json = resp.json()
        if (&#39;metadata&#39; in metrics_json and
                &#39;state&#39; in metrics_json[&#39;metadata&#39;] and
                metrics_json[&#39;metadata&#39;][&#39;state&#39;][&#39;error&#39;] is False):
            return True
        return False

    success = polling2.poll(
        _wait_for_node_to_appear,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for node to power-on&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.poweroff_host_maintenance_mode"><code class="name flex">
<span>def <span class="ident">poweroff_host_maintenance_mode</span></span>(<span>request, admin_session, harvester_api_endpoints)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def poweroff_host_maintenance_mode(request, admin_session,
                                   harvester_api_endpoints):
    host_poweroff = lookup_host_not_harvester_endpoint(request, admin_session,
                                                       harvester_api_endpoints)
    # Enable Maintenance Mode
    enable_maintenance_mode(request, admin_session,
                            harvester_api_endpoints, host_poweroff)
    node_name = host_poweroff[&#39;id&#39;]
    poll_for_resource_ready(request, admin_session,
                            harvester_api_endpoints.get_node % (node_name))
    resp = admin_session.get(
        harvester_api_endpoints.get_node % (node_name))
    resp.status_code == 200, &#39;Failed to get host: %s&#39; % (resp.content)
    ret_data = resp.json()
    assert ret_data[&#34;spec&#34;][&#34;unschedulable&#34;]
    s = ret_data[&#34;metadata&#34;][&#34;annotations&#34;][&#34;harvesterhci.io/maintain-status&#34;]
    assert s in [&#34;running&#34;, &#34;completed&#34;]
    node_name = host_poweroff[&#39;id&#39;]
    # Power Off Node
    power_off_node(request, admin_session, harvester_api_endpoints,
                   node_name)
    resp = admin_session.get(
        harvester_api_endpoints.get_node % (node_name))
    resp.status_code == 200, &#39;Failed to get host: %s&#39; % (resp.content)
    ret_data = resp.json()
    assert &#34;NotReady,SchedulingDisabled&#34; in ret_data[&#34;metadata&#34;][&#34;fields&#34;]
    return host_poweroff</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.random_alphanumeric"><code class="name flex">
<span>def <span class="ident">random_alphanumeric</span></span>(<span>length=5, upper_case=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a random alphanumeric string of given length</p>
<p>:param length: the size of the string
:param upper_case: whether to return the upper case string</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_alphanumeric(length=5, upper_case=False):
    &#34;&#34;&#34;Generate a random alphanumeric string of given length

    :param length: the size of the string
    :param upper_case: whether to return the upper case string
    &#34;&#34;&#34;
    if upper_case:
        return &#39;&#39;.join(random.choice(
            string.ascii_uppercase + string.digits) for _ in range(length))
    else:
        return &#39;&#39;.join(random.choice(
            string.ascii_lowercase + string.digits) for _ in range(length))</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.random_name"><code class="name flex">
<span>def <span class="ident">random_name</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a random alphanumeric name using uuid.uuid4()</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_name():
    &#34;&#34;&#34;Generate a random alphanumeric name using uuid.uuid4()&#34;&#34;&#34;
    return uuid.uuid4().hex</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.reboot_node"><code class="name flex">
<span>def <span class="ident">reboot_node</span></span>(<span>request, admin_session, harvester_api_endpoints, node_name, node_ip=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reboot_node(request, admin_session, harvester_api_endpoints, node_name,
                node_ip=None):
    reboot_script = _get_node_script_path(request, &#39;reboot.sh&#39;)
    if node_ip is None:
        node_ip = _lookup_node_ip(admin_session, harvester_api_endpoints,
                                  node_name)
    result = subprocess.run([reboot_script, node_name, node_ip],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    assert result.returncode == 0, (
        &#39;Failed to reboot node %s: rc: %s, stdout: %s, stderr: %s&#39; % (
            node_name, result.returncode, result.stderr, result.stdout))

    # wait for the node to power-on
    time.sleep(180)

    def _wait_for_node_to_appear():
        resp = admin_session.get(harvester_api_endpoints.get_node_metrics % (
            node_name))
        metrics_json = resp.json()
        if (&#39;metadata&#39; in metrics_json and
                &#39;state&#39; in metrics_json[&#39;metadata&#39;] and
                metrics_json[&#39;metadata&#39;][&#39;state&#39;][&#39;error&#39;] is False):
            return True
        return False

    success = polling2.poll(
        _wait_for_node_to_appear,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Timed out while waiting for node to reboot&#39;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.restart_vm"><code class="name flex">
<span>def <span class="ident">restart_vm</span></span>(<span>admin_session, harvester_api_endpoints, previous_uid, vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restart_vm(admin_session, harvester_api_endpoints, previous_uid, vm_name,
               wait_timeout):
    resp = admin_session.put(harvester_api_endpoints.restart_vm % (
        vm_name))
    assert resp.status_code == 202, &#39;Failed to restart VM instance %s: %s&#39; % (
        vm_name, resp.content)
    assert_vm_restarted(admin_session, harvester_api_endpoints, previous_uid,
                        vm_name, wait_timeout)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.restore_vm_backup"><code class="name flex">
<span>def <span class="ident">restore_vm_backup</span></span>(<span>request, admin_session, harvester_api_endpoints, name=None, vm_name=None, backup_name=None, vm_new=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restore_vm_backup(request, admin_session, harvester_api_endpoints,
                      name=None, vm_name=None,
                      backup_name=None, vm_new=None):
    request_json = get_json_object_from_template(
        &#39;basic_vm_restore&#39;,
        name=name,
        vm_name=vm_name,
        backup_name=backup_name
    )
    if vm_new:
        request_json[&#39;spec&#39;][&#39;newVM&#39;] = vm_new
    resp = admin_session.post(harvester_api_endpoints.create_vm_restore,
                              json=request_json)
    assert resp.status_code in [200, 201], &#39;Failed to restore bakup %s: %s&#39; % (
        backup_name, resp.content)
    restore_json = resp.json()

    # wait for the restore to get ready
    time.sleep(30)

    def _wait_for_restore_to_finish():
        # we want the update response to return back to the caller

        resp = admin_session.get(harvester_api_endpoints.get_vm_restore % (
            restore_json[&#39;metadata&#39;][&#39;name&#39;]))
        assert resp.status_code == 200, &#39;Failed to restore %s: %s&#39; % (
            restore_json[&#39;metadata&#39;][&#39;name&#39;], resp.content)
        resp_json = resp.json()
        if (&#39;status&#39; in resp_json and
                &#39;complete&#39; in resp_json[&#39;status&#39;] and
                resp_json[&#39;status&#39;][&#39;complete&#39;]):
            return True
        return False

    success = polling2.poll(
        _wait_for_restore_to_finish,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Failed to Restore  VM: %s&#39; % (
        vm_name)

    return restore_json</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.retry_session"><code class="name flex">
<span>def <span class="ident">retry_session</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a session that will retry on connection errors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def retry_session():
    &#34;&#34;&#34;Create a session that will retry on connection errors&#34;&#34;&#34;
    # TODO(gyee): should we make retries and backoff_factor configurable?
    # We should retry on connection error only.
    # See https://urllib3.readthedocs.io/en/latest/reference/
    # urllib3.util.html#urllib3.util.Retry for more information.
    allowed_methods = frozenset({&#39;DELETE&#39;, &#39;GET&#39;, &#39;HEAD&#39;, &#39;OPTIONS&#39;, &#39;PUT&#39;,
                                 &#39;TRACE&#39;, &#39;POST&#39;})
    retry_strategy = Retry(total=5, backoff_factor=10.0,
                           status_forcelist=[500],
                           allowed_methods=allowed_methods)
    adapter = HTTPAdapter(max_retries=retry_strategy)
    s = requests.Session()
    # TODO(gyee): do we need to support other auth methods?
    # NOTE(gyee): ignore SSL certificate validation for now
    s.verify = False
    s.mount(&#39;https://&#39;, adapter)
    s.mount(&#39;http://&#39;, adapter)
    return s</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.stop_vm"><code class="name flex">
<span>def <span class="ident">stop_vm</span></span>(<span>request, admin_session, harvester_api_endpoints, vm_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_vm(request, admin_session, harvester_api_endpoints,
            vm_name):
    resp = admin_session.put(harvester_api_endpoints.stop_vm % (
        vm_name))
    assert resp.status_code == 202, &#39;Failed to stop VM instance %s&#39; % (
        vm_name)

    # give it some time for the VM instance to stop
    time.sleep(120)

    def _check_vm_instance_stopped():
        resp = admin_session.get(
            harvester_api_endpoints.get_vm_instance % (
                vm_name))
        if resp.status_code == 404:
            return True
        return False

    success = polling2.poll(
        _check_vm_instance_stopped,
        step=5,
        timeout=request.config.getoption(&#39;--wait-timeout&#39;))
    assert success, &#39;Failed to stop VM: %s&#39; % (
        vm_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.utils.wait_for_ssh_client"><code class="name flex">
<span>def <span class="ident">wait_for_ssh_client</span></span>(<span>ip, timeout, keypair=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_ssh_client(ip, timeout, keypair=None):
    client = SSHClient()
    # automatically add host since we only care about connectivity
    client.set_missing_host_key_policy(AutoAddPolicy)

    def _wait_for_connect():
        try:
            # NOTE: for the default openSUSE Leap image, the root user
            # password is &#39;linux&#39;
            if keypair is not None:
                private_key = RSAKey.from_private_key(
                    StringIO(keypair[&#39;spec&#39;][&#39;privateKey&#39;]))
                client.connect(ip, username=&#39;root&#39;, pkey=private_key)
            else:
                client.connect(ip, username=&#39;root&#39;, password=&#39;linux&#39;)
        except Exception as e:
            print(&#39;Unable to connect to %s: %s&#39; % (ip, e))
            return False
        return True

    ready = polling2.poll(
        _wait_for_connect,
        step=5,
        timeout=timeout)
    assert ready, &#39;Timed out while waiting for SSH server to be ready&#39;
    return client</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests" href="index.html">harvester_e2e_tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="harvester_e2e_tests.utils.assert_image_ready" href="#harvester_e2e_tests.utils.assert_image_ready">assert_image_ready</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.assert_vm_ready" href="#harvester_e2e_tests.utils.assert_vm_ready">assert_vm_ready</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.assert_vm_restarted" href="#harvester_e2e_tests.utils.assert_vm_restarted">assert_vm_restarted</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.assert_vm_unschedulable" href="#harvester_e2e_tests.utils.assert_vm_unschedulable">assert_vm_unschedulable</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_clusternetworks_terraform" href="#harvester_e2e_tests.utils.create_clusternetworks_terraform">create_clusternetworks_terraform</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_image" href="#harvester_e2e_tests.utils.create_image">create_image</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_image_terraform" href="#harvester_e2e_tests.utils.create_image_terraform">create_image_terraform</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_image_upload" href="#harvester_e2e_tests.utils.create_image_upload">create_image_upload</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_keypair_terraform" href="#harvester_e2e_tests.utils.create_keypair_terraform">create_keypair_terraform</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_kubeconfig_from_template" href="#harvester_e2e_tests.utils.create_kubeconfig_from_template">create_kubeconfig_from_template</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_network_terraform" href="#harvester_e2e_tests.utils.create_network_terraform">create_network_terraform</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_tf_from_template" href="#harvester_e2e_tests.utils.create_tf_from_template">create_tf_from_template</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_vm" href="#harvester_e2e_tests.utils.create_vm">create_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_vm_backup" href="#harvester_e2e_tests.utils.create_vm_backup">create_vm_backup</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_vm_terraform" href="#harvester_e2e_tests.utils.create_vm_terraform">create_vm_terraform</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.create_volume_terraform" href="#harvester_e2e_tests.utils.create_volume_terraform">create_volume_terraform</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.delete_host" href="#harvester_e2e_tests.utils.delete_host">delete_host</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.delete_image" href="#harvester_e2e_tests.utils.delete_image">delete_image</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.delete_image_by_name" href="#harvester_e2e_tests.utils.delete_image_by_name">delete_image_by_name</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.delete_vm" href="#harvester_e2e_tests.utils.delete_vm">delete_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.delete_vm_backup" href="#harvester_e2e_tests.utils.delete_vm_backup">delete_vm_backup</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.delete_volume" href="#harvester_e2e_tests.utils.delete_volume">delete_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.delete_volume_by_name" href="#harvester_e2e_tests.utils.delete_volume_by_name">delete_volume_by_name</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.destroy_resource" href="#harvester_e2e_tests.utils.destroy_resource">destroy_resource</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.disable_maintenance_mode" href="#harvester_e2e_tests.utils.disable_maintenance_mode">disable_maintenance_mode</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.enable_maintenance_mode" href="#harvester_e2e_tests.utils.enable_maintenance_mode">enable_maintenance_mode</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.execute_script_on_vm" href="#harvester_e2e_tests.utils.execute_script_on_vm">execute_script_on_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.get_backup_create_files_script" href="#harvester_e2e_tests.utils.get_backup_create_files_script">get_backup_create_files_script</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.get_json_object_from_template" href="#harvester_e2e_tests.utils.get_json_object_from_template">get_json_object_from_template</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.get_latest_resource_version" href="#harvester_e2e_tests.utils.get_latest_resource_version">get_latest_resource_version</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.get_total_objects_nfs_share" href="#harvester_e2e_tests.utils.get_total_objects_nfs_share">get_total_objects_nfs_share</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.get_total_objects_s3_bucket" href="#harvester_e2e_tests.utils.get_total_objects_s3_bucket">get_total_objects_s3_bucket</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.get_vm_ip_address" href="#harvester_e2e_tests.utils.get_vm_ip_address">get_vm_ip_address</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.is_marker_enabled" href="#harvester_e2e_tests.utils.is_marker_enabled">is_marker_enabled</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.lookup_host_not_harvester_endpoint" href="#harvester_e2e_tests.utils.lookup_host_not_harvester_endpoint">lookup_host_not_harvester_endpoint</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.lookup_hosts_with_cpu_and_memory" href="#harvester_e2e_tests.utils.lookup_hosts_with_cpu_and_memory">lookup_hosts_with_cpu_and_memory</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.lookup_hosts_with_most_available_cpu" href="#harvester_e2e_tests.utils.lookup_hosts_with_most_available_cpu">lookup_hosts_with_most_available_cpu</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.lookup_hosts_with_most_available_memory" href="#harvester_e2e_tests.utils.lookup_hosts_with_most_available_memory">lookup_hosts_with_most_available_memory</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.lookup_vm_instance" href="#harvester_e2e_tests.utils.lookup_vm_instance">lookup_vm_instance</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.poll_for_resource_ready" href="#harvester_e2e_tests.utils.poll_for_resource_ready">poll_for_resource_ready</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.poll_for_update_resource" href="#harvester_e2e_tests.utils.poll_for_update_resource">poll_for_update_resource</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.power_off_node" href="#harvester_e2e_tests.utils.power_off_node">power_off_node</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.power_on_node" href="#harvester_e2e_tests.utils.power_on_node">power_on_node</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.poweroff_host_maintenance_mode" href="#harvester_e2e_tests.utils.poweroff_host_maintenance_mode">poweroff_host_maintenance_mode</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.random_alphanumeric" href="#harvester_e2e_tests.utils.random_alphanumeric">random_alphanumeric</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.random_name" href="#harvester_e2e_tests.utils.random_name">random_name</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.reboot_node" href="#harvester_e2e_tests.utils.reboot_node">reboot_node</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.restart_vm" href="#harvester_e2e_tests.utils.restart_vm">restart_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.restore_vm_backup" href="#harvester_e2e_tests.utils.restore_vm_backup">restore_vm_backup</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.retry_session" href="#harvester_e2e_tests.utils.retry_session">retry_session</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.stop_vm" href="#harvester_e2e_tests.utils.stop_vm">stop_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.utils.wait_for_ssh_client" href="#harvester_e2e_tests.utils.wait_for_ssh_client">wait_for_ssh_client</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>